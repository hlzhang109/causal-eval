{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvINbWH8+6h1QhEAB1q2Xn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDMrYjOu0CD1","executionInfo":{"status":"ok","timestamp":1747504900369,"user_tz":420,"elapsed":17572,"user":{"displayName":"Jikai Jin","userId":"04217813959834526607"}},"outputId":"eb8c38d4-5620-4114-c52d-dd5f19e91690"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Change the directory to the Tables folder\n","TABLE_DIR = '/content/drive/MyDrive/LLM causality/Tables/'\n","FIG_DIR = '/content/drive/MyDrive/LLM causality/Figures/'"],"metadata":{"id":"JPX5KgEyeNjX","executionInfo":{"status":"ok","timestamp":1747504900372,"user_tz":420,"elapsed":1,"user":{"displayName":"Jikai Jin","userId":"04217813959834526607"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"26fJ_RYAeBOR","executionInfo":{"status":"ok","timestamp":1747504904463,"user_tz":420,"elapsed":4092,"user":{"displayName":"Jikai Jin","userId":"04217813959834526607"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from scipy.optimize import minimize\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.decomposition import PCA\n","from sklearn.impute import KNNImputer\n","import seaborn as sns"]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","# Define a dictionary mapping base model families to their pretraining token counts\n","BASE_MODEL_TOKENS = {\n","    # Llama models\n","    'llama-3': 15.0,\n","    'llama-3.1': 15.0,\n","    'llama-3.2': 15.0,\n","    'llama-2': 2.0,\n","    'llama-1': 1.4,\n","\n","    # Mistral models\n","    'mistral-7b': 0.8,\n","    'mixtral': 1.0,\n","\n","    # Gemma models - corrected values\n","    'gemma-2-2b': 2.0,\n","    'gemma-2-9b': 8.0,\n","    'gemma-2-27b': 13.0,\n","    'gemma-1': 2.0,\n","\n","    # Qwen models\n","    'qwen2.5': 18.0,\n","    'qwen2': 7.0,\n","    'qwen1': 2.0,\n","\n","    # Yi models\n","    'yi-1.5': 3.0,\n","    'yi-1': 3.0,\n","\n","    # Phi models\n","    'phi-3': 3.3,\n","    'phi-2': 1.4,\n","    'phi-1.5': 0.03,\n","    'phi-1': 0.006,\n","\n","    # Other models\n","    'falcon-180b': 3.5,\n","    'falcon-40b': 1.0,\n","    'falcon-7b': 1.5,\n","    'glm3': 3.9,\n","    'glm2': 1.4,\n","    'deepseek2': 8.0,\n","    'deepseek': 2.0,\n","    'mpt-30b': 1.5,\n","    'mpt-7b': 1.0,\n","    'stablelm': 1.5,\n","    'bloom': 0.366,\n","    'baichuan-3': 3.2,\n","    'baichuan-2': 2.6\n","}\n","\n","# Define specific parameter size ranges for common model families with relaxed boundaries\n","# Format: (min_size, max_size, model_identifier)\n","PARAM_SIZE_RANGES = [\n","    # Llama-2 parameter sizes (relaxed ranges)\n","    (6.7, 7.5, 'llama-2-7b'),\n","    (12.5, 13.5, 'llama-2-13b'),\n","    (64.0, 71.0, 'llama-2-70b'),\n","\n","    # Llama-3 parameter sizes (relaxed ranges)\n","    (7.8, 8.3, 'llama-3-8b'),\n","    (69.0, 72.0, 'llama-3-70b'),\n","\n","    # Mistral parameter sizes\n","    (7.0, 7.5, 'mistral-7b'),\n","\n","    # Mixtral parameter sizes\n","    (45.0, 48.0, 'mixtral-8x7b'),\n","\n","    # Gemma-2 parameter sizes with corrected ranges\n","    (1.8, 2.2, 'gemma-2-2b'),  # Added 2B model\n","    (8.9, 9.5, 'gemma-2-9b'),\n","    (26.5, 29.0, 'gemma-2-27b'),\n","\n","    # Qwen2 parameter sizes\n","    (0.45, 0.55, 'qwen2-0.5b'),\n","    (1.4, 1.7, 'qwen2-1.5b'),\n","    (7.4, 7.8, 'qwen2-7b'),\n","    (14.0, 15.0, 'qwen2-14b'),\n","    (72.0, 73.5, 'qwen2-72b'),\n","\n","    # Phi-3 parameter sizes\n","    (3.7, 4.0, 'phi-3-4b'),\n","    (6.8, 7.2, 'phi-3-7b'),\n","    (13.0, 15.0, 'phi-3-14b')\n","]\n","\n","def extract_size_from_name(name):\n","    \"\"\"\n","    Extract the model size (e.g., 7b, 13b, 70b) from a model name\n","\n","    Args:\n","        name (str): The model name to analyze\n","\n","    Returns:\n","        str: The extracted size or None if not found\n","    \"\"\"\n","    # Look for common size patterns like 7b, 13b, 70b, etc.\n","    size_patterns = [\n","        r'[\\-_](\\d+\\.?\\d*)b(?!\\w)',  # matches -7b, _13b, -70b, etc.\n","        r'(\\d+\\.?\\d*)b[\\-_]',        # matches 7b-, 13b_, etc.\n","        r'[\\-_](\\d+\\.?\\d*)B(?!\\w)',  # matches -7B, _13B, -70B, etc.\n","        r'(\\d+\\.?\\d*)B[\\-_]',        # matches 7B-, 13B_, etc.\n","        r'/(\\d+\\.?\\d*)[bB](?![a-zA-Z0-9])', # matches /7b, /13B, etc.\n","    ]\n","\n","    for pattern in size_patterns:\n","        match = re.search(pattern, name)\n","        if match:\n","            size = match.group(1)\n","            return f\"{size}b\"\n","\n","    return None\n","\n","def match_param_size_to_model(param_size):\n","    \"\"\"\n","    Match parameter size to known model variants\n","\n","    Args:\n","        param_size (float): Parameter size in billions\n","\n","    Returns:\n","        str or None: Matched model identifier or None if no match\n","    \"\"\"\n","    if pd.isna(param_size):\n","        return None\n","\n","    for min_size, max_size, model_id in PARAM_SIZE_RANGES:\n","        if min_size <= param_size <= max_size:\n","            return model_id\n","\n","    return None\n","\n","def get_explicit_base_model(name):\n","    \"\"\"\n","    Check if the model name explicitly mentions a specific base model\n","\n","    Args:\n","        name (str): The model name\n","\n","    Returns:\n","        str or None: Identified base model with size or None if not found\n","    \"\"\"\n","    if pd.isna(name):\n","        return None\n","\n","    name_lower = name.lower()\n","\n","    # First, check for exact matches without capture groups\n","    exact_patterns = {\n","        r'gemma[\\-_]?2[\\-_]?2[bB]': 'gemma-2-2b',     # Explicitly match 2B\n","        r'gemma[\\-_]?2[\\-_]?9[bB]': 'gemma-2-9b',     # Explicitly match 9B\n","        r'gemma[\\-_]?2[\\-_]?27[bB]': 'gemma-2-27b',   # Explicitly match 27B\n","        r'mixtral[\\-_]?8x7[bB]': 'mixtral-8x7b'       # Explicitly match mixtral-8x7b\n","    }\n","\n","    # Check exact patterns first\n","    for pattern, result in exact_patterns.items():\n","        if re.search(pattern, name_lower):\n","            return result\n","\n","    # Then check patterns with capture groups\n","    # Llama models\n","    llama_patterns = [\n","        (r'llama[\\-_]?3\\.2[\\-_]?(\\d+\\.?\\d*)[bB]', 'llama-3.2-{}b'),\n","        (r'llama[\\-_]?3\\.1[\\-_]?(\\d+\\.?\\d*)[bB]', 'llama-3.1-{}b'),\n","        (r'llama[\\-_]?3(?!\\.)[\\-_]?(\\d+\\.?\\d*)[bB]', 'llama-3-{}b'),\n","        (r'llama[\\-_]?2[\\-_]?(\\d+\\.?\\d*)[bB]', 'llama-2-{}b'),\n","        (r'llama[\\-_]?1[\\-_]?(\\d+\\.?\\d*)[bB]', 'llama-1-{}b')\n","    ]\n","\n","    # Mistral models\n","    mistral_patterns = [\n","        (r'mistral[\\-_]?(\\d+\\.?\\d*)[bB]', 'mistral-{}b')\n","    ]\n","\n","    # Gemma models - with capture groups for variable sizes\n","    gemma_patterns = [\n","        (r'gemma[\\-_]?2[\\-_]?(\\d+\\.?\\d*)[bB]', 'gemma-2-{}b'),  # For other sizes\n","        (r'gemma(?![\\-_]?2)[\\-_]?(\\d+\\.?\\d*)[bB]', 'gemma-1-{}b')\n","    ]\n","\n","    # Qwen models\n","    qwen_patterns = [\n","        (r'qwen[\\-_]?2\\.5[\\-_]?(\\d+\\.?\\d*)[bB]', 'qwen2.5-{}b'),\n","        (r'qwen[\\-_]?2(?!\\.5)[\\-_]?(\\d+\\.?\\d*)[bB]', 'qwen2-{}b'),\n","        (r'qwen(?![\\-_]?2)[\\-_]?(\\d+\\.?\\d*)[bB]', 'qwen1-{}b')\n","    ]\n","\n","    # Yi models\n","    yi_patterns = [\n","        (r'yi[\\-_]?1\\.5[\\-_]?(\\d+\\.?\\d*)[bB]', 'yi-1.5-{}b'),\n","        (r'yi(?![\\-_]?1\\.5)[\\-_]?1[\\-_]?(\\d+\\.?\\d*)[bB]', 'yi-1-{}b')\n","    ]\n","\n","    # Phi models\n","    phi_patterns = [\n","        (r'phi[\\-_]?3[\\-_]?(\\d+\\.?\\d*)[bB]', 'phi-3-{}b'),\n","        (r'phi[\\-_]?2[\\-_]?(\\d+\\.?\\d*)[bB]', 'phi-2-{}b'),\n","        (r'phi[\\-_]?1\\.5[\\-_]?(\\d+\\.?\\d*)[bB]', 'phi-1.5-{}b'),\n","        (r'phi[\\-_]?1(?!\\.5)[\\-_]?(\\d+\\.?\\d*)[bB]', 'phi-1-{}b')\n","    ]\n","\n","    # Other models\n","    other_patterns = [\n","        (r'falcon[\\-_]?(\\d+\\.?\\d*)[bB]', 'falcon-{}b'),\n","        (r'mpt[\\-_]?(\\d+\\.?\\d*)[bB]', 'mpt-{}b')\n","    ]\n","\n","    # Combine all patterns\n","    all_patterns = llama_patterns + mistral_patterns + gemma_patterns + qwen_patterns + yi_patterns + phi_patterns + other_patterns\n","\n","    # Check each pattern\n","    for pattern, template in all_patterns:\n","        match = re.search(pattern, name_lower)\n","        if match:\n","            size = match.group(1)\n","            return template.format(size)\n","\n","    # Special cases that don't follow the standard pattern\n","    if 'mixtral' in name_lower and '8x7b' in name_lower:\n","        return 'mixtral-8x7b'\n","\n","    return None\n","\n","def extract_base_model_from_name(model_name, param_size=None):\n","    \"\"\"\n","    Extract the base model including size from a model name\n","\n","    Args:\n","        model_name (str): The model name to analyze\n","        param_size (float, optional): Parameter size in billions\n","\n","    Returns:\n","        tuple: (base_model_family, full_base_model_with_size)\n","    \"\"\"\n","    if pd.isna(model_name):\n","        return None, None\n","\n","    name = model_name.lower()\n","\n","    # First, check for explicit base model mention\n","    explicit_base = get_explicit_base_model(name)\n","    if explicit_base:\n","        # Extract the family from the explicit base\n","        family_match = re.match(r'([a-z0-9\\.\\-]+)-\\d+', explicit_base)\n","        if family_match:\n","            family = family_match.group(1)\n","\n","            # Special handling for Gemma-2 with different sizes\n","            if family == 'gemma-2':\n","                size_match = re.search(r'gemma-2-(\\d+)b', explicit_base)\n","                if size_match:\n","                    size = size_match.group(1)\n","                    if size == '2':\n","                        return 'gemma-2-2b', explicit_base\n","                    elif size == '9':\n","                        return 'gemma-2-9b', explicit_base\n","                    elif size == '27':\n","                        return 'gemma-2-27b', explicit_base\n","\n","            return family, explicit_base\n","\n","    # If no explicit base, try to identify the family\n","    base_model_family = None\n","\n","    # Try to match base models in descending order of specificity\n","    if re.search(r'llama[\\-_]?3\\.2|llama[/\\\\]3\\.2', name):\n","        base_model_family = 'llama-3.2'\n","    elif re.search(r'llama[\\-_]?3\\.1|llama[/\\\\]3\\.1', name):\n","        base_model_family = 'llama-3.1'\n","    elif re.search(r'llama[\\-_]?3(?!\\.)|llama[/\\\\]3(?!\\.)', name):\n","        base_model_family = 'llama-3'\n","    elif re.search(r'llama[\\-_]?2|llama[/\\\\]2', name):\n","        base_model_family = 'llama-2'\n","    elif re.search(r'llama[\\-_]?1|llama[/\\\\]1', name):\n","        base_model_family = 'llama-1'\n","    elif re.search(r'mistral[\\-_]?7b|mistral[/\\\\]7b', name):\n","        base_model_family = 'mistral-7b'\n","    elif 'mixtral' in name:\n","        base_model_family = 'mixtral'\n","    elif re.search(r'gemma[\\-_]?2[\\-_]?2b|gemma[/\\\\]2[\\-_]?2b', name):\n","        base_model_family = 'gemma-2-2b'\n","    elif re.search(r'gemma[\\-_]?2[\\-_]?9b|gemma[/\\\\]2[\\-_]?9b', name):\n","        base_model_family = 'gemma-2-9b'\n","    elif re.search(r'gemma[\\-_]?2[\\-_]?27b|gemma[/\\\\]2[\\-_]?27b', name):\n","        base_model_family = 'gemma-2-27b'\n","    elif re.search(r'gemma[\\-_]?2|gemma[/\\\\]2', name):\n","        # If gemma-2 without size, we need to determine from param_size\n","        if param_size:\n","            if 1.8 <= param_size <= 2.2:\n","                base_model_family = 'gemma-2-2b'\n","            elif 8.9 <= param_size <= 9.5:\n","                base_model_family = 'gemma-2-9b'\n","            elif 26.5 <= param_size <= 28.5:\n","                base_model_family = 'gemma-2-27b'\n","    elif re.search(r'gemma[\\-_]?1|gemma[/\\\\]1', name):\n","        base_model_family = 'gemma-1'\n","    elif re.search(r'qwen[\\-_]?2\\.5|qwen[/\\\\]2\\.5', name):\n","        base_model_family = 'qwen2.5'\n","    elif re.search(r'qwen[\\-_]?2(?!\\.)|qwen[/\\\\]2(?!\\.)', name):\n","        base_model_family = 'qwen2'\n","    elif re.search(r'qwen[\\-_]?1|qwen[/\\\\]1', name):\n","        base_model_family = 'qwen1'\n","    elif re.search(r'yi[\\-_]?1\\.5|yi[/\\\\]1\\.5', name):\n","        base_model_family = 'yi-1.5'\n","    elif re.search(r'yi[\\-_]?1(?!\\.)|yi[/\\\\]1(?!\\.)', name):\n","        base_model_family = 'yi-1'\n","    elif re.search(r'phi[\\-_]?3|phi[/\\\\]3', name):\n","        base_model_family = 'phi-3'\n","    elif re.search(r'phi[\\-_]?2|phi[/\\\\]2', name):\n","        base_model_family = 'phi-2'\n","    elif re.search(r'phi[\\-_]?1\\.5|phi[/\\\\]1\\.5', name):\n","        base_model_family = 'phi-1.5'\n","    elif re.search(r'phi[\\-_]?1(?!\\.)|phi[/\\\\]1(?!\\.)', name):\n","        base_model_family = 'phi-1'\n","    elif re.search(r'falcon[\\-_]?180b|falcon[/\\\\]180b', name):\n","        base_model_family = 'falcon-180b'\n","    elif re.search(r'falcon[\\-_]?40b|falcon[/\\\\]40b', name):\n","        base_model_family = 'falcon-40b'\n","    elif re.search(r'falcon[\\-_]?7b|falcon[/\\\\]7b', name):\n","        base_model_family = 'falcon-7b'\n","    elif re.search(r'chatglm3|glm3', name):\n","        base_model_family = 'glm3'\n","    elif re.search(r'chatglm2|glm2', name):\n","        base_model_family = 'glm2'\n","    elif re.search(r'deepseek[\\-_]?llm[\\-_]?2|deepseek2', name):\n","        base_model_family = 'deepseek2'\n","    elif 'deepseek' in name:\n","        base_model_family = 'deepseek'\n","    elif re.search(r'mpt[\\-_]?30b|mpt[/\\\\]30b', name):\n","        base_model_family = 'mpt-30b'\n","    elif re.search(r'mpt[\\-_]?7b|mpt[/\\\\]7b', name):\n","        base_model_family = 'mpt-7b'\n","    elif 'stablelm' in name:\n","        base_model_family = 'stablelm'\n","    elif 'bloom' in name:\n","        base_model_family = 'bloom'\n","    elif re.search(r'baichuan[\\-_]?3|baichuan[/\\\\]3', name):\n","        base_model_family = 'baichuan-3'\n","    elif re.search(r'baichuan[\\-_]?2|baichuan[/\\\\]2', name):\n","        base_model_family = 'baichuan-2'\n","\n","    # If we identified a base model family\n","    if base_model_family:\n","        # Check if the base model already includes size info (like mistral-7b)\n","        if re.search(r'\\d+b$', base_model_family):\n","            return base_model_family, base_model_family\n","\n","        # If size is in the name, use it\n","        size_from_name = extract_size_from_name(name)\n","        if size_from_name:\n","            return base_model_family, f\"{base_model_family}-{size_from_name}\"\n","\n","        # If size isn't in the name but we have param_size, use that\n","        if param_size:\n","            # Try to match to known parameter size ranges\n","            param_match = match_param_size_to_model(param_size)\n","            if param_match and param_match.startswith(base_model_family):\n","                return base_model_family, param_match\n","\n","            # Fallback to generic size format\n","            return base_model_family, f\"{base_model_family}-{int(param_size)}b\"\n","\n","        # Return just the family if we can't determine size\n","        return base_model_family, base_model_family\n","\n","    # No base model identified\n","    return None, None\n","\n","def architecture_to_base_model(architecture, param_size, model_name):\n","    \"\"\"\n","    Map architecture and parameter size to a likely base model with size\n","\n","    Args:\n","        architecture (str): Model architecture\n","        param_size (float): Parameter size in billions\n","        model_name (str): The model name (to check for Qwen version)\n","\n","    Returns:\n","        tuple: (base_model_family, full_base_model_with_size)\n","    \"\"\"\n","    if pd.isna(architecture) or pd.isna(param_size):\n","        return None, None\n","\n","    arch = architecture.lower()\n","    name_lower = str(model_name).lower() if not pd.isna(model_name) else \"\"\n","\n","    # Check if we can match the parameter size to a known model\n","    param_match = match_param_size_to_model(param_size)\n","\n","    # For LlamaForCausalLM, be more careful\n","    if 'llama' in arch:\n","        # Only match if parameter size fits known Llama models\n","        if param_match and ('llama-2' in param_match or 'llama-3' in param_match):\n","            family = param_match.rsplit('-', 1)[0]  # Extract family from match\n","            return family, param_match\n","        return None, None  # Don't guess if size doesn't match\n","\n","    # For non-Llama architectures, we can be more confident\n","    if 'mistral' in arch and not 'mixtral' in arch:\n","        if 7.0 <= param_size <= 7.5:\n","            return 'mistral-7b', 'mistral-7b'\n","\n","    if 'mixtral' in arch or 'mistral' in arch and 'moe' in name_lower:\n","        if 45.0 <= param_size <= 48.0:\n","            return 'mixtral', 'mixtral-8x7b'\n","\n","    if 'gemma2' in arch:\n","        if 1.8 <= param_size <= 2.2:\n","            return 'gemma-2-2b', 'gemma-2-2b'\n","        elif 8.9 <= param_size <= 9.5:\n","            return 'gemma-2-9b', 'gemma-2-9b'\n","        elif 27.0 <= param_size <= 29.0:\n","            return 'gemma-2-27b', 'gemma-2-27b'\n","\n","    if 'gemma' in arch and 'gemma2' not in arch:\n","        if 2.0 <= param_size <= 3.0:\n","            return 'gemma-1', 'gemma-1-2b'\n","        elif 7.0 <= param_size <= 9.0:\n","            return 'gemma-1', 'gemma-1-7b'\n","\n","    # For Qwen2, check the name first to disambiguate between Qwen2 and Qwen2.5\n","    if 'qwen2' in arch:\n","        if 'qwen2.5' in name_lower:\n","            base_family = 'qwen2.5'\n","        elif 3.0 <= param_size <= 3.2 or 14.0 <= param_size <= 15.0 or 32.0 <= param_size <= 35.0:\n","            base_family = 'qwen2.5'\n","        elif 'qwen2' in name_lower:\n","            base_family = 'qwen2'\n","        else:\n","            # If name doesn't clarify, default to Qwen2 (the architecture name)\n","            base_family = 'qwen2'\n","\n","        # Now determine the size\n","        if param_match and param_match.startswith(base_family):\n","            return base_family, param_match\n","        elif 0.45 <= param_size <= 0.55:\n","            return base_family, f\"{base_family}-0.5b\"\n","        elif 1.4 <= param_size <= 1.7:\n","            return base_family, f\"{base_family}-1.5b\"\n","        elif 3.0 <= param_size <= 3.2:\n","            return base_family, f\"{base_family}-3b\"\n","        elif 7.4 <= param_size <= 7.8:\n","            return base_family, f\"{base_family}-7b\"\n","        elif 14.0 <= param_size <= 15.0:\n","            return base_family, f\"{base_family}-14b\"\n","        elif 72.0 <= param_size <= 73.5:\n","            return base_family, f\"{base_family}-72b\"\n","\n","    if 'falcon' in arch:\n","        if 175.0 <= param_size <= 185.0:\n","            return 'falcon-180b', 'falcon-180b'\n","        elif 39.0 <= param_size <= 41.0:\n","            return 'falcon-40b', 'falcon-40b'\n","        elif 6.5 <= param_size <= 7.5:\n","            return 'falcon-7b', 'falcon-7b'\n","\n","    if 'phi' in arch:\n","        if 'phi-3' in name_lower:\n","            base_family = 'phi-3'\n","        elif 'phi-2' in name_lower:\n","            base_family = 'phi-2'\n","        elif 'phi-1.5' in name_lower:\n","            base_family = 'phi-1.5'\n","        elif 'phi-1' in name_lower and 'phi-1.5' not in name_lower:\n","            base_family = 'phi-1'\n","        else:\n","            # If name doesn't clarify, try to infer from param size\n","            if param_size > 10:\n","                base_family = 'phi-3'\n","            elif param_size > 2:\n","                base_family = 'phi-2'\n","            elif param_size > 1:\n","                base_family = 'phi-1.5'\n","            else:\n","                base_family = 'phi-1'\n","\n","        if param_match and param_match.startswith(base_family):\n","            return base_family, param_match\n","        else:\n","            return base_family, f\"{base_family}-{int(param_size)}b\"\n","\n","    # No match found\n","    return None, None\n","\n","def determine_pretraining_tokens(row):\n","    \"\"\"\n","    Comprehensive approach to determine pretraining token count\n","\n","    Args:\n","        row: A pandas DataFrame row containing model information\n","\n","    Returns:\n","        tuple: (token_count, base_model_family, full_base_model_with_size)\n","    \"\"\"\n","    # Extract relevant fields\n","    model_name = row.get('fullname', '')\n","    architecture = row.get('Architecture', '')\n","    param_size = row.get('#Params (B)', 0)\n","    base_model_field = row.get('Base Model', '')\n","\n","    # Check for NaN values\n","    if pd.isna(model_name):\n","        return None, None, None\n","\n","    # 1. First check if the name explicitly mentions a base model\n","    explicit_base = get_explicit_base_model(str(model_name))\n","    if explicit_base:\n","        family_match = re.match(r'([a-z0-9\\.\\-]+)-\\d+', explicit_base)\n","        if family_match:\n","            family = family_match.group(1)\n","            if family in BASE_MODEL_TOKENS:\n","                return BASE_MODEL_TOKENS[family], family, explicit_base\n","\n","    # 2. Check if Base Model field contains useful information\n","    if not pd.isna(base_model_field):\n","        explicit_base = get_explicit_base_model(str(base_model_field))\n","        if explicit_base:\n","            family_match = re.match(r'([a-z0-9\\.\\-]+)-\\d+', explicit_base)\n","            if family_match:\n","                family = family_match.group(1)\n","                if family in BASE_MODEL_TOKENS:\n","                    return BASE_MODEL_TOKENS[family], family, explicit_base\n","\n","        base_family, base_with_size = extract_base_model_from_name(str(base_model_field), param_size)\n","        if base_family and base_family in BASE_MODEL_TOKENS:\n","            return BASE_MODEL_TOKENS[base_family], base_family, base_with_size\n","\n","    # 3. Try to extract base model from the model name\n","    base_family, base_with_size = extract_base_model_from_name(model_name, param_size)\n","    if base_family and base_family in BASE_MODEL_TOKENS:\n","        return BASE_MODEL_TOKENS[base_family], base_family, base_with_size\n","\n","    # 4. Use architecture and parameter size\n","    if not pd.isna(architecture) and not pd.isna(param_size):\n","        base_family, base_with_size = architecture_to_base_model(architecture, param_size, model_name)\n","        if base_family and base_family in BASE_MODEL_TOKENS:\n","            return BASE_MODEL_TOKENS[base_family], base_family, base_with_size\n","\n","    # No match found\n","    return None, None, None\n","\n","def process_csv(input_file, output_file):\n","    \"\"\"\n","    Process a CSV file to add pretraining token information\n","\n","    Args:\n","        input_file (str): Path to input CSV file\n","        output_file (str): Path to save the output CSV file\n","    \"\"\"\n","    # Read the CSV file\n","    df = pd.read_csv(input_file)\n","\n","    # Apply the function to determine pretraining tokens and base models\n","    results = df.apply(determine_pretraining_tokens, axis=1)\n","\n","    # Unpack the results\n","    df['Pretraining tokens (T)'] = [r[0] for r in results]\n","    df['Base model family'] = [r[1] for r in results]\n","    df['Identified base model'] = [r[2] for r in results]\n","\n","    # Save the updated dataframe to a new CSV file\n","    df.to_csv(output_file, index=False)\n","\n","    # Print summary statistics\n","    token_counts = df['Pretraining tokens (T)'].value_counts().sort_index()\n","    matched = df['Pretraining tokens (T)'].notna().sum()\n","    total = len(df)\n","\n","    print(f\"Matched {matched} out of {total} models ({matched/total*100:.2f}%)\")\n","    print(\"\\nDistribution of pretraining token counts:\")\n","    for tokens, count in token_counts.items():\n","        print(f\"{tokens} trillion: {count} models\")\n","\n","    # Print base model statistics\n","    base_model_counts = df['Base model family'].value_counts().sort_values(ascending=False)\n","    print(\"\\nDistribution of identified base model families:\")\n","    for base_model, count in base_model_counts.items():\n","        if pd.isna(base_model):\n","            print(f\"Unknown: {count} models\")\n","        else:\n","            print(f\"{base_model}: {count} models\")\n","\n","    # Print some examples for verification\n","    print(\"\\nSample models for each token value:\")\n","    for tokens in token_counts.index:\n","        examples = df[df['Pretraining tokens (T)'] == tokens][['fullname', 'Identified base model', 'Architecture', '#Params (B)']].head(3)\n","        print(f\"\\n{tokens} trillion tokens examples:\")\n","        for _, example in examples.iterrows():\n","            print(f\"  - {example['fullname']} (Base: {example['Identified base model']}, Arch: {example['Architecture']}, Params: {example['#Params (B)']}B)\")\n","\n","process_csv(TABLE_DIR + 'open_llm_leaderboard.csv', TABLE_DIR + 'open_llm_leaderboard_with_tokens.csv')\n","# process_csv(TABLE_DIR + 'open_llm_leaderboard_old.csv', TABLE_DIR + 'open_llm_leaderboard_old_with_tokens.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FT0_FLe0MxZI","executionInfo":{"status":"ok","timestamp":1747504908895,"user_tz":420,"elapsed":4430,"user":{"displayName":"Jikai Jin","userId":"04217813959834526607"}},"outputId":"67066d73-6df5-4967-bfe8-dfef48f127ba"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Matched 3360 out of 4576 models (73.43%)\n","\n","Distribution of pretraining token counts:\n","0.006 trillion: 4 models\n","0.366 trillion: 6 models\n","0.8 trillion: 306 models\n","1.0 trillion: 26 models\n","1.4 trillion: 71 models\n","1.5 trillion: 12 models\n","2.0 trillion: 291 models\n","3.0 trillion: 21 models\n","3.3 trillion: 69 models\n","7.0 trillion: 351 models\n","8.0 trillion: 135 models\n","13.0 trillion: 25 models\n","15.0 trillion: 1252 models\n","18.0 trillion: 791 models\n","\n","Distribution of identified base model families:\n","llama-3: 807 models\n","qwen2.5: 791 models\n","qwen2: 351 models\n","mistral-7b: 306 models\n","llama-3.1: 278 models\n","llama-3.2: 167 models\n","gemma-2-9b: 135 models\n","llama-2: 118 models\n","qwen1: 89 models\n","phi-3: 69 models\n","phi-2: 51 models\n","gemma-1: 38 models\n","gemma-2-2b: 29 models\n","gemma-2-27b: 25 models\n","mixtral: 24 models\n","yi-1.5: 20 models\n","llama-1: 20 models\n","deepseek: 17 models\n","stablelm: 7 models\n","bloom: 6 models\n","falcon-7b: 5 models\n","phi-1: 4 models\n","falcon-40b: 2 models\n","yi-1: 1 models\n","\n","Sample models for each token value:\n","\n","0.006 trillion tokens examples:\n","  - Quazim0t0/Lo-Phi-14b (Base: phi-1-4b, Arch: LlamaForCausalLM, Params: 14.66B)\n","  - microsoft/phi-1 (Base: phi-1-1b, Arch: PhiForCausalLM, Params: 1.418B)\n","  - microsoft/phi-1_5 (Base: phi-1-1b, Arch: PhiForCausalLM, Params: 1.418B)\n","\n","0.366 trillion tokens examples:\n","  - Quazim0t0/bloom-14b-stock (Base: bloom-14b, Arch: LlamaForCausalLM, Params: 14.66B)\n","  - bigscience/bloom-1b1 (Base: bloom-1b, Arch: BloomForCausalLM, Params: 1.065B)\n","  - bigscience/bloom-1b7 (Base: bloom-1b, Arch: BloomForCausalLM, Params: 1.722B)\n","\n","0.8 trillion tokens examples:\n","  - 0-hero/Matter-0.2-7B-DPO (Base: mistral-7b, Arch: MistralForCausalLM, Params: 7.242B)\n","  - 1TuanPham/T-VisStar-7B-v0.1 (Base: mistral-7b, Arch: MistralForCausalLM, Params: 7.294B)\n","  - 1TuanPham/T-VisStar-v0.1 (Base: mistral-7b, Arch: MistralForCausalLM, Params: 7.294B)\n","\n","1.0 trillion tokens examples:\n","  - GritLM/GritLM-8x7B-KTO (Base: mixtral-8x7b, Arch: MixtralForCausalLM, Params: 46.703B)\n","  - HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1 (Base: mixtral-22b, Arch: MixtralForCausalLM, Params: 140.621B)\n","  - LeroyDyer/Mixtral_AI_SwahiliTron_7b (Base: mixtral-7b, Arch: MistralForCausalLM, Params: 7.242B)\n","\n","1.4 trillion tokens examples:\n","  - Artples/L-MChat-Small (Base: phi-2-2b, Arch: PhiForCausalLM, Params: 2.78B)\n","  - Ayush-Singh/Llama1B-sft-2 (Base: llama-1-1b, Arch: LlamaForCausalLM, Params: 1.236B)\n","  - BEE-spoke-data/smol_llama-101M-GQA (Base: llama-1-0b, Arch: LlamaForCausalLM, Params: 0.101B)\n","\n","1.5 trillion tokens examples:\n","  - stabilityai/stablelm-2-12b (Base: stablelm-12b, Arch: StableLmForCausalLM, Params: 12.143B)\n","  - stabilityai/stablelm-2-12b-chat (Base: stablelm-12b, Arch: StableLmForCausalLM, Params: 12.143B)\n","  - stabilityai/stablelm-2-1_6b (Base: stablelm-6b, Arch: StableLmForCausalLM, Params: 1.645B)\n","\n","2.0 trillion tokens examples:\n","  - 1024m/QWEN-14B-B100 (Base: qwen1-14b, Arch: Qwen2ForCausalLM, Params: 14.77B)\n","  - 152334H/miqu-1-70b-sf (Base: llama-2-70b, Arch: LlamaForCausalLM, Params: 68.977B)\n","  - AI-MO/NuminaMath-7B-CoT (Base: deepseek-7b, Arch: LlamaForCausalLM, Params: 6.91B)\n","\n","3.0 trillion tokens examples:\n","  - 01-ai/Yi-1.5-34B (Base: yi-1.5-34b, Arch: LlamaForCausalLM, Params: 34.389B)\n","  - 01-ai/Yi-1.5-34B-32K (Base: yi-1.5-34b, Arch: LlamaForCausalLM, Params: 34.389B)\n","  - 01-ai/Yi-1.5-34B-Chat (Base: yi-1.5-34b, Arch: LlamaForCausalLM, Params: 34.389B)\n","\n","3.3 trillion tokens examples:\n","  - Ba2han/Llama-Phi-3_DoRA (Base: phi-3-4b, Arch: MistralForCausalLM, Params: 3.821B)\n","  - BlackBeenie/Neos-Phi-3-14B-v0.1 (Base: phi-3-14b, Arch: Phi3ForCausalLM, Params: 13.96B)\n","  - Danielbrdz/Barcenas-14b-Phi-3-medium-ORPO (Base: phi-3-14b, Arch: MistralForCausalLM, Params: 13.96B)\n","\n","7.0 trillion tokens examples:\n","  - AI4free/t2 (Base: qwen2-7b, Arch: Qwen2ForCausalLM, Params: 7.613B)\n","  - AIDC-AI/Marco-o1 (Base: qwen2-7b, Arch: Qwen2ForCausalLM, Params: 7.616B)\n","  - Aashraf995/Creative-7B-nerd (Base: qwen2-7b, Arch: Qwen2ForCausalLM, Params: 7.616B)\n","\n","8.0 trillion tokens examples:\n","  - AELLM/gemma-2-aeria-infinity-9b (Base: gemma-2-9b, Arch: Gemma2ForCausalLM, Params: 9.242B)\n","  - BAAI/Gemma2-9B-IT-Simpo-Infinity-Preference (Base: gemma-2-9b, Arch: Gemma2ForCausalLM, Params: 9.242B)\n","  - BlackBeenie/Neos-Gemma-2-9b (Base: gemma-2-9b, Arch: Gemma2ForCausalLM, Params: 9.242B)\n","\n","13.0 trillion tokens examples:\n","  - AALF/gemma-2-27b-it-SimPO-37K (Base: gemma-2-27b, Arch: Gemma2ForCausalLM, Params: 27.227B)\n","  - AALF/gemma-2-27b-it-SimPO-37K-100steps (Base: gemma-2-27b, Arch: Gemma2ForCausalLM, Params: 27.227B)\n","  - INSAIT-Institute/BgGPT-Gemma-2-27B-IT-v1.0 (Base: gemma-2-27b, Arch: Gemma2ForCausalLM, Params: 27.227B)\n","\n","15.0 trillion tokens examples:\n","  - 3rd-Degree-Burn/L-3.1-Science-Writer-8B (Base: llama-3-8b, Arch: LlamaForCausalLM, Params: 8.03B)\n","  - 3rd-Degree-Burn/Llama-3.1-8B-Squareroot (Base: llama-3.1-8b, Arch: LlamaForCausalLM, Params: 8.03B)\n","  - 3rd-Degree-Burn/Llama-3.1-8B-Squareroot-v1 (Base: llama-3.1-8b, Arch: LlamaForCausalLM, Params: 8.03B)\n","\n","18.0 trillion tokens examples:\n","  - 1-800-LLMs/Qwen-2.5-14B-Hindi (Base: qwen2.5-14b, Arch: Qwen2ForCausalLM, Params: 14.77B)\n","  - 1-800-LLMs/Qwen-2.5-14B-Hindi-Custom-Instruct (Base: qwen2.5-14b, Arch: Qwen2ForCausalLM, Params: 14.77B)\n","  - AGI-0/Art-v0-3B (Base: qwen2.5-3b, Arch: Qwen2ForCausalLM, Params: 3.086B)\n"]}]}]}