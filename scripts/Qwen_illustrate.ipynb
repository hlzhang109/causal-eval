{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPX5KgEyeNjX"
   },
   "outputs": [],
   "source": [
    "# Change the directory to the Tables folder\n",
    "TABLE_DIR = '../Tables/'\n",
    "FIG_DIR = '../Figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JU0gPLJU7dYv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH-7-1lMBOxL"
   },
   "source": [
    "This notbook illustrates a clear correlation between a model's benchmark performance and the base model. We will look at models on open LLM leaderboard based on three pretrained models: Qwen2.5 0.5B, 7B and 14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R-L_EvnBu7b"
   },
   "outputs": [],
   "source": [
    "# Base models that we will look at\n",
    "frequent_base_models_new = ['llama-3.1-8b', 'qwen2.5-14b', 'qwen2.5-7b', 'qwen2.5-0.5b', 'mistral-7b', 'gemma-2-9b']\n",
    "# Benchmarks included in the leaderboard\n",
    "cols_to_transform_new = ['IFEval', 'BBH', 'MATH Lvl 5', 'GPQA', 'MUSR', 'MMLU-PRO']\n",
    "# Load the leaderboard\n",
    "df_filtered_new = pd.read_csv(TABLE_DIR + 'open_llm_leaderboard_with_token_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i0wV5VmBOWw"
   },
   "outputs": [],
   "source": [
    "models = ['qwen2.5-14b', 'qwen2.5-7b', 'qwen2.5-0.5b']\n",
    "benchmarks = ['IFEval', 'BBH', 'MATH Lvl 5', 'GPQA', 'MUSR', 'MMLU-PRO']\n",
    "palette = sns.color_palette(\"Spectral\", len(benchmarks))\n",
    "\n",
    "bar_width = 0.15  # Width of each bar\n",
    "index = np.arange(len(models))  # X-axis positions for the groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, benchmark in enumerate(benchmarks):\n",
    "    benchmark_data = [df_filtered_new[df_filtered_new['fullname'].str.lower().str.contains(model)][benchmark].mean() for model in models]\n",
    "    offset = bar_width * i\n",
    "    ax.bar(index + offset, benchmark_data, bar_width, label=benchmark, color=palette[i])\n",
    "\n",
    "# Add grid lines and customize appearance\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)  # Add horizontal grid lines\n",
    "ax.spines['top'].set_visible(False)  # Remove top spine\n",
    "ax.spines['right'].set_visible(False)  # Remove right spine\n",
    "\n",
    "ax.set_xlabel('Models', fontsize = 15)\n",
    "ax.set_ylabel('Average Accuracy', fontsize = 15)\n",
    "# ax.set_title('Average Accuracy across Benchmarks for Different Qwen Models')\n",
    "ax.set_xticks(index + bar_width * (len(benchmarks) / 2))  # Center the x-axis ticks\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(fontsize=12, frameon=False)\n",
    "\n",
    "plt.savefig(TABLE_DIR + \"Qwen_perf_compare.png\", dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM81E9pJtsf9tMwp2pvxfN0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
