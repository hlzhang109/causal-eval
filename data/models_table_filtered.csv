Model,Unnamed: 0,eval_name,Precision,Type,T,Weight type,Architecture,fullname,Model sha,Average ⬆️,Hub License,Hub ❤️,#Params (B),Available on the hub,MoE,Flagged,Chat Template,CO₂ cost (kg),IFEval Raw,IFEval,BBH Raw,BBH,MATH Lvl 5 Raw,MATH Lvl 5,GPQA Raw,GPQA_x,MUSR Raw,MUSR,MMLU-PRO Raw,MMLU-PRO,Merged,Official Providers,Upload To Hub Date,Submission Date,Generation,Base Model,Model_Variants,Lab,Playground,"Parameters
(B)","Tokens
trained (B)","Ratio Tokens:Params
(Chinchilla scaling≥20:1)","ALScore
""ALScore"" is a quick and dirty rating of the model's power. The formula is:
Sqr Root of (Parameters x Tokens) ÷ 300.
Any ALScore ≥ 1.0 is a powerful model in mid-2023.",MMLU,"MMLU
-Pro",GPQA_y,Training dataset,"Announced
▼",Public?,Paper / Repo,Arch,Notes
"<a target=""_blank"" href=""https://huggingface.co/01-ai/Yi-1.5-34B-Chat"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">01-ai/Yi-1.5-34B-Chat</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-34B-Chat-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3,01-ai_Yi-1.5-34B-Chat_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,LlamaForCausalLM,01-ai/Yi-1.5-34B-Chat,f3128b2d02d82989daae566c0a7eadc621ca3254,32.89223334850039,apache-2.0,266,34.389,True,False,False,True,11.211921933663872,0.6066758423205982,60.66758423205982,0.6083748310271819,44.262825981005655,0.2492447129909365,24.924471299093657,0.3649328859060403,15.324384787472036,0.4281979166666667,13.058072916666664,0.4520445478723404,39.11606087470449,False,True,2024-05-10,2024-06-12,0,01-ai/Yi-1.5-34B-Chat,yi 1.5 34b-chat,01-ai,https://huggingface.co/01-ai/Yi-1.5-34B-Chat,34.4,3600.0,105:1,1.2,76.8,52.3,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://github.com/01-ai/Yi-1.5,Dense,Uses 600B more training tokens than Yi 1.0 (Nov/2023).
"<a target=""_blank"" href=""https://huggingface.co/01-ai/Yi-34B"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">01-ai/Yi-34B</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-34B-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",11,01-ai_Yi-34B_bfloat16,bfloat16,🟢 pretrained,🟢,Original,LlamaForCausalLM,01-ai/Yi-34B,e1e7da8c75cfd5c44522228599fd4d2990cedd1c,22.385715135754378,apache-2.0,1291,34.389,True,False,False,False,12.828741644389773,0.3045751938190667,30.45751938190668,0.5457099951794562,35.542431259008794,0.0521148036253776,5.211480362537765,0.3666107382550335,15.548098434004473,0.4118541666666667,9.648437500000004,0.441156914893617,37.90632387706855,False,True,2023-11-01,2024-06-12,0,01-ai/Yi-34B,yi 34b,01-ai,https://huggingface.co/01-ai/Yi-34B,34.4,3000.0,88:1,1.1,76.3,43.0,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://github.com/01-ai/Yi,Dense,Controversy about Llama 2 base. https://twitter.com/kaifulee/status/1724673131875377465 MMLU=76.3 (PaLM 2=78.3) Outperforms Llama 2. Chinese and English. https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-lee-s-open-source-01-ai-bests-llama-2-according-to-hugging-face
"<a target=""_blank"" href=""https://huggingface.co/AIDC-AI/Marco-o1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">AIDC-AI/Marco-o1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/AIDC-AI__Marco-o1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",45,AIDC-AI_Marco-o1_float16,float16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Qwen2ForCausalLM,AIDC-AI/Marco-o1,5e4deeeb286b7a2e35a6d16989e64df860f7f4e5,23.170441795344043,apache-2.0,705,7.616,True,False,False,True,0.7864352872257779,0.477083028586373,47.7083028586373,0.5364362696398749,34.84254498655976,0.1064954682779456,10.649546827794564,0.259228187919463,1.230425055928408,0.41384375,9.963802083333334,0.4116522606382978,34.62802895981088,False,False,2024-11-13,2025-01-31,0,AIDC-AI/Marco-o1,marco-o1,Alibaba,https://huggingface.co/AIDC-AI/Marco-o1,7.0,7000.0,"1,000:1",0.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2411.14405,Dense,"Reasoning. No evals. Qwen2-7B-Instruct with a combination of the filtered Open-O1 CoT dataset, Marco-o1 CoT dataset, and Marco-o1 Instruction dataset."
"<a target=""_blank"" href=""https://huggingface.co/CohereForAI/aya-expanse-32b"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">CohereForAI/aya-expanse-32b</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__aya-expanse-32b-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",140,CohereForAI_aya-expanse-32b_float16,float16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,CohereForCausalLM,CohereForAI/aya-expanse-32b,08b69cfa4240e2009c80ad304f000b491d1b8c38,29.391219089316227,cc-by-nc-4.0,208,32.296,True,False,False,True,5.517735099910102,0.7301737168490716,73.01737168490715,0.5648670099212114,38.70961143301418,0.1336858006042296,13.36858006042296,0.3255033557046979,10.067114093959727,0.3872708333333333,6.408854166666668,0.4129820478723404,34.77578309692671,False,True,2024-10-23,2024-10-24,0,CohereForAI/aya-expanse-32b,aya expanse 32b,Cohere,https://huggingface.co/CohereForAI/aya-expanse-32b,32.0,8000.0,250:1,1.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Oct/2024,🟢,https://cohere.com/blog/aya-expanse-connecting-our-world,Dense,"""Aya Expanse, a family of highly performant multilingual models that excels across 23 languages and outperforms other leading open-weights models...we have collaborated with over 3,000 researchers from 119 countries to expand cutting-edge multilingual research... 220 language ambassadors from around the world who have been part of this release"""
"<a target=""_blank"" href=""https://huggingface.co/CohereForAI/c4ai-command-r-plus"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">CohereForAI/c4ai-command-r-plus</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__c4ai-command-r-plus-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",142,CohereForAI_c4ai-command-r-plus_float16,float16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,CohereForCausalLM,CohereForAI/c4ai-command-r-plus,fa1bd7fb1572ceb861bbbbecfa8af83b29fa8cca,30.96124684625396,cc-by-nc-4.0,1706,103.811,True,False,False,True,28.631531711670775,0.7664186580495308,76.64186580495308,0.581542357407793,39.91995423143177,0.0815709969788519,8.157099697885197,0.3053691275167785,7.38255033557047,0.48071875,20.42317708333333,0.3991855053191489,33.242833924349874,False,True,2024-04-03,2024-06-13,0,CohereForAI/c4ai-command-r-plus,c4ai command r plus,Cohere,https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus,104.0,4000.0,39:1,2.1,75.7,,,📚 🕸,Apr/2024,🟢,https://huggingface.co/CohereForAI/c4ai-command-r-plus,Dense,purpose-built to excel at real-world enterprise use cases. Announce with no arch details: https://txt.cohere.com/command-r-plus-microsoft-azure/
"<a target=""_blank"" href=""https://huggingface.co/EleutherAI/pythia-12b"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">EleutherAI/pythia-12b</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-12b-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",425,EleutherAI_pythia-12b_float16,float16,🟢 pretrained,🟢,Original,GPTNeoXForCausalLM,EleutherAI/pythia-12b,35c9d7f32fbb108fb8b5bdd574eb03369d1eed49,5.9339603247654615,apache-2.0,134,12.0,True,False,False,False,1.1180071531663731,0.2471475684517081,24.71475684517081,0.3179653957935337,4.987531038290507,0.0090634441087613,0.906344410876133,0.2466442953020134,0.0,0.3646979166666667,3.787239583333335,0.1108710106382978,1.2078900709219855,False,True,2023-02-28,2024-06-12,0,EleutherAI/pythia-12b,pythia 12b,EleutherAI,https://huggingface.co/EleutherAI/pythia-12b,12.0,300.0,25:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://arxiv.org/abs/2304.01373,Dense,
"<a target=""_blank"" href=""https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">HuggingFaceH4/zephyr-7b-alpha</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-7b-alpha-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",588,HuggingFaceH4_zephyr-7b-alpha_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,MistralForCausalLM,HuggingFaceH4/zephyr-7b-alpha,2ce2d025864af849b3e5029e2ec9d568eeda892d,18.571864220384587,mit,1104,7.242,True,False,False,True,0.7956751991931201,0.5191480826429429,51.91480826429429,0.4587863505904412,23.955291427068445,0.0173716012084592,1.7371601208459215,0.2978187919463087,6.375838926174497,0.3949583333333333,7.503125000000001,0.2795046542553192,19.94496158392435,False,True,2023-10-09,2024-06-12,1,mistralai/Mistral-7B-v0.1,zephyr 7b-alpha,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,7.3,800.0,110:1,0.3,,33.0,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,Dense,Mistral with 'aligned' data removed from dataset
"<a target=""_blank"" href=""https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-orpo-141b-A35b-v0.1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",591,HuggingFaceH4_zephyr-orpo-141b-A35b-v0.1_float16,float16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,MixtralForCausalLM,HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,a3be084543d278e61b64cd600f28157afc79ffd3,34.063022800582324,apache-2.0,265,140.621,True,False,False,True,42.06778642306777,0.6510891102275296,65.10891102275296,0.6290439728524093,47.5037962865412,0.2009063444108761,20.09063444108761,0.3783557046979866,17.114093959731544,0.4465208333333333,14.715104166666668,0.4586103723404255,39.84559692671394,False,True,2024-04-10,2024-06-12,1,mistral-community/Mixtral-8x22B-v0.1,zephyr orpo 141b a35b-v0.1,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,35.0,2000.0,58:1,0.9,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2403.07691,MoE,mixtral-8x22b finetune using Odds Ratio Preference Optimization (ORPO).
"<a target=""_blank"" href=""https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/LGAI-EXAONE__EXAONE-3.0-7.8B-Instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",833,LGAI-EXAONE_EXAONE-3.0-7.8B-Instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,ExaoneForCausalLM,LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,7f15baedd46858153d817445aff032f4d6cf4939,21.403463325745587,other,400,7.8,True,False,False,True,0.8251279774796095,0.7192826145737754,71.92826145737754,0.4174432647784512,17.97733539518049,0.0445619335347432,4.456193353474321,0.2659395973154362,2.1252796420581683,0.366125,3.298958333333333,0.3577127659574468,28.63475177304965,False,False,2024-07-31,2024-08-18,0,LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,exaone-3.0-7.8b-instruct,LG,https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,7.8,8000.0,"1,026:1",0.8,,27.4,10.1,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2408.03541,Dense,“EXAONE”=“EXpert AI for EveryONE”
"<a target=""_blank"" href=""https://huggingface.co/LLM360/K2"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">LLM360/K2</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/LLM360__K2-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",837,LLM360_K2_float16,float16,🟢 pretrained,🟢,Original,LlamaForCausalLM,LLM360/K2,49d159b6f2b64d562e745f0ff06e65b9a4c28ead,14.568224589032942,apache-2.0,86,65.286,True,False,False,False,8.838206417729227,0.2252157608478836,22.52157608478836,0.4971835676523677,28.220402834201128,0.0226586102719033,2.2658610271903328,0.2768456375838926,3.5794183445190177,0.3979999999999999,8.550000000000004,0.3004488031914893,22.27208924349881,False,True,2024-04-17,2024-06-26,0,LLM360/K2,k2,LLM360,https://huggingface.co/LLM360/K2,65.0,1400.0,22:1,1.0,64.8,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.llm360.ai/blog/several-new-releases-to-further-our-mission.html,Dense,"""K2-65B is a fully reproducible LLM outperforming Llama 2 70B using 35% less compute."""
"<a target=""_blank"" href=""https://huggingface.co/PrimeIntellect/INTELLECT-1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">PrimeIntellect/INTELLECT-1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/PrimeIntellect__INTELLECT-1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1144,PrimeIntellect_INTELLECT-1_bfloat16,bfloat16,🟢 pretrained,🟢,Original,LlamaForCausalLM,PrimeIntellect/INTELLECT-1,3b8d48b5ce11ee9526495f1db9eb1644518bfce0,3.806301801901869,apache-2.0,60,10.211,True,False,False,False,0.9953176565006856,0.1757315035217667,17.57315035217667,0.2759800780121471,1.043500157895444,0.0,0.0,0.2533557046979866,0.4474272930648763,0.3339375,2.408854166666666,0.112283909574468,1.3648788416075646,False,True,2024-11-28,2024-11-29,0,PrimeIntellect/INTELLECT-1,intellect-1,Prime Intellect,https://huggingface.co/PrimeIntellect/INTELLECT-1,10.0,1000.0,100:1,0.3,49.89,,28.32,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf,Dense,"Training complete 22/Nov/2024. Fully distributed training: ""the first decentralized training run of a 10-billion-parameter model, inviting anyone to contribute compute and participate. This brings us one step closer towards open source AGI."""
"<a target=""_blank"" href=""https://huggingface.co/Qwen/QwQ-32B-Preview"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">Qwen/QwQ-32B-Preview</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/Qwen__QwQ-32B-Preview-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1194,Qwen_QwQ-32B-Preview_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Qwen2ForCausalLM,Qwen/QwQ-32B-Preview,1032e81cb936c486aae1d33da75b2fbcd5deed4a,30.44412325748903,apache-2.0,1612,32.764,True,False,False,True,10.210389776239603,0.4035437084713006,40.354370847130056,0.6691381482252744,53.3876763517132,0.2288519637462235,22.88519637462236,0.2818791946308725,4.250559284116337,0.4109895833333333,9.80703125,0.5678191489361702,51.97990543735224,False,True,2024-11-27,2024-11-29,2,Qwen/Qwen2.5-32B,qwq-32b-preview,Alibaba,https://huggingface.co/spaces/Qwen/QwQ-32B-preview,32.0,18000.0,563:1,2.5,,,65.2,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://qwenlm.github.io/blog/qwq-32b-preview/,Dense,Reasoning. Scores 1/5 on latest ALPrompt 2024 H2. Qwen with Question=QwQ
"<a target=""_blank"" href=""https://huggingface.co/Qwen/Qwen2-72B-Instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">Qwen/Qwen2-72B-Instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-72B-Instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1218,Qwen_Qwen2-72B-Instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Qwen2ForCausalLM,Qwen/Qwen2-72B-Instruct,1af63c698f59c4235668ec9c1395468cb7cd7e79,42.91430415552085,other,703,72.706,True,False,False,False,37.55397442269999,0.7989168738945996,79.89168738945996,0.697730968386067,57.48300911876294,0.3768882175226586,37.68882175226586,0.3724832214765101,16.33109619686801,0.4560104166666667,17.167968749999996,0.5403091755319149,48.92324172576833,False,True,2024-05-28,2024-06-26,1,Qwen/Qwen2-72B,qwen2 72b-instruct,Alibaba,https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct,72.0,7000.0,98:1,2.4,84.2,55.6,37.9,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2407.10671,Dense,Instruct MMLU=82. Instruct GPQA=41.9. https://qwenlm.github.io/blog/qwen2/
"<a target=""_blank"" href=""https://huggingface.co/Qwen/Qwen2.5-72B-Instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">Qwen/Qwen2.5-72B-Instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-72B-Instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1238,Qwen_Qwen2.5-72B-Instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Qwen2ForCausalLM,Qwen/Qwen2.5-72B-Instruct,a13fff9ad76700c7ecff2769f75943ba8395b4a7,38.21208126161476,other,721,72.706,True,False,False,True,33.00676832439911,0.863837949972739,86.3837949972739,0.7272747321744824,61.87325566878789,0.0120845921450151,1.2084592145015105,0.375,16.666666666666664,0.4206041666666666,11.742187500000004,0.5625831117021277,51.39812352245864,False,True,2024-09-16,2024-10-16,1,Qwen/Qwen2.5-72B,qwen2.5 72b-instruct,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,32.5,5500.0,170:1,1.4,79.1,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2412.15115,Dense,https://qwenlm.github.io/blog/qwen2.5-coder-family/ Jack Clark from Anthropic is saying it’s actually 18T tokens from Qwen2.5 + 5.5T tokens for a total of 23.5T tokens. That doesn’t seem right from my interpretation of the technical report.
"<a target=""_blank"" href=""https://huggingface.co/Rakuten/RakutenAI-7B"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">Rakuten/RakutenAI-7B</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/Rakuten__RakutenAI-7B-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1258,Rakuten_RakutenAI-7B_float16,float16,🔶 fine-tuned on domain-specific datasets,🔶,Original,MistralForCausalLM,Rakuten/RakutenAI-7B,c687b10cbf1aa6c34868904b62ecfcef2e0946bf,11.546978376746557,apache-2.0,47,7.373,True,False,False,False,0.641848681233742,0.1555971488982566,15.559714889825662,0.4314905261361543,20.98205231291448,0.0196374622356495,1.9637462235649543,0.2894295302013422,5.257270693512303,0.3738125,4.659895833333335,0.2877327127659574,20.859190307328607,False,False,2024-03-18,2024-09-06,1,Rakuten/RakutenAI-7B (Merge),rakutenai-7b,Rakuten Group,https://huggingface.co/Rakuten/RakutenAI-7B,7.0,3000.0,429:1,0.5,61.31,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.15484,Dense,Japanese. Mistral 7B derivative.
"<a target=""_blank"" href=""https://huggingface.co/TencentARC/LLaMA-Pro-8B"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">TencentARC/LLaMA-Pro-8B</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/TencentARC__LLaMA-Pro-8B-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1496,TencentARC_LLaMA-Pro-8B_bfloat16,bfloat16,🔶 fine-tuned on domain-specific datasets,🔶,Original,LlamaForCausalLM,TencentARC/LLaMA-Pro-8B,7115e7179060e0623d1ee9ff4476faed7e478d8c,8.778934275693588,llama2,170,8.357,True,False,False,False,47.80773355108649,0.2277135777514772,22.77135777514772,0.3484197711435169,9.2939499758607,0.0166163141993957,1.6616314199395772,0.2600671140939597,1.342281879194629,0.4018124999999999,8.593229166666669,0.1811003989361702,9.011155437352246,False,True,2024-01-05,2024-06-12,0,TencentARC/LLaMA-Pro-8B,llama pro-8b,Tencent,https://huggingface.co/TencentARC/LLaMA-Pro-8B,8.3,2080.0,251:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.02415,Dense,We pre-train LLAMA PRO’s expanded blocks on 80B tokens using open-source code and math data for 2830 GPU Hours (16 NVIDIA H800 GPUs for about 7 days).
"<a target=""_blank"" href=""https://huggingface.co/WizardLMTeam/WizardLM-70B-V1.0"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">WizardLMTeam/WizardLM-70B-V1.0</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/WizardLMTeam__WizardLM-70B-V1.0-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1641,WizardLMTeam_WizardLM-70B-V1.0_float16,float16,🔶 fine-tuned on domain-specific datasets,🔶,Original,LlamaForCausalLM,WizardLMTeam/WizardLM-70B-V1.0,54aaecaff7d0790eb9f0ecea1cc267a94cc66949,22.359677748876223,llama2,235,70.0,True,False,False,False,29.09606329195423,0.4951428875383981,49.51428875383981,0.5590366047184262,37.54335453368136,0.0370090634441087,3.700906344410877,0.2659395973154362,2.1252796420581683,0.4391145833333333,14.089322916666667,0.3446642287234042,27.184914302600472,False,True,2023-08-09,2024-06-12,0,WizardLMTeam/WizardLM-70B-V1.0,wizardlm 70b v1.0,Microsoft,https://huggingface.co/WizardLM/WizardLM-70B-V1.0,70.0,2000.0,29:1,1.2,,,,🆆 📚⬆ 🕸 👥,Aug/2023,🟢,https://github.com/nlpxucan/WizardLM,Dense,Assume Llama-2 fine-tune. Outperforms text-davinci-003. May merge this entry with the Apr/2023 7B release
"<a target=""_blank"" href=""https://huggingface.co/ai21labs/Jamba-v0.1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">ai21labs/Jamba-v0.1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/ai21labs__Jamba-v0.1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1721,ai21labs_Jamba-v0.1_bfloat16,bfloat16,🟢 pretrained,🟢,Original,JambaForCausalLM,ai21labs/Jamba-v0.1,ce13f3fe99555a2606d1892665bb67649032ff2d,9.14283638861454,apache-2.0,1179,51.57,True,True,False,True,10.11214260696736,0.2025592095639569,20.2559209563957,0.3602260245164572,10.722058918870276,0.0113293051359516,1.1329305135951662,0.2684563758389262,2.460850111856823,0.3590208333333333,3.7109375,0.2491688829787234,16.574320330969268,False,True,2024-03-28,2024-09-16,0,ai21labs/Jamba-v0.1,jamba-v0.1,AI21,https://huggingface.co/ai21labs/Jamba-v0.1,52.0,5000.0,97:1,1.7,67.4,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.19887,MoE,"MoE. Open weights, licensed under Apache 2.0. Announce: https://arxiv.org/abs/2403.19887"
"<a target=""_blank"" href=""https://huggingface.co/amd/AMD-Llama-135m"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">amd/AMD-Llama-135m</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/amd__AMD-Llama-135m-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1841,amd_AMD-Llama-135m_float16,float16,🔶 fine-tuned on domain-specific datasets,🔶,Original,LlamaForCausalLM,amd/AMD-Llama-135m,8f9c39b5ed86d422ab332ed1ecf042fdaeb57903,4.759627159992882,apache-2.0,111,0.135,True,False,False,False,0.1287191757663833,0.1842245242622907,18.42245242622907,0.2973931917569524,2.4854950529752244,0.0052870090634441,0.5287009063444109,0.2525167785234899,0.3355704697986553,0.3779687499999999,4.912760416666667,0.1168550531914893,1.8727836879432624,False,True,2024-07-19,2024-09-29,0,amd/AMD-Llama-135m,amd llama-135m,AMD,https://huggingface.co/amd/AMD-Llama-135m,0.135,670.0,"4,963:1",0.0,23.02,,,📚 🌋,Sep/2024,🟢,https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html,Dense,"Small language model (SLM). Trained on AMD Instinct™ MI250 accelerators. ""Pretrain Dataset: We employed the SlimPajama and Project Gutenberg dataset to pretrain the 135M model. Project Gutenberg is a library of over 70,000 free eBooks approximately. This sums up to 670B tokens"""
"<a target=""_blank"" href=""https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">berkeley-nest/Starling-LM-7B-alpha</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/berkeley-nest__Starling-LM-7B-alpha-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",1918,berkeley-nest_Starling-LM-7B-alpha_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,MistralForCausalLM,berkeley-nest/Starling-LM-7B-alpha,1dddf3b95bc1391f6307299eb1c162c194bde9bd,20.826772930450105,apache-2.0,557,7.242,True,False,False,True,0.5516288866920487,0.5480491761858536,54.80491761858535,0.4440065261164004,21.95402808715926,0.0830815709969788,8.308157099697885,0.296979865771812,6.263982102908276,0.4120104166666666,9.501302083333334,0.3171542553191489,24.128250591016545,False,True,2023-11-25,2024-06-12,0,berkeley-nest/Starling-LM-7B-alpha,starling-lm-7b-alpha,Berkeley,https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha,7.0,2000.0,286:1,0.4,,37.9,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://starling.cs.berkeley.edu/,Dense,Llama 2 7B -> OpenChat 7B -> Starling-7B (RLAIF)
"<a target=""_blank"" href=""https://huggingface.co/databricks/dbrx-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">databricks/dbrx-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/databricks__dbrx-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2057,databricks_dbrx-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,DbrxForCausalLM,databricks/dbrx-instruct,c0a9245908c187da8f43a81e538e67ff360904ea,25.19901027244322,other,1110,131.597,True,False,False,True,47.958027273119946,0.5415796752616391,54.15796752616392,0.5428960796934387,35.96381960359357,0.0687311178247734,6.873111782477341,0.3414429530201342,12.192393736017896,0.4269270833333333,12.19921875,0.3682679521276595,29.807550236406616,False,True,2024-03-26,2024-06-12,0,databricks/dbrx-instruct,dbrx-instruct,MosaicML,https://huggingface.co/spaces/databricks/dbrx-instruct,132.0,12000.0,91:1,4.2,73.7,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,MoE,"MoE. Trained for $10M on 3,072 NVIDIA H100s connected by 3.2Tbps Infiniband."
"<a target=""_blank"" href=""https://huggingface.co/databricks/dolly-v2-12b"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">databricks/dolly-v2-12b</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/databricks__dolly-v2-12b-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2059,databricks_dolly-v2-12b_bfloat16,bfloat16,🔶 fine-tuned on domain-specific datasets,🔶,Original,GPTNeoXForCausalLM,databricks/dolly-v2-12b,19308160448536e378e3db21a73a751579ee7fdd,6.3830238203141,mit,1953,12.0,True,False,False,False,1.3971194622796634,0.2355073427394867,23.55073427394868,0.3319973167377127,6.377894137452961,0.0143504531722054,1.435045317220544,0.2407718120805369,0.0,0.37390625,5.504947916666668,0.1128656914893617,1.4295212765957446,False,True,2023-04-11,2024-06-12,0,databricks/dolly-v2-12b,dolly-v2-12b,Databricks,https://huggingface.co/databricks/dolly-v2-12b,12.0,300.0,25:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm,Dense,Fine-tuned Pythia 12B
"<a target=""_blank"" href=""https://huggingface.co/google/gemma-2-27b-it"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">google/gemma-2-27b-it</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-27b-it-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2256,google_gemma-2-27b-it_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Gemma2ForCausalLM,google/gemma-2-27b-it,f6c533e5eb013c7e31fc74ef042ac4f3fb5cf40b,32.32231876887986,gemma,518,27.227,True,False,False,True,4.826211186692737,0.7977677008116243,79.77677008116243,0.6451387433168799,49.27284215130387,0.0075528700906344,0.755287009063444,0.375,16.666666666666664,0.4033020833333333,9.112760416666667,0.4451462765957447,38.34958628841608,False,True,2024-06-24,2024-08-07,1,google/gemma-2-27b,gemma 2-27b-it,Google DeepMind,https://huggingface.co/google/gemma-2-27b-it,27.0,13000.0,482:1,2.0,75.2,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,Dense,Announce: https://blog.google/technology/developers/google-gemma-2/
"<a target=""_blank"" href=""https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">ibm-granite/granite-3.0-3b-a800m-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-3b-a800m-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2371,ibm-granite_granite-3.0-3b-a800m-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,GraniteForCausalLM,ibm-granite/granite-3.0-3b-a800m-instruct,ab0c732243cfd50a601fa393dd46a2c5993746f7,13.66035997552182,apache-2.0,17,3.374,True,False,False,True,3.076680553126776,0.4298217618142085,42.98217618142085,0.3752780529173344,13.16300959501,0.0679758308157099,6.797583081570996,0.2810402684563758,4.138702460850116,0.3486666666666667,2.0833333333333326,0.2151761968085106,12.797355200945626,False,True,2024-10-03,2024-10-20,1,ibm-granite/granite-3.0-3b-a800m-instruct (Merge),granite-3.0-3b-a800m-instruct,IBM,https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct,3.0,10000.0,"3,334:1",0.6,50.16,20.51,26.85,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,http://ibm.biz/granite-report,MoE,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
"<a target=""_blank"" href=""https://huggingface.co/ibm-granite/granite-3.0-8b-base"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">ibm-granite/granite-3.0-8b-base</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-8b-base-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2372,ibm-granite_granite-3.0-8b-base_bfloat16,bfloat16,🟢 pretrained,🟢,Original,GraniteForCausalLM,ibm-granite/granite-3.0-8b-base,1edd1f646abfcd90ed5d6c0d9711fbb02c947884,21.65316004834632,apache-2.0,23,8.171,True,False,False,False,1.885655879319516,0.4583482936386566,45.83482936386566,0.4943760637365333,27.974358298982427,0.0989425981873111,9.894259818731118,0.3255033557046979,10.067114093959727,0.4081354166666666,10.450260416666664,0.3312832446808511,25.69813829787234,False,True,2024-10-02,2024-10-20,0,ibm-granite/granite-3.0-8b-base,granite 3.0 8b base,IBM,https://huggingface.co/ibm-granite/granite-3.0-8b-base,8.0,12000.0,"1,500:1",1.0,65.54,33.27,32.13,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,http://ibm.biz/granite-report,Dense,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
"<a target=""_blank"" href=""https://huggingface.co/ibm-granite/granite-3.1-8b-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">ibm-granite/granite-3.1-8b-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.1-8b-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2381,ibm-granite_granite-3.1-8b-instruct_float16,float16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,GraniteForCausalLM,ibm-granite/granite-3.1-8b-instruct,f6749f3946b2dc9983b870317a71ddf7a65c0806,30.552690540891803,apache-2.0,145,8.171,True,False,False,True,0.6178333438185504,0.7207564816908026,72.07564816908027,0.5364460433816018,34.089655299414055,0.2167673716012084,21.676737160120847,0.3120805369127516,8.277404921700223,0.4707083333333333,19.00520833333333,0.3537234042553192,28.191489361702125,False,True,2024-12-06,2024-12-16,1,ibm-granite/granite-3.1-8b-instruct (Merge),granite 3.1 8b instruct,IBM,https://huggingface.co/ibm-granite/granite-3.1-8b-instruct,8.0,12000.0,"1,500:1",1.0,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://github.com/ibm-granite/granite-3.1-language-models?tab=readme-ov-file,Dense,
"<a target=""_blank"" href=""https://huggingface.co/internlm/internlm2_5-20b-chat"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">internlm/internlm2_5-20b-chat</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2_5-20b-chat-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2453,internlm_internlm2_5-20b-chat_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,InternLM2ForCausalLM,internlm/internlm2_5-20b-chat,ef17bde929761255fee76d95e2c25969ccd93b0d,32.08201273924976,other,89,19.86,True,False,False,True,3.732707871332407,0.7009977969565198,70.09977969565199,0.7473580533672884,62.83245915287989,0.0,0.0,0.3213087248322148,9.50782997762864,0.4558229166666667,16.744531249999994,0.3997672872340425,33.30747635933806,False,True,2024-07-30,2024-08-12,0,internlm/internlm2_5-20b-chat,internlm2_5 20b-chat,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm2_5-20b-chat,20.0,2600.0,130:1,0.8,73.5,,38.4,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_7b.md,Dense,"""The release of InternLM2.5 series contains 7B model size for now and we are going to release the 1.8B and 20B versions soon"" [20B released around 1/Aug/2024]"
"<a target=""_blank"" href=""https://huggingface.co/kyutai/helium-1-preview-2b"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">kyutai/helium-1-preview-2b</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/kyutai__helium-1-preview-2b-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2758,kyutai_helium-1-preview-2b_bfloat16,bfloat16,🟢 pretrained,🟢,Original,HeliumForCausalLM,kyutai/helium-1-preview-2b,dab850c85de673482dbf28b873064a274583e3b3,9.253614899505555,cc-by-4.0,133,2.173,True,False,False,False,0.3440807806955423,0.2613609666795214,26.136096667952145,0.3638164815956466,10.945139672307278,0.0090634441087613,0.906344410876133,0.2785234899328859,3.803131991051453,0.3549583333333333,4.036458333333333,0.187250664893617,9.694518321513002,False,True,2025-01-13,2025-01-14,0,kyutai/helium-1-preview-2b,helium-1-preview-2b,Kyutai,https://huggingface.co/kyutai/helium-1-preview-2b,2.0,2500.0,"1,250:1",0.2,51.2,,,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://kyutai.org/2025/01/13/helium.html,Dense,"""Helium-1 preview, an initial version of our new backbone language model with 2B parameters, targeting edge and mobile devices... We use token level distillation of a 7B parameters model to train Helium-1 preview."""
"<a target=""_blank"" href=""https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">meta-llama/Llama-3.3-70B-Instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-3.3-70B-Instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2873,meta-llama_Llama-3.3-70B-Instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,LlamaForCausalLM,meta-llama/Llama-3.3-70B-Instruct,,36.82884103840853,llama3.3,1908,70.554,True,False,False,True,38.27953705372647,0.8997581971391464,89.97581971391463,0.6919312828325811,56.561410788022194,0.0022658610271903,0.2265861027190332,0.3288590604026846,10.514541387024613,0.446125,15.565625,0.5331615691489362,48.12906323877069,False,True,2024-11-26,2024-12-03,1,meta-llama/Llama-3.3-70B-Instruct (Merge),llama 3.3 70b-instruct,Meta AI,https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct,70.0,15000.0,215:1,3.4,86.0,68.9,50.5,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,Dense,"Drop-in replacement for Llama 3.1 70B, comparable performance to Llama 3.1 405B."
"<a target=""_blank"" href=""https://huggingface.co/microsoft/Phi-3-medium-128k-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/Phi-3-medium-128k-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-medium-128k-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2887,microsoft_Phi-3-medium-128k-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Phi3ForCausalLM,microsoft/Phi-3-medium-128k-instruct,fa7d2aa4f5ea69b2e36b20d050cdae79c9bfbb3f,31.711653255665578,mit,378,13.96,True,False,False,True,1.947558744526168,0.6040029344361849,60.400293443618494,0.6382322530870549,48.46045127399018,0.1729607250755287,17.29607250755287,0.3364093959731543,11.521252796420578,0.4129479166666667,11.351822916666665,0.4711602393617021,41.24002659574468,False,True,2024-05-07,2024-08-21,0,microsoft/Phi-3-medium-128k-instruct,phi 3 medium 128k instruct,Microsoft,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,14.0,4800.0,343:1,0.9,78.2,55.7,,⚛️,Apr/2024,🟢,https://arxiv.org/abs/2404.14219,Dense,"Preview only, benchmarks being investigated as of May/2024."
"<a target=""_blank"" href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/Phi-3-mini-128k-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-mini-128k-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2889,microsoft_Phi-3-mini-128k-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Phi3ForCausalLM,microsoft/Phi-3-mini-128k-instruct,5be6479b4bc06a081e8f4c6ece294241ccd32dec,25.62628727325536,mit,1632,3.821,True,False,False,True,24.22225159268882,0.5976331688807919,59.76331688807919,0.5574531792679852,37.09976663224031,0.0974320241691843,9.74320241691843,0.3179530201342282,9.060402684563762,0.3936875,7.710937500000003,0.3734208776595745,30.38009751773049,False,True,2024-04-22,2024-08-21,0,microsoft/Phi-3-mini-128k-instruct,phi 3-mini-128k-instruct,Microsoft,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,3.8,3300.0,869:1,0.4,68.8,45.7,,⚛️,Apr/2024,🟢,https://arxiv.org/abs/2404.14219,Dense,"""phi3-mini can be quantized to 4-bits so that it only occupies ≈ 1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running natively on-device and fully offline achieving more than 12 tokens per second."""
"<a target=""_blank"" href=""https://huggingface.co/microsoft/Phi-3.5-MoE-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/Phi-3.5-MoE-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3.5-MoE-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2894,microsoft_Phi-3.5-MoE-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Phi3ForCausalLM,microsoft/Phi-3.5-MoE-instruct,482a9ba0eb0e1fa1671e3560e009d7cec2e5147c,35.45650752160648,mit,551,42.0,True,True,False,True,4.632278613944312,0.692454908531585,69.2454908531585,0.640762564622586,48.77464635932187,0.2265861027190332,22.658610271903324,0.3557046979865771,14.093959731543624,0.4564791666666667,17.326562499999998,0.4657579787234042,40.639775413711575,False,True,2024-08-17,2024-08-21,0,microsoft/Phi-3.5-MoE-instruct,phi 3.5 moe-instruct,Microsoft,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,60.0,4900.0,82:1,1.8,78.9,54.3,36.8,⚛️,Aug/2024,🟢,https://arxiv.org/abs/2407.13833https://arxiv.org/abs/2407.13833,MoE,
"<a target=""_blank"" href=""https://huggingface.co/microsoft/Phi-3.5-mini-instruct"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/Phi-3.5-mini-instruct</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3.5-mini-instruct-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2895,microsoft_Phi-3.5-mini-instruct_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,Phi3ForCausalLM,microsoft/Phi-3.5-mini-instruct,64963004ad95869fa73a30279371c8778509ac84,27.567573468796144,mit,799,3.821,True,False,False,True,3.696004302337374,0.5774500547436359,57.74500547436358,0.5517785126111956,36.74585390851661,0.1593655589123867,15.93655589123867,0.3397651006711409,11.968680089485462,0.402125,10.098958333333334,0.3961934840425531,32.91038711583924,False,True,2024-08-16,2024-08-21,0,microsoft/Phi-3.5-mini-instruct,phi 3.5-mini-instruct,Microsoft,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,3.8,3400.0,895:1,0.4,69.0,47.4,30.4,⚛️,Aug/2024,🟢,https://arxiv.org/abs/2407.13833,Dense,
"<a target=""_blank"" href=""https://huggingface.co/microsoft/phi-1_5"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/phi-1_5</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-1_5-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2897,microsoft_phi-1_5_float16,float16,🟢 pretrained,🟢,Original,PhiForCausalLM,microsoft/phi-1_5,675aa382d814580b22651a30acb1a585d7c25963,7.057673794439714,mit,1323,1.418,True,False,False,False,0.3408620690576331,0.2032839532440591,20.32839532440591,0.3359758321199665,7.468938770070243,0.0113293051359516,1.1329305135951662,0.2676174496644295,2.348993288590602,0.3404166666666666,3.385416666666666,0.1691323138297872,7.681368203309693,False,True,2023-09-10,2024-06-09,0,microsoft/phi-1_5,phi 1_5,Microsoft,https://huggingface.co/microsoft/phi-1_5,1.3,150.0,116:1,0.0,,,,📚⚛️,Sep/2023,🟢,https://arxiv.org/abs/2309.05463,Dense,Textbooks only. 30B-token dataset
"<a target=""_blank"" href=""https://huggingface.co/microsoft/phi-2"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/phi-2</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-2-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2898,microsoft_phi-2_float16,float16,🟢 pretrained,🟢,Original,PhiForCausalLM,microsoft/phi-2,ef382358ec9e382308935a992d908de099b64c23,15.47135097412628,mit,3271,2.78,True,False,False,False,0.423520982590239,0.273875539125077,27.3875539125077,0.4881208771249696,28.038519293439304,0.0256797583081571,2.56797583081571,0.2718120805369127,2.9082774049217,0.4098958333333333,13.83697916666666,0.2627992021276595,18.088800236406616,False,True,2023-12-13,2024-06-09,0,microsoft/phi-2,phi 2,Microsoft,https://replicate.com/lucataco/phi-2,2.7,1400.0,519:1,0.2,,,,⚛️,Nov/2023,🟢,https://huggingface.co/microsoft/phi-2,Dense,https://twitter.com/SebastienBubeck/status/1724854157004190095
"<a target=""_blank"" href=""https://huggingface.co/microsoft/phi-4"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">microsoft/phi-4</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-4-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2899,microsoft_phi-4_float16,float16,🟢 pretrained,🟢,Original,Phi3ForCausalLM,microsoft/phi-4,381727a5ee103da6c1b14ecd3d39cd09832cbcf8,29.48341679122318,mit,1715,14.66,True,False,False,False,0.8783618221059865,0.0487850015736024,4.878500157360248,0.6703464626619114,52.57567157258284,0.2787009063444108,27.870090634441087,0.401006711409396,20.134228187919465,0.5033541666666667,23.71927083333333,0.5295046542553191,47.722739361702125,False,True,2024-12-11,2025-01-08,0,microsoft/phi-4,phi-4,Microsoft,https://huggingface.co/microsoft/phi-4,14.0,10000.0,715:1,1.2,84.8,70.4,56.1,⚛️,Dec/2024,🟢,https://arxiv.org/abs/2412.08905,Dense,Use unsloth: https://huggingface.co/unsloth/phi-4-GGUF & https://www.reddit.com/r/singularity/comments/1i0kso4/i_fixed_4_bugs_in_microsofts_opensource_phi4_model/
"<a target=""_blank"" href=""https://huggingface.co/mistralai/Codestral-22B-v0.1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">mistralai/Codestral-22B-v0.1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Codestral-22B-v0.1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2914,mistralai_Codestral-22B-v0.1_bfloat16,bfloat16,🟢 pretrained,🟢,Original,MistralForCausalLM,mistralai/Codestral-22B-v0.1,8f5fe23af91885222a1563283c87416745a5e212,23.27991740686463,other,1213,22.247,True,False,False,True,1.3066695609381,0.5771752283939946,57.71752283939946,0.5139136921003167,30.737634411945635,0.100453172205438,10.045317220543806,0.2986577181208054,6.487695749440718,0.4187083333333333,10.738541666666668,0.3155751329787234,23.95279255319149,False,True,2024-05-29,2024-09-28,0,mistralai/Codestral-22B-v0.1,codestral 22b v0.1,Mistral,https://huggingface.co/mistralai/Codestral-22B-v0.1,22.0,2000.0,91:1,0.7,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://mistral.ai/news/codestral/,Dense,Fluent in 80+ programming languages
"<a target=""_blank"" href=""https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">mistralai/Ministral-8B-Instruct-2410</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Ministral-8B-Instruct-2410-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2915,mistralai_Ministral-8B-Instruct-2410_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,MistralForCausalLM,mistralai/Ministral-8B-Instruct-2410,199e57c1d66379760f6413f79d27008d1d1dbd6e,22.007858930308583,other,422,8.02,True,False,False,True,0.797086392156932,0.5896399331551394,58.963993315513946,0.4761640201689138,25.82477440941784,0.0649546827794562,6.495468277945619,0.2843959731543624,4.586129753914992,0.41375,10.718750000000004,0.3291223404255319,25.458037825059098,False,True,2024-10-15,2024-12-01,0,mistralai/Ministral-8B-Instruct-2410,ministral 8b instruct-2410,Mistral,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,8.0,6000.0,750:1,0.7,65.0,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://mistral.ai/news/ministraux/,Dense,"""Introducing the world’s best edge models"""
"<a target=""_blank"" href=""https://huggingface.co/mistralai/Mistral-Nemo-Base-2407"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">mistralai/Mistral-Nemo-Base-2407</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Nemo-Base-2407-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2922,mistralai_Mistral-Nemo-Base-2407_bfloat16,bfloat16,🟢 pretrained,🟢,Original,MistralForCausalLM,mistralai/Mistral-Nemo-Base-2407,d2efb15544d5401f761235bef327babb850887d0,15.138651108214134,apache-2.0,291,11.58,True,False,False,False,1.7029954549103143,0.1629919724109806,16.299197241098064,0.5035062000369291,29.374736440966878,0.0536253776435045,5.362537764350454,0.2936241610738255,5.8165548098434,0.3921354166666667,6.516927083333336,0.347157579787234,27.46195330969267,False,True,2024-07-18,2024-07-19,0,mistralai/Mistral-Nemo-Base-2407,mistral nemo base-2407,Mistral,https://huggingface.co/mistralai/Mistral-Nemo-Base-2407,12.0,2000.0,167:1,0.5,68.0,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://mistral.ai/news/mistral-nemo/,Dense,"With NVIDIA. ""Drop-in replacement of Mistral 7B"". ""trained using Megatron-LM, part of NVIDIA NeMo, with 3,072 H100 80GB Tensor Core GPUs"" https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/"
"<a target=""_blank"" href=""https://huggingface.co/mistralai/Mixtral-8x22B-v0.1"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">mistralai/Mixtral-8x22B-v0.1</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x22B-v0.1-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2928,mistralai_Mixtral-8x22B-v0.1_bfloat16,bfloat16,🟢 pretrained,🟢,Original,MixtralForCausalLM,mistralai/Mixtral-8x22B-v0.1,b03e260818710044a2f088d88fab12bb220884fb,25.72834815840493,apache-2.0,210,140.621,True,True,False,False,104.69731627511148,0.2582636293922348,25.82636293922349,0.6239807473187268,45.58840384342722,0.1827794561933535,18.277945619335352,0.3758389261744966,16.778523489932887,0.4036979166666667,7.462239583333335,0.4639295212765957,40.4366134751773,False,True,2024-04-16,2024-06-12,0,mistralai/Mixtral-8x22B-v0.1,mixtral 8x22b-v0.1,Mistral,https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1,141.0,2000.0,15:1,1.8,77.75,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://mistral.ai/news/mixtral-8x22b/,MoE,"MoE=22Bx8, seq=65536."
"<a target=""_blank"" href=""https://huggingface.co/mosaicml/mpt-7b"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">mosaicml/mpt-7b</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/mosaicml__mpt-7b-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",2960,mosaicml_mpt-7b_bfloat16,bfloat16,🟢 pretrained,🟢,Original,MPTForCausalLM,mosaicml/mpt-7b,039e37745f00858f0e01e988383a8c4393b1a4f5,5.994264988690563,apache-2.0,1164,7.0,True,False,False,False,0.6435034251930756,0.2151990053059216,21.51990053059216,0.3299741596080132,6.550600790794161,0.0135951661631419,1.3595166163141996,0.2600671140939597,1.342281879194629,0.3672395833333333,2.904947916666668,0.120595079787234,2.288342198581559,False,True,2023-05-05,2024-06-08,0,mosaicml/mpt-7b,mpt-7b,MosaicML,https://huggingface.co/mosaicml/mpt-7b,7.0,1000.0,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://twitter.com/NaveenGRao/status/1654496162492084227,Dense,Llongboi' -Apache 2.0 license suitable for commercial use. -Base 7B LLM trained on 1T tokens outperforms LLaMA and GPT3. -64K+ context length. -$200k to train from scratch.
"<a target=""_blank"" href=""https://huggingface.co/nvidia/Minitron-4B-Base"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">nvidia/Minitron-4B-Base</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Minitron-4B-Base-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3120,nvidia_Minitron-4B-Base_bfloat16,bfloat16,🟢 pretrained,🟢,Original,NemotronForCausalLM,nvidia/Minitron-4B-Base,d6321f64412982046a32d761701167e752fedc02,11.939972705176745,other,129,4.0,True,False,False,False,1.1892668131564303,0.2217937295265451,22.17937295265451,0.4083876243992497,17.215600655061085,0.0173716012084592,1.7371601208459215,0.2692953020134228,2.572706935123044,0.413375,9.938541666666667,0.261968085106383,17.99645390070922,False,True,2024-07-19,2024-09-25,0,nvidia/Minitron-4B-Base,minitron-4b-base,NVIDIA,https://huggingface.co/nvidia/Minitron-4B-Base,4.0,94.0,24:1,0.1,58.6,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2407.14679,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
"<a target=""_blank"" href=""https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">nvidia/Mistral-NeMo-Minitron-8B-Base</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Mistral-NeMo-Minitron-8B-Base-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3122,nvidia_Mistral-NeMo-Minitron-8B-Base_bfloat16,bfloat16,🔶 fine-tuned on domain-specific datasets,🔶,Original,MistralForCausalLM,nvidia/Mistral-NeMo-Minitron-8B-Base,cc94637b669b62c4829b1e0c3b9074fecd883b74,17.660161507076435,other,170,7.88,True,False,False,False,3.404028320245036,0.1945659738383045,19.456597383830456,0.5219098090521418,31.822015157490156,0.04607250755287,4.607250755287009,0.3255033557046979,10.067114093959727,0.40915625,8.944531250000002,0.3795711436170212,31.06346040189125,False,True,2024-08-19,2024-08-22,0,nvidia/Mistral-NeMo-Minitron-8B-Base,mistral nemo minitron-8b-base,NVIDIA,https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base,4.0,94.0,24:1,0.1,63.8,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
"<a target=""_blank"" href=""https://huggingface.co/openchat/openchat_3.5"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">openchat/openchat_3.5</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat_3.5-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3154,openchat_openchat_3.5_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,MistralForCausalLM,openchat/openchat_3.5,0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b,21.64841522838232,apache-2.0,1118,7.0,True,False,False,True,0.5012105911805675,0.5931118321608887,59.31118321608887,0.4426319686283289,21.58216684769999,0.073262839879154,7.326283987915408,0.2986577181208054,6.487695749440718,0.4228645833333333,11.258072916666668,0.3153257978723404,23.92508865248227,False,True,2023-10-30,2024-06-12,0,openchat/openchat_3.5,openchat_3.5,Tsinghua,https://huggingface.co/openchat/openchat_3.5,13.0,2000.0,154:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2022,🟢,https://arxiv.org/abs/2309.11235,Dense,Llama 2 13B -> OpenChat 13B
"<a target=""_blank"" href=""https://huggingface.co/tiiuae/Falcon3-10B-Base"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">tiiuae/Falcon3-10B-Base</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__Falcon3-10B-Base-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3603,tiiuae_Falcon3-10B-Base_bfloat16,bfloat16,🟢 pretrained,🟢,Original,LlamaForCausalLM,tiiuae/Falcon3-10B-Base,0b20cceec08ec598ed2de7a6dfbeb208f1eae656,27.592674645858228,other,33,10.306,True,False,False,False,0.8103894399819915,0.3647754624396601,36.47754624396601,0.595004253437141,41.37546218651794,0.2477341389728096,24.773413897280967,0.3456375838926174,12.751677852348994,0.4397916666666666,14.173958333333331,0.4240359042553192,36.003989361702125,False,True,2024-12-03,2024-12-12,0,tiiuae/Falcon3-10B-Base,falcon3 10b base,TII,https://huggingface.co/tiiuae/Falcon3-10B-Base,10.0,16000.0,"1,600:1",1.3,73.1,42.5,34.1,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://huggingface.co/blog/falcon3,Dense,"""We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens... upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data."""
"<a target=""_blank"" href=""https://huggingface.co/tiiuae/falcon-11B"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">tiiuae/falcon-11B</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-11B-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3613,tiiuae_falcon-11B_bfloat16,bfloat16,🟢 pretrained,🟢,Original,FalconForCausalLM,tiiuae/falcon-11B,066e3bf4e2d9aaeefa129af0a6d39727d27816b3,13.81413823572704,unknown,212,11.103,True,False,False,False,1.0828709914176635,0.3261324397044287,32.613243970442866,0.4391637035549384,21.937999462890275,0.0256797583081571,2.56797583081571,0.2709731543624161,2.796420581655479,0.3986458333333333,7.530729166666667,0.2389461436170212,15.438460401891252,False,True,2024-05-09,2024-06-09,0,tiiuae/falcon-11B,falcon 11b,TII,https://huggingface.co/tiiuae/falcon-11B,11.0,5500.0,500:1,0.8,58.37,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas,Dense,Announce: https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas
"<a target=""_blank"" href=""https://huggingface.co/togethercomputer/LLaMA-2-7B-32K"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">togethercomputer/LLaMA-2-7B-32K</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__LLaMA-2-7B-32K-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3622,togethercomputer_LLaMA-2-7B-32K_float16,float16,🔶 fine-tuned on domain-specific datasets,🔶,Original,LlamaForCausalLM,togethercomputer/LLaMA-2-7B-32K,46c24bb5aef59722fa7aa6d75e832afd1d64b980,6.737010933052859,llama2,538,7.0,True,False,False,False,0.5845727731386725,0.1864973825006538,18.649738250065383,0.3399517521730171,8.089984229889549,0.0083081570996978,0.8308157099697886,0.25,0.0,0.3753645833333333,4.320572916666666,0.1767785904255319,8.530954491725769,False,True,2023-07-26,2024-06-12,0,togethercomputer/LLaMA-2-7B-32K,llama-2-7b-32k,Together,https://huggingface.co/togethercomputer/LLaMA-2-7B-32K,7.0,2000.0,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://together.ai/blog/llama-2-7b-32k,Dense,32k context window instead of 4k (Llama 2)
"<a target=""_blank"" href=""https://huggingface.co/unsloth/phi-4"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">unsloth/phi-4</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/unsloth__phi-4-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3635,unsloth_phi-4_bfloat16,bfloat16,"💬 chat models (RLHF, DPO, IFT, ...)",💬,Original,LlamaForCausalLM,unsloth/phi-4,682399cd249206f583fc19473d5a28af0a9bcea7,34.48459834946983,mit,69,14.66,True,False,False,True,0.9432692633333016,0.6882083981613231,68.8208398161323,0.6885874406040138,55.25314499847013,0.1253776435045317,12.537764350453172,0.3364093959731543,11.521252796420578,0.4114270833333333,10.128385416666667,0.5378158244680851,48.64620271867612,False,False,2025-01-08,2025-01-09,1,microsoft/phi-4,phi-4,Microsoft,https://huggingface.co/microsoft/phi-4,14.0,10000.0,715:1,1.2,84.8,70.4,56.1,⚛️,Dec/2024,🟢,https://arxiv.org/abs/2412.08905,Dense,Use unsloth: https://huggingface.co/unsloth/phi-4-GGUF & https://www.reddit.com/r/singularity/comments/1i0kso4/i_fixed_4_bugs_in_microsofts_opensource_phi4_model/
"<a target=""_blank"" href=""https://huggingface.co/upstage/SOLAR-10.7B-v1.0"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">upstage/SOLAR-10.7B-v1.0</a>  <a target=""_blank"" href=""https://huggingface.co/datasets/open-llm-leaderboard/upstage__SOLAR-10.7B-v1.0-details"" style=""color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;"">📑</a>",3639,upstage_SOLAR-10.7B-v1.0_float16,float16,🟢 pretrained,🟢,Original,LlamaForCausalLM,upstage/SOLAR-10.7B-v1.0,a45090b8e56bdc2b8e32e46b3cd782fc0bea1fa5,4.916447886280902,apache-2.0,298,10.732,True,False,False,False,1.5191940183802333,0.171584728520326,17.158472852032606,0.2998351737549512,2.147162763818688,0.0234138972809667,2.3413897280966767,0.2609060402684564,1.45413870246085,0.3681979166666667,4.524739583333332,0.1168550531914893,1.8727836879432624,False,True,2023-12-12,2024-06-12,0,upstage/SOLAR-10.7B-v1.0,solar 10.7b v1.0,Upstage AI,https://huggingface.co/upstage/SOLAR-10.7B-v1.0,10.7,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.15166,Dense,South Korean. Llama-2 arch. SOTA for its size (Dec/2023).
