Model,Lab,Playground,"Parameters
(B)","Tokens
trained (B)","Ratio Tokens:Params
(Chinchilla scaling≥20:1)","ALScore
""ALScore"" is a quick and dirty rating of the model's power. The formula is:
Sqr Root of (Parameters x Tokens) ÷ 300.
Any ALScore ≥ 1.0 is a powerful model in mid-2023.",MMLU,"MMLU
-Pro",GPQA,Training dataset,"Announced
▼",Public?,Paper / Repo,Arch,Notes
WormGPT,(Undisclosed),,6,402,67:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟡,https://slashnext.com/blog/wormgpt-the-generative-ai-tool-cybercriminals-are-using-to-launch-business-email-compromise-attacks/,Dense,GPT-J (2021) finetune/module.
Yi-Lightning,01-ai,https://platform.lingyiwanwu.com/,200,10000,50:1,4.7,,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://platform.lingyiwanwu.com/docs#%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%AE%A1%E8%B4%B9,MoE,"""New MoE hybrid expert architecture"" and https://x.com/01AI_Yi/status/1845776529185476613"
Yi-Coder,01-ai,https://huggingface.co/collections/01-ai/yi-coder-66bdb00f5bdd611f9a008f30,9,6200,689:1,0.8,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md,Dense,"6B=3T tokens, 9B=+0.8T tokens, 9B-Coder=+2.4T tokens=6.2T tokens. See Yi 1.5 34B in this table"
Yi-XLarge,01-ai,https://platform.01.ai/,2000,20000,10:1,21.1,85.1,,48.2,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.aixinzhijie.com/article/6845768,MoE,"Still training as of May/2024: https://appserversrc.8btc.cn/FnDYlEC4STBhphu6M3NL4CKH43FW dead link, use: https://finance.china.com.cn/roll/20240513/6116857.shtml"
Yi-Large,01-ai,https://platform.01.ai/,1000,15000,15:1,12.9,83.8,58.1,43.5,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.aixinzhijie.com/article/6845768,Dense,
Yi 1.5 34B,01-ai,https://huggingface.co/01-ai/Yi-1.5-34B-Chat,34.4,3600,105:1,1.2,76.8,52.3,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://github.com/01-ai/Yi-1.5,Dense,Uses 600B more training tokens than Yi 1.0 (Nov/2023).
Yi-34B,01-ai,https://huggingface.co/01-ai/Yi-34B,34.4,3000,88:1,1.1,76.3,43,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://github.com/01-ai/Yi,Dense,Controversy about Llama 2 base. https://twitter.com/kaifulee/status/1724673131875377465 MMLU=76.3 (PaLM 2=78.3) Outperforms Llama 2. Chinese and English. https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-lee-s-open-source-01-ai-bests-llama-2-according-to-hugging-face
Zhinao (Intellectual Brain),360 cn,https://ai.360.com/,100,2000,20:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://arxiv.org/abs/2402.01723v1,Dense,
Fuyu-Heavy,Adept,,120,5000,42:1,2.6,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟡,https://www.adept.ai/blog/adept-fuyu-heavy,Dense,"Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger.' Token estimate is based on Adept Persimmon-8B using many more tokens."
Fuyu,Adept,https://huggingface.co/adept/fuyu-8b,8,,,,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://www.adept.ai/blog/fuyu-8b,Dense,"VLM. 8B available under open licence, Medium size is closed"
Persimmon-8B,Adept,https://www.adept.ai/blog/persimmon-8b,8,737,93:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://github.com/persimmon-ai-labs/adept-inference,Dense,Open Apache license and publicly accessible weights.
SEA-LIONv3,AI Singapore,https://huggingface.co/aisingapore/gemma2-9b-cpt-sea-lionv3-base,9.24,8200,888:1,0.9,,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://www.linkedin.com/posts/leslieteo01_ai-machinelearning-nlp-activity-7258042808891027456-Tqab/,Dense,SEA-LION is a collection of Large Language Models (LLMs) which has been pretrained and instruct-tuned for the Southeast Asia (SEA) region. The Gemma2 9B CPT SEA-LIONv3 base model which has undergone continued pre-training from the base Gemma-2-9B model. SEA-LION stands for Southeast Asian Languages In One Network.' News: https://www.techinasia.com/news/ai-singapore-boosts-sea-ai-sealion-v3-model
Sea-Lion,AI Singapore,https://aisingapore.org/aiproducts/sea-lion/,7.5,980,131:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://huggingface.co/aisingapore/sealion7b,Dense,"MPT base. MMLU=26.87. Southeast Asian languages like Thai, Vietnamese and Bahasa Indonesia. https://www.computerweekly.com/feature/Sea-Lion-explained-Southeast-Asias-first-large-language-model"
Jamba 1.5,AI21,https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251,398,8000,21:1,5.9,81.2,53.5,36.9,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2408.12570,MoE,"Jamba 1.5 Mini (12B active/52B total) and Jamba 1.5 Large (94B active/398B total) are also optimized for business use cases and capabilities such as function calling, structured output (JSON), and grounded generation."
Jamba 1,AI21,https://huggingface.co/ai21labs/Jamba-v0.1,52,5000,97:1,1.7,67.4,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.19887,MoE,"MoE. Open weights, licensed under Apache 2.0. Announce: https://arxiv.org/abs/2403.19887"
Jurassic-2,AI21,Studio,178,,,,,,,🆆 📚 ⬆ 🕸,Mar/2023,🟢,https://www.ai21.com/blog/introducing-j2,Dense,
J-1 RBG,AI21,ask-rbg.ai,178,300,2:1,0.8,,,,🆆 📚 ⬆ 🕸,Jul/2022,🟢,https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1,Dense,J-1 fine-tuned with RBG law corpus
Jurassic-1,AI21,Studio,178,300,2:1,0.8,,,,🆆 📚 ⬆ 🕸,Aug/2021,🟢,https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1,Dense,Emulated GPT-3 dataset
Weaver,AIWaves.cn,https://www.wawawriter.com/,34,2018,60:1,0.9,,,,📚,Jan/2024,🟢,https://arxiv.org/abs/2401.17268,Dense,Llama? 'All Weaver models are initialized from powerful open-source LLMs.' English waitlist: https://www.wawawriter.com/en/
aiXcoder-7B,aiXcoder,https://github.com/aixcoder-plugin/aixcoder-7b,7,1200,172:1,0.3,,,,🌋,Oct/2024,🟢,https://arxiv.org/abs/2410.13187v1,Dense,Dataset: The Stack
Pharia-1-LLM-7B,Aleph Alpha,https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,7,7700,"1,100:1",0.8,,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://aleph-alpha.com/introducing-pharia-1-llm-transparent-and-compliant/,Dense,
Luminous Supreme Control,Aleph Alpha,https://app.aleph-alpha.com/playground/completion,70,588,9:1,0.7,,,,🆆 📚⬆ 🕸 👥,Feb/2023,🟢,https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf,Dense,‘Control’ means instruction tuned
Luminous,Aleph Alpha,AA playground,200,,,,,,,🕸,Nov/2021,🟢,https://www.aleph-alpha.de/pricing,Dense,Devs from EleutherAI
Qwen2.5-Max,Alibaba,https://chat.qwenlm.ai/,325,20000,62:1,8.5,87.9,69,60.1,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://qwenlm.github.io/blog/qwen2.5-max/,MoE,"""Qwen2.5-Max emerges as a milestone in MoE development, featuring an impressive 325 billion parameters. The model has been pretrained on over 20 trillion tokens and further refined with advanced post-training methodologies such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF)."" https://wandb.ai/byyoung3/ml-news/reports/Qwen2-5-Max-Advancing-Large-Scale-Mixture-of-Expert-Models---VmlldzoxMTEyMjUyNg"
QwQ-32B,Alibaba,https://huggingface.co/spaces/Qwen/QwQ-32B-preview,32,18000,563:1,2.5,,,65.2,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://qwenlm.github.io/blog/qwq-32b-preview/,Dense,Reasoning. Scores 1/5 on latest ALPrompt 2024 H2. Qwen with Question=QwQ
Marco-o1,Alibaba,https://huggingface.co/AIDC-AI/Marco-o1,7,7000,"1,000:1",0.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2411.14405,Dense,"Reasoning. No evals. Qwen2-7B-Instruct with a combination of the filtered Open-O1 CoT dataset, Marco-o1 CoT dataset, and Marco-o1 Instruction dataset."
Qwen2.5-Coder,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,32.5,5500,170:1,1.4,79.1,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2412.15115,Dense,https://qwenlm.github.io/blog/qwen2.5-coder-family/ Jack Clark from Anthropic is saying it’s actually 18T tokens from Qwen2.5 + 5.5T tokens for a total of 23.5T tokens. That doesn’t seem right from my interpretation of the technical report.
Qwen2.5,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,72,18000,250:1,3.8,86.1,71.1,49,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://arxiv.org/abs/2412.15115,Dense,
Qwen2,Alibaba,https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct,72,7000,98:1,2.4,84.2,55.6,37.9,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2407.10671,Dense,Instruct MMLU=82. Instruct GPQA=41.9. https://qwenlm.github.io/blog/qwen2/
Qwen2-57B-A14B,Alibaba,https://github.com/QwenLM/Qwen2?tab=readme-ov-file,57,4500,79:1,1.7,76.5,43,34.3,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2407.10671,MoE,https://qwenlm.github.io/blog/qwen2/
Qwen-Max,Alibaba,https://chat.lmsys.org/,300,6000,20:1,4.5,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction,Dense,https://twitter.com/JustinLin610/status/1787584325367529509
Qwen-1.5 110B,Alibaba,https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo,111,3000,28:1,1.9,80.4,49.9,35.9,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://qwenlm.github.io/blog/qwen1.5-110b/,Dense,"Worse performance on GPQA (72B=36.3, 110B=35.9)."
Qwen1.5-MoE-A2.7B,Alibaba,https://qwenlm.github.io/blog/qwen-moe/,14.3,1500,105:1,0.5,62.5,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://qwenlm.github.io/blog/qwen-moe/,MoE,"MoE. ""Of particular significance is the fact that, through upcycling, the necessity for training an equivalent volume of tokens as in the original model has been eliminated."" I assumed half of the original 3T tokens"
Qwen-1.5 72B,Alibaba,https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat,72,3000,42:1,1.5,77.5,52.6,36.3,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://qwenlm.github.io/blog/qwen1.5/,Dense,
SeaLLM-13b,Alibaba,https://github.com/damo-nlp-sg/seallms,13,2000,154:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.00738,Dense,"Llama 2 for Southeast Asian (SEA) languages: Vietnamese 🇻🇳, Indonesian 🇮🇩, Thai 🇹🇭, Malay 🇲🇾, Khmer🇰🇭, Lao🇱🇦, Tagalog🇵🇭 and Burmese🇲🇲"
Qwen,Alibaba,https://huggingface.co/Qwen,72,3000,42:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2309.16609,Dense,Chinese. Full name is 'Tongyi Qianwen' 通义千问. 'Lags behind both GPT-3.5 and GPT-4'. Originally 7B/14B params Apr/2023
Llama-3.1-Tulu-3-405B,Allen AI,https://playground.allenai.org/,405,15600,39:1,8.4,87,,,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B,Dense,Lower MMLU score than Llama 3.1 405B base.
OLMo 2,Allen AI,https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc,13,5600,431:1,0.9,68.6,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2501.00656,Dense,Open Language Model (OLMo) 2 Apache 2.0 license for research and educational use. Paper coming. Data: 5 trillion tokens (1.2 epochs of 4T tokens) + 100B tokens (3 runs) + 300B tokens (1 run) merged. https://huggingface.co/allenai/OLMo-2-1124-13B & playground: https://playground.allenai.org/
TÜLU 3,Allen AI,https://playground.allenai.org/,70,15600,223:1,3.5,83.1,65.8,45.1,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://allenai.org/papers/tulu-3-report.pdf,Dense,"Llama 3.1 post-training, worse performance on most benchmarks. Post training methods include new Reinforcement Learning with Verifiable Rewards (RLVR). ""We perform supervised fine-tuning on new capability-focused synthetic data mixed with existing instruction datasets. We then perform preference tuning on on-policy synthetic preference data. We finish training Llama Tülu3 with our new method, Reinforcement Learning with Verifiable Rewards."""
Molmo,Allen AI,https://molmo.allenai.org/,72,7000,98:1,2.4,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://molmo.allenai.org/paper.pdf,Dense,ViT: Llava as Qwen2 (or Olmo) + CLIP. Multimodal Open Language Model built by Ai2. Announce: https://molmo.allenai.org/blog
OLMoE-1B-7B,Allen AI,https://huggingface.co/collections/allenai/olmoe-66cf678c047657a30c8cd3da,6.9,5900,856:1,0.7,54.1,,23,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://arxiv.org/abs/2409.02060v1,MoE,"Open Language (OL) Mixture of Experts (MoE). ""We train OLMoE-1B-7B for 5 trillion tokens, however, some recent dense models train significantly longer, such as Llama 3 with 15 trillion tokens. To the best of our knowledge, there has been no large MoE that has been overtrained as much as OLMoE-1B-7B. Specifically, taking the active parameters of OLMoE-1B-7B, our token multiplier is around 5,000 (5T / 1B). There are likely benefits to training even longer, but to what degree overtraining is effective for MoEs and how it differs from dense models still requires more research."""
OLMo,Allen AI,https://huggingface.co/allenai/OLMo-7B,7,2500,358:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://allenai.org/olmo/olmo-paper.pdf,Dense,Open Language Model (OLMo)
Unified-IO 2,Allen AI,https://unified-io-2.allenai.org/,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.17172,Dense,"600TB dataset (plus 120+ fine-tuning datasets) includes '1B imagetext pairs, 1T text tokens, 180M video clips, 130M interleaved image & text, 3M 3D assets, and 1M agent trajectories.'"
TÜLU 2,Allen AI,https://huggingface.co/allenai/tulu-2-dpo-70b,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://arxiv.org/abs/2311.10702,Dense,Llama 2 finetune with RLHF direct preference optimization (DPO).
Unified-IO,Allen AI,Limited demo,2.8,,,,,,,🆆 📚⬆ 🕸 🌋,Jun/2022,🔴,https://github.com/jiasenlu/unified-io/blob/main/UnifiedIOv1.pdf,Dense,Based on T5. Demo only
Macaw,Allen AI,Allen (static demo only),11,,,,,,,🆀🅰,Sep/2021,🟡,https://arxiv.org/abs/2109.02593,Dense,Chatbot
Nova Pro,Amazon,https://aws.amazon.com/bedrock/,90,10000,112:1,3.2,85.9,,46.9,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,Dense,"Multimodal, same performance as Llama 3.2 90B ∴ est 90B. Model card was hidden: https://assets.amazon.science/9f/a3/ae41627f4ab2bde091f1ebc6b830/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf via https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card"
HLAT,Amazon,,7,1800,258:1,0.4,41.318,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🔴,https://arxiv.org/abs/2404.10630,Dense,HLAT=High-quality LLM pre-trained on AWS Trainium. Same arch as Llama 7B. The pre-training is performed up to 64 Amazon EC2 trn1.32xlarge instances with totalling up to 1024 AWS Trainium accelerators. Read more about Trainium: https://www.aboutamazon.com/news/aws/what-you-need-to-know-about-the-aws-ai-chips-powering-amazons-partnership-with-anthropic
Titan,Amazon,https://aws.amazon.com/bedrock/titan/,200,4000,20:1,3,70.4,,,🆆 📚⬆ 🕸 👥,Apr/2023,🟢,https://www.techrepublic.com/article/amazon-bedrock-titan-cloud-artificial-intelligence/,Dense,"No official information at all. 2nd hand via Jack Clark: https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon '$65m training run. Specifically, they trained a 200B dense model on 4T tokens of data across 13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.'"
Multimodal-CoT,Amazon,https://github.com/amazon-science/mm-cot,0.738,,,,,,,🌋,Feb/2023,🟢,https://arxiv.org/abs/2302.00923,Dense,Models <1B with vision CoT
AlexaTM 20B,Amazon,Github (train/deploy),20,1300,65:1,0.5,,,,🆆 🕸,Aug/2022,🟢,https://assets.amazon.science/ee/20/3abcf2304d9b8d68da2006ff7107/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model.pdf,Dense,Wikipedia and mC4 only. seq2seq
AMD OLMo,AMD,https://huggingface.co/amd/AMD-OLMo,1,1308,"1,308:1",0.1,30.52,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html,Dense,1 billion parameter LMs trained from scratch using 1.3T tokens on a cluster of AMD Instinct MI250 GPUs.
AMD-Llama-135m,AMD,https://huggingface.co/amd/AMD-Llama-135m,0.135,670,"4,963:1",0,23.02,,,📚 🌋,Sep/2024,🟢,https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html,Dense,"Small language model (SLM). Trained on AMD Instinct™ MI250 accelerators. ""Pretrain Dataset: We employed the SlimPajama and Project Gutenberg dataset to pretrain the 135M model. Project Gutenberg is a library of over 70,000 free eBooks approximately. This sums up to 670B tokens"""
Transformer++,American Express,,0.212,0.1,1:1,0,,,,📚,Mar/2020,🔴,https://arxiv.org/abs/2003.04974,Dense,"Not to be confused with the more common usage of Transformer++, the ~2023 Transformer++ based on Llama. See Mamba paper."
Claude 3.5 Sonnet (new),Anthropic,https://claude.ai/,175,20000,115:1,6.2,90.5,78,65,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf#page=51,Dense,Absurd naming scheme. Paper addendum pp51-64: https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf#page=51
Claude 3.5 Sonnet,Anthropic,https://poe.com/Claude-3.5-Sonnet,70,15000,215:1,3.4,88.7,76.1,67.2,🆆 📚⬆ 🕸 🌋,Jun/2024,🔴,https://www.anthropic.com/news/claude-3-5-sonnet,Dense,MMLU=90.4 with prompting. Model card: https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf
Claude 3 Opus,Anthropic,https://claude.ai/,2000,40000,20:1,29.8,86.8,68.5,59.5,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://www.anthropic.com/claude-3-model-card,MoE,"Original MMLU=86.8 (GPT-4=86.4). MMLU=88.2 with CoT prompting. Original GPQA=50.4. 200k context, 1M for researchers."
Claude 2.1,Anthropic,https://claude.ai/,130,2500,20:1,1.9,78.5,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://www.anthropic.com/index/claude-2-1,Dense,"Less hallucinations, 200k context length, tool use"
Claude 2,Anthropic,https://claude.ai/,130,2500,20:1,1.9,78.5,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf,Dense,"More HHH, 200k context length"
RL-CAI,Anthropic,,52,400,8:1,0.5,,,,🆆 📚⬆ 🕸 👥,Dec/2022,🔴,https://arxiv.org/abs/2212.08073,Dense,RLAIF=reinforcement learning with AI feedback
Anthropic-LM 52B,Anthropic,,52,400,8:1,0.5,,,,🆆 📚⬆ 🕸 👥,Dec/2021,🔴,https://arxiv.org/abs/2112.00861,Dense,Internal research only
4M-21,Apple,https://github.com/apple/ml-4m/,3,1000,334:1,0.2,,,,🌋,Jun/2024,🟢,https://arxiv.org/abs/2406.09406,Dense,"Vision model based on T5-XXL. Modalities: RGB, Caption, Bounding boxes, Semantic segmentation, Depth, Human poses, Surface normals, CLIP, DINOv2, ImageBind, Metadata, Canny edges, SAM edges, SAM instances, Color palette. Project page: https://4m.epfl.ch/"
Apple On-Device model Jun/2024,Apple,https://github.com/apple/corenet/tree/main/projects/openelm,3.04,1500,494:1,0.2,26.76,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2404.14619,Dense,"https://lifearchitect.ai/apple/ Likely to be the Apple OpenELM model (Apr/2024). ""two of these models — a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute"". https://machinelearning.apple.com/research/introducing-apple-foundation-models The server-based model is possibly Ferret, although it is more properly called a multimodal model (not just language). It could also be Apple GPT based on their Ajax framework: https://archive.md/f3C0r"
OpenELM,Apple,https://huggingface.co/apple/OpenELM-3B-Instruct,3.04,1500,494:1,0.2,26.76,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2404.14619,Dense,"On-device model (laptop, phone). Open-source Efficient Language Models (OpenELM). https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/"
Ferret-UI,Apple,https://github.com/apple/ml-ferret,13,2000,154:1,0.5,,,,🆆 📚⬆ 🕸 👥,Apr/2024,🟢,https://arxiv.org/abs/2404.05719,Dense,"Vicuna base, multimodal. Extension of Ferret from Oct/2023."
ReALM-3B,Apple,,3,134,45:1,0.1,,,,🌋,Mar/2024,🔴,https://arxiv.org/abs/2403.20329,Dense,FLAN-T5 (Oct/2022) finetune.
MM1,Apple,,30,2010,67:1,0.8,,,,🌋,Mar/2024,🔴,https://arxiv.org/abs/2403.09611,Dense,"VLM, outperforms Flamingo 80B (Apr/2022) across benchmarks. 2T text tokens + ~10B+ other text (estimate). Unreleased."
Ask,Apple,,20,,,,,,,🌋,Feb/2024,🔴,https://www.macrumors.com/2024/02/22/applecare-advisors-testing-new-ask-tool/,Dense,Internal employee model only
MGIE,Apple,https://github.com/tsujuifu/pytorch_mgie,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://openreview.net/forum?id=S1RKWSyZ2Y,Dense,MLLM and diffusion model initialized from LLaVA-7B (Llama 2 + Vicuna) + StableDiffusion-v1.5.
Ferret,Apple,https://github.com/apple/ml-ferret,13,2000,154:1,0.5,,,,🆆 📚⬆ 🕸 👥,Oct/2023,🟢,https://arxiv.org/abs/2310.07704,Dense,"Vicuna base, multimodal"
UniLM,Apple,https://jackcook.com/2023/09/08/predictive-text.html,0.034,1,30:1,0,,,,🌋,Sep/2023,🟢,https://github.com/jackcook/predictive-spy,Dense,Apple's Transformer model for iOS 17 + macOS Sonoma. Announce is actually Jun/2023. GPT-2 base? 128 token context window
AuroraGPT (ScienceGPT),Argonne National Laboratory,https://lifearchitect.ai/auroragpt/,2000,30000,15:1,,,,,,TBA,🔴,,,Three models targeted in Jul/2024: AuroraGPT-7B-P (Ponte Vecchio GPU testing) AuroraGPT-7B-A (Aurora) AuroraGPT-7B-A-S (Aurora + Science).
Formosa (FFM),Asus/TWS,,176,366,3:1,0.8,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟡,https://www.asus.com/news/xxifirl2s2tzesl0/,Dense,"BLOOMZ finetune? Chinese, Taiwan's first LLM. Subscription hardware: https://archive.md/cVdJt"
DukunLM,AzaleAI,https://huggingface.co/azale-ai/DukunLM-13B-V1.0-Uncensored,13,1500,116:1,0.5,,,,🆆 📚⬆ 🕸 👥,Aug/2023,🟢,https://huggingface.co/azale-ai/DukunLM-13B-V1.0-Uncensored,Dense,Indonesian fine-tune of WizardLM (which is a Llama fine-tune).
Emu3,BAAI,https://huggingface.co/BAAI/Emu3-Gen,8,1000,125:1,0.3,,,,🌋,Sep/2024,🟢,https://arxiv.org/abs/2409.18869,Dense,"VLM. Dataset estimates are based on the unrelated UW/Salesforce dataset MINT-1T (3.4B images, 927M documents) https://arxiv.org/abs/2406.11271v1"
Tele-FLM-1T,BAAI,https://huggingface.co/CofeAI/Tele-FLM-1T,1000,15.7,1:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://arxiv.org/abs/2407.02783,Dense,"Technical arch testing only, ratio is too low for decent performance."
Tele-FLM,BAAI,https://huggingface.co/CofeAI/Tele-FLM,52,2000,39:1,1.1,64,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2404.16645,Dense,"Also known as FLM-2. ""We will open-source a 1T model checkpoint, namely Tele-FLM-1T, to advance further training and research."" Discussion paper Jul/2024: https://arxiv.org/abs/2407.02783"
Emu2,BAAI,https://baaivision.github.io/emu2/,37,4,1:1,0,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.13286,Dense,"VLM. Gemini clone. Outperforms Flamingo 80B. The Pile for text, but only sampled 3.6B tokens (1.4% of the dataset)."
FLM-101B,BAAI,https://huggingface.co/CofeAI/FLM-101B,101,245,3:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2309.03852,Dense,Train for $100k compute budget (on a cluster of 24 DGX-A800 GPU 8×80G servers for 21 days)
Baichuan 2,Baichuan,https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md,13,2600,200:1,0.6,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf,Dense,Great paper. Chinese-English bilingual dataset
ERNIE 4.0 Turbo,Baidu,https://yiyan.baidu.com/,200,20000,100:1,6.7,,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/,Dense,"""Ernie Bot has reached 300 million users since its launch [on 16/Mar/2023, public Aug/2023]"" Jun/2024"
ERNIE 4.0,Baidu,https://yiyan.baidu.com/,1000,20000,20:1,14.9,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://reuters.com/technology/chinas-baidu-unveils-latest-version-its-ernie-ai-model-2023-10-17/,Dense,Dense (confirmed). English-dubbed launch video (2h52m): https://twitter.com/i/broadcasts/1yNGaZaeallJj & https://youtu.be/wYozcsavRuM
ERNIE-Code,Baidu,,0.56,,,,,,,🕸,Dec/2022,🟢,https://arxiv.org/abs/2212.06742#baidu,Dense,
ERNIE 3.0 Titan,Baidu,,260,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2021,🟢,https://arxiv.org/abs/2112.12731,Dense,
PLATO-XL,Baidu,Baidu,11,,,,,,,⬆ 👥,Sep/2021,🟢,https://arxiv.org/abs/2109.09519,Dense,Chatbot. Reddit comments + CN social
Sky-T1-32B-Preview,Berkeley,https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview,32,18000,563:1,2.5,,,56.8,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://novasky-ai.github.io/posts/sky-t1/,Dense,"""To generate our training data we use QwQ-32B-Preview, an open-source model with reasoning capabilities comparable to o1-preview. We curate the data mixture (see later section) to cover diverse domains that require reasoning, and a reject sampling procedure to improve the data quality. We then rewrite QwQ traces with GPT-4o-mini into a well-formatted version, inspired by Still-2, to improve data quality and ease parsing... Rejection Sampling: We discard QwQ samples if they are incorrect according to the solutions provided in datasets."""
Starling-7B,Berkeley,https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha,7,2000,286:1,0.4,,37.9,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://starling.cs.berkeley.edu/,Dense,Llama 2 7B -> OpenChat 7B -> Starling-7B (RLAIF)
Koala-13B,Berkeley,https://chat.lmsys.org/?model=koala-13b,13,,,,,,,🆆 📚⬆ 🕸 👥,Apr/2023,🟢,https://bair.berkeley.edu/blog/2023/04/03/koala/,Dense,LLaMA base. Academic licence only.
LVM-3B,Berkeley/JHU,,3,420,140:1,0.1,,,,🖼,Dec/2023,🔴,https://arxiv.org/abs/2312.00785,Dense,Paper is 25MB. First Large Vision Model (LVM); no text. Based on Llama and LAION 5B (1.49B).
mT0,BigScience,https://github.com/bigscience-workshop/xmtf,13,1000,77:1,0.4,,,,🆀🅰 🕸,Nov/2022,🟢,https://arxiv.org/abs/2211.01786,Dense,fine-tuned
BLOOMZ,BigScience,https://github.com/bigscience-workshop/xmtf,176,366,3:1,0.8,,,,⬆ 🕸,Nov/2022,🟢,https://arxiv.org/abs/2211.01786,Dense,fine-tuned
BLOOM (tr11-176B-ml),BigScience,https://huggingface.co/spaces/huggingface/bloom_demo,176,366,3:1,0.8,39.1,,,⬆ 🕸,Jul/2022,🟢,https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml,Dense,
BloombergGPT,Bloomberg,,50,569,12:1,0.6,39.2,,,🆆 📚⬆ 🕸 🌋,Mar/2023,🔴,https://arxiv.org/abs/2303.17564,Dense,"Video: https://youtu.be/m2Scj2SO85Y Underperforms GPT-3, based on BLOOM. Tokens: 'We select a model size motivated by Hoffmann et al. (2022) and train a 50 billion parameter model on 569 billion tokens from our corpus of over 700 billion tokens to produce a model that is competitive with larger models.'"
Platypus,Boston University,https://platypus-llm.github.io/,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Aug/2023,🟢,https://platypus-llm.github.io/Platypus.pdf,Dense,"Fine-tune of Llama 2, family includes merges with Beluga, Dolphin, and Camel fine-tunes."
GOODY-2,BRAIN,https://www.goody2.ai/chat,,,,,,,,🌋,Feb/2024,🟢,https://www.goody2.ai/goody2-modelcard.pdf,Dense,Satire (and hilarious). Probably Llama 2 with aggressive prompt. Wired interview: https://archive.md/toxHq
UI-TARS-72B,ByteDance,https://github.com/bytedance/UI-TARS-desktop?tab=readme-ov-file,72,9000,125:1,2.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://arxiv.org/abs/2501.12326,Dense,VLM. SoTA agent 'computer use' model to 23/Jan/2024.
Doubao-1.5-pro,ByteDance,https://www.volcengine.com/docs/82379/1330310#474f7dec,300,9000,30:1,5.5,88.6,80.1,65,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://team.doubao.com/en/special/doubao_1_5_pro,MoE,"Reasoning. Includes 2.4B param ViT. ""Doubao-1.5-pro uses a sparse MoE architecture. In the pre-training stage, the performance of the MoE model activated with only a small number of parameters can exceed that of ultra-large dense pre-trained models such as Llama3.1-405B. Through the study of the sparsity scaling law, the team determined the sparse ratio that balances performance and efficiency, and determined based on the MoE scaling law that a model activated with a small number of parameters can achieve the performance of a world-class model."""
530B,ByteDance,,530,300,1:1,1.3,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🔴,https://arxiv.org/abs/2402.15627,Dense,"Trained using 12,288 A100 GPUs, replicating MT-NLG size"
175B,ByteDance,,175,300,2:1,0.8,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🔴,https://arxiv.org/abs/2402.15627,Dense,"Trained using 12,288 A100 GPUs, replicating GPT-3 size"
PandaGPT,Cambridge/Tencent,https://panda-gpt.github.io/,13,1000,77:1,0.4,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://github.com/yxuansu/PandaGPT/blob/main/PandaGPT.pdf,Dense,"Proto-AGI. 6 modalities (text, image/video, audio, depth, thermal, and IMU/accelerometer/gyroscope/compass). Based on Vicuna."
Rene,Cartesia,https://huggingface.co/cartesia-ai/Rene-v0.1-1.3b-pytorch,1.3,1500,"1,154:1",0.1,32.6,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://cartesia.ai/blog/2024-08-27-on-device,Dense,"On-device. ""hybrid architecture based on Mamba-2, with feedforward and sliding window attention layers interspersed"""
1T,Cerebras,https://cerebras.ai/press-release/cerebras-demonstrates-trillion-parameter-model-training-on-a-single-cs-3-system,1000,20000,20:1,14.9,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🔴,https://cerebras.ai/press-release/cerebras-demonstrates-trillion-parameter-model-training-on-a-single-cs-3-system,Dense,"""For Sandia’s trillion parameter training run, Cerebras configured a 55 terabyte MemoryX device."""
Sparse Llama 7B,Cerebras,https://huggingface.co/spaces/neuralmagic/llama-2-sparse-transfer-chat-deepsparse,7,145,21:1,0.1,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.03594,Hybrid,"https://www.cerebras.net/blog/introducing-sparse-llama-70-smaller-3x-faster-full-accuracy ""For the 50% sparse model, we utilized 45 billion tokens of pretraining data, while an additional 100 billion tokens were used for the 70% model. This represents approximately 2% to 8% of the original 2 trillion tokens used to train the base Llama-2 model."""
FLOR-6.3B,Cerebras,https://huggingface.co/projecte-aina/FLOR-6.3B,6.3,481,77:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://www.cerebras.net/press-release/cerebras-systems-and-barcelona-supercomputing-center-train-industry-leading-multilingual-spanish-catalan-english-llm,Dense,"Spanish, Catalan. Bloom-7.1B (341B tok) + continued pre-training on 140B tok. Trained on Cerebras hardware."
BTLM-3B-8K,Cerebras,https://huggingface.co/cerebras/btlm-3b-8k-base,3,627,209:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://www.cerebras.net/blog/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/,Dense,"Runs on devices with as little as 3GB of memory [iPhone, Macbook] when quantized to 4-bit"
Cerebras-GPT,Cerebras,https://huggingface.co/cerebras,13,260,20:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Mar/2023,🟢,https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/,Dense,20:1 tokens to parameters as per https://lifearchitect.ai/chinchilla/
C1.2,Character.ai,https://blog.character.ai/character-ai/,20,1000,50:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Mar/2023,🟢,https://blog.character.ai/character-ai/,Dense,No details released.
Natural-SQL-7B,ChatDB,,7,2000,286:1,0.4,,,,🌋,Feb/2024,🟢,https://huggingface.co/chatdb/natural-sql-7b,Dense,Based on DeepSeek-Coder 6.7B.
Unnamed 1T,China Telecom Artificial Intelligence Research Institute,https://www.scmp.com/tech/big-tech/article/3280588/china-telecom-say-ai-model-1-trillion-parameters-trained-chinese-chips,1000,20000,20:1,14.9,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🔴,https://www.scmp.com/tech/big-tech/article/3280588/china-telecom-say-ai-model-1-trillion-parameters-trained-chinese-chips,Dense,"Trained on Chinese GPUs: ""Ascend Atlas 800T A2 training server – a Huawei product listed as supporting the Kunpeng 920 7265 or Kunpeng 920 5250 processors"" https://www.theregister.com/2024/10/02/china_telecom_model_trained_local_tech/"
TeleChat2-115B,China Telecom Artificial Intelligence Research Institute,https://modelscope.cn/models/TeleAI/TeleChat2-115B,115,10000,87:1,3.6,80.9,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://github.com/Tele-AI/TeleChat2,Dense,"Trained on Chinese GPUs: ""Ascend Atlas 800T A2 training server – a Huawei product listed as supporting the Kunpeng 920 7265 or Kunpeng 920 5250 processors"" https://www.theregister.com/2024/10/02/china_telecom_model_trained_local_tech/"
Bamba-9B,CMU,https://huggingface.co/blog/bamba,9,2200,245:1,0.5,60.77,17.53,4.14,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://huggingface.co/blog/bamba,Dense,"""trained by IBM, Princeton, CMU, and UIUC on completely open data. At inference time, the model demonstrates 2.5x throughput improvement and 2x latency speedup compared to standard transformers in vLLM."""
Bi-Mamba,CMU,,2.7,1260,467:1,0.2,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🔴,https://arxiv.org/abs/2411.11843,Dense,"Unreleased, but will be replicated. ""a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models"""
Mamba-2,CMU,https://github.com/state-spaces/mamba,2.7,300,112:1,0.1,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.21060,Dense,Analysis: https://tridao.me/blog/2024/mamba2-part1-model/
Mamba,CMU,https://huggingface.co/havenhq/mamba-chat,2.8,300,108:1,0.1,26.2,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.00752,Dense,"The Pile, new arch beyond just Transformers. 2.7B MMLU=26.2. 7B MMLU=33.3."
Command R7B,Cohere,https://cohereforai-c4ai-command.hf.space/models/command-r7b-12-2024,7,2000,286:1,0.4,,28.5,7.7,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024,Dense,
Maya,Cohere,https://huggingface.co/maya-multimodal/maya,8,4800,600:1,0.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://arxiv.org/abs/2412.07112,Dense,VLM.
Aya-Expanse-32B,Cohere,https://huggingface.co/CohereForAI/aya-expanse-32b,32,8000,250:1,1.7,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Oct/2024,🟢,https://cohere.com/blog/aya-expanse-connecting-our-world,Dense,"""Aya Expanse, a family of highly performant multilingual models that excels across 23 languages and outperforms other leading open-weights models...we have collaborated with over 3,000 researchers from 119 countries to expand cutting-edge multilingual research... 220 language ambassadors from around the world who have been part of this release"""
Aya-23-35B,Cohere,https://huggingface.co/spaces/CohereForAI/aya-23,35,4800,138:1,1.4,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://drive.google.com/file/d/1YKBPo61pnl97C1c_1C2ZVOnPhqf7MLSc/view,Dense,
Rerank 3,Cohere,https://docs.cohere.com/reference/rerank-1,104,4000,39:1,2.1,,,,📚 🕸,Apr/2024,🟢,https://txt.cohere.com/rerank-3/,Dense,"RAG + semantic search, possibly backed by Command-R+."
Command-R+,Cohere,https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus,104,4000,39:1,2.1,75.7,,,📚 🕸,Apr/2024,🟢,https://huggingface.co/CohereForAI/c4ai-command-r-plus,Dense,purpose-built to excel at real-world enterprise use cases. Announce with no arch details: https://txt.cohere.com/command-r-plus-microsoft-azure/
Command-R,Cohere,Cohere,35,700,20:1,0.5,,37.9,,📚 🕸,Mar/2024,🟢,https://txt.cohere.com/command-r/,Dense,RAG and tool use
Aya-101,Cohere,https://huggingface.co/CohereForAI/aya-101,13,1000,77:1,0.4,,,,📚 🕸,Feb/2024,🟢,https://cohere.com/research/aya/aya-model-paper.pdf,Dense,mT5 base.
Command xlarge,Cohere,Cohere,52.4,,,,,,,📚 🕸,Sep/2021,🟢,https://arxiv.org/abs/2108.07790,Dense,Stealth 'ebooks and webpages'. 52B: https://crfm.stanford.edu/helm/v1.0/?models=1
PLLuM,Consortium,,20,2000,100:1,0.7,,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://opi.org.pl/en/the-launch-of-the-first-polish-open-large-language-model-pllum/,Dense,Polish Large Language Model. Not yet available as of Sep/2024
MambaByte,Cornell,https://github.com/kyegomez/MambaByte,0.972,37.5,39:1,0,,,,📚🌋,Jan/2024,🔴,https://arxiv.org/abs/2401.13660,Dense,"Used bytes instead of tokens. 4 bytes≈1 token, so 150B bytes≈37.5B tokens"
Cedille FR-Boris,Coteries,"Cedille, TS",6,,,,,,,🆆 📚 🕸 🇫🇷,Nov/2021,🟢,https://github.com/coteries/cedille-ai,Dense,French only. GPT-J.
RFM-1,Covariant,https://vimeo.com/921866765,8,160,20:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟡,https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/,Dense,"Commercial, multimodal for robotics"
Dolly 2.0,Databricks,https://huggingface.co/databricks/dolly-v2-12b,12,300,25:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm,Dense,Fine-tuned Pythia 12B
DeciLM-7B,Deci,https://console.deci.ai/infery-llm-demo,7.04,200,29:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://deci.ai/blog/introducing-DeciLM-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date,Dense,4.4x times faster than Mistral. English only.
DeciLM,Deci,https://huggingface.co/Deci/DeciLM-6b,5.7,200,36:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/,Dense,Faster inference (4.8× throughput of Llama 2)
next-gen,DeepL,https://www.deepl.com/en/translator,7,1000,143:1,0.3,,,,🌋,Jul/2024,🟢,https://www.deepl.com/en/blog/next-gen-language-model,Dense,"""Built using our own groundbreaking, specialized LLM technology and proprietary training data, designed specifically for translation"""
SED,DeepMind,,,,,,,,,🕸,Nov/2022,🔴,https://arxiv.org/abs/2211.04236,Dense,SED 420M (diffusion text model)
Sparrow,DeepMind,,70,1400,20:1,1,,,,🆆 📚⬆ 🕸 🌋,Sep/2022,🔴,https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf,Dense,Chatbot as a fine-tuned version of Chinchilla 70B
Perceiver AR,DeepMind,,1,,,,,,,🆆 📚⬆ 🕸 🌋,Jun/2022,🔴,https://arxiv.org/abs/2202.07765,Dense,"Context window=100,000. Params=364m wiki, 975M pg-19, 826M books, music=?, imagenet=770M,"
Gato (Cat),DeepMind,,1,,,,,,,🆆 📚⬆ 🕸 🌋,May/2022,🔴,https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf,Dense,"Proto-AGI. Generalist agent (LLM, VLM, robot)"
Chinchilla,DeepMind,,70,1400,20:1,1,67.5,,,🆆 📚⬆ 🕸 🌋,Mar/2022,🔴,https://arxiv.org/abs/2203.15556,Dense,First to double tokens per size increase
Gopher,DeepMind,,280,300,2:1,1,60,,,🆆 📚⬆ 🕸 🌋,Dec/2021,🔴,https://arxiv.org/abs/2112.11446,Dense,Dataset: https://lifearchitect.ai/whats-in-my-ai/
RETRO,DeepMind,,7.5,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2021,🔴,https://arxiv.org/abs/2112.04426,Dense,with retrieval
DeepSeek-R1,DeepSeek-AI,https://huggingface.co/deepseek-ai/DeepSeek-R1,685,14800,22:1,10.6,90.8,84,71.5,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf,MoE,"Reasoning. ""DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks"""
DeepSeek-V3,DeepSeek-AI,https://chat.deepseek.com/,685,14800,22:1,10.6,87.1,64.4,59.1,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,MoE,37B active. Explain: https://threadreaderapp.com/thread/1872318161883959485.html Announce: https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file
DeepSeek-R1-Lite,DeepSeek-AI,https://chat.deepseek.com/,67,2000,30:1,1.2,,,58.5,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://x.com/deepseek_ai/status/1859200141355536422,Dense,"Reasoning. Scores 0/5 on latest ALPrompt 2024 H2 ""DeepSeek-R1-Lite is currently still in the iterative development stage. It currently only supports web usage and does not support API calls. The base model used by DeepSeek-R1-Lite is also a relatively small model, unable to fully unleash the potential of long reasoning chains. At present, we are continuously iterating on the inference series models. In the future, the official DeepSeek-R1 model will be fully open-sourced. We will publicly release the technical report and deploy API services."" https://mp-weixin-qq-com.translate.goog/s/e1YnTxZlzFvjcmrLLTA8fw?_x_tr_sl=zh-CN&_x_tr_tl=en&_x_tr_hl=zh-TW"
DeepSeek-V2.5,DeepSeek-AI,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,236,10200,44:1,5.2,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf,MoE,"""DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct."""
DeepSeek-Coder-V2,DeepSeek-AI,https://chat.deepseek.com/coder,236,10200,44:1,5.2,79.2,63.63,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf,MoE,DeepSeek-V2 with additional 6 trillion tokens.
DeepSeek-V2,DeepSeek-AI,https://chat.deepseek.com/,236,8100,35:1,4.6,78.5,54.8,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.04434,MoE,"Huge dataset, 12% Chinese ""Therefore, we acknowledge that DeepSeek-V2 still has a slight gap in basic English capabilities with LLaMA3 70B""."
DeepSeek-VL,DeepSeek-AI,https://github.com/deepseek-ai/DeepSeek-VL?tab=readme-ov-file,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.05525,Dense,"Vision, based on DeepSeek-LLM-7B"
DeepSeek-Coder,DeepSeek-AI,https://coder.deepseek.com/,33,2000,61:1,0.9,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.14196,Dense,surpasses existing closed-source models like Codex and GPT-3.5... permissive license that allows for both research and unrestricted commercial use.'
DeepSeekMoE,DeepSeek-AI,,16,2000,125:1,0.6,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🔴,https://arxiv.org/abs/2401.06066,MoE,"MoE activated parameters is 10-15% of dense, so I need to rethink ALScore for MoE. 'preliminary efforts to scale up DeepSeekMoE to 145B'"
DeepSeek,DeepSeek-AI,https://chat.deepseek.com/,67,2000,30:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.02954,Dense,Chinese/English. Outperforms Llama 2. MMLU=71.3 outperforms GPT-3.5.
Pile-T5,EleutherAI,https://huggingface.co/EleutherAI/pile-t5-xxl,11,2000,182:1,0.5,53.84,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://blog.eleuther.ai/pile-t5/,Dense,
Pythia,EleutherAI,https://huggingface.co/EleutherAI/pythia-12b,12,300,25:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://arxiv.org/abs/2304.01373,Dense,
GPT-NeoX-20B,EleutherAI,"TS, Goose",20,,,,,,,🆆 📚⬆ 🕸 🌋,Feb/2022,🟢,https://github.com/EleutherAI/gpt-neox,Dense,Latest model to Feb/2022
GPT-J,EleutherAI,"TS, Goose",6,402,67:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Jun/2021,🟢,https://github.com/kingoflolz/mesh-transformer-jax,Dense,Popular
xLSTM,ELLIS,,2.7,15,6:1,0,,,,🆆 📚⬆ 🕸 🌋,May/2024,🔴,https://arxiv.org/abs/2405.04517,Dense,"New method LSTM to xLSTM, see also RNNs. Code/weights doesn't seem to be released. https://github.com/AI-Guru/xlstm-resources"
MEDITRON,EPFL,https://huggingface.co/epfl-llm/meditron-70b,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://arxiv.org/abs/2311.16079,Dense,"Llama 2 trained on med data using NVIDIA Megatron-LM. ""outperforms Llama-2-70B, GPT-3.5 (text-davinci-003, 8-shot), and Flan-PaLM on multiple medical reasoning tasks."""
PassGPT,ETH Zürich,,,,,,,,,🌋,Jun/2023,🔴,https://arxiv.org/abs/2306.01545,Dense,GPT-2 trained on leaked passwords
ESM3,EvolutionaryScale,https://github.com/evolutionaryscale/esm,98,771,8:1,0.9,,,,🌋,Jun/2024,🟡,https://www.evolutionaryscale.ai/blog/esm3-release,Dense,"Biology large language model: ""sequence, structure, and function are all masked and predicted during training, ESM3 can generate in all three modalities."" 1.4B only released."
ULMFiT,Fast.ai,https://docs.fast.ai/tutorial.text.html,0.034,0.1,3:1,0,,,,🆆,Jan/2018,🟢,https://arxiv.org/abs/1801.06146,Dense,Aussie Prof Jeremy Howard: https://www.abc.net.au/news/science/2023-11-15/jeremy-howard-taught-ai-to-the-world-and-helped-invent-chatgpt/103092474
f1,Fireworks,https://fireworks.ai/models/fireworks/f1-preview/playground,,,,,,,42.4,,Nov/2024,🟢,https://fireworks.ai/blog/fireworks-compound-ai-system-f1,Compound,"""a compound AI model specialized in complex reasoning, that interweaves multiple open models at the inference layer. """
AnyGPT,Fudan University,https://junzhan2000.github.io/AnyGPT.github.io/,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2402.12226,Dense,Llama 2 7B backbone with new matrices ('reshaping the embedding matrix and prediction layer')
MOSS,Fudan University,https://moss.fastnlp.top/,16,430,27:1,0.3,,,,🕸 🌋,Feb/2023,🟢,https://txsun1997.github.io/blogs/moss.html,Dense,Major bandwidth issues: https://www.reuters.com/technology/china-fudan-university-team-apologises-after-chatgpt-style-platform-crashes-2023-02-21/
Fugaku-LLM,Fujitsu,https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct,13,380,30:1,0.2,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html,Dense,"Japanese. CPU trained: 158,976+ A64FX CPUs (7M+ cores), zero GPUs. https://en.wikipedia.org/wiki/Fugaku_(supercomputer)"
Luna,Galileo,https://www.rungalileo.io/blog/introducing-galileo-luna-a-family-of-evaluation-foundation-models,0.44,162,369:1,0,,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2406.00975,Dense,Based on DeBERTA-large (440M). RoBERTa=162B token dataset.
TimesFM,Google,,0.2,100,500:1,0,,,,🆆 🌋,Feb/2024,🔴,https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html,Dense,Time-series forecasting only. 'a large pretraining corpus of 100B real world time-points' may be more than 100B tokens.
AudioPaLM,Google,https://google-research.github.io/seanet/audiopalm/examples/,340,3600,11:1,3.7,,,,🆆 📚⬆ 🕸 👥,Jun/2023,🔴,https://arxiv.org/abs/2306.12925,Dense,a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation
PaLM 2,Google,https://console.cloud.google.com/vertex-ai/generative/language/create/chat,340,3600,11:1,3.7,,,,🆆 📚⬆ 🕸 👥,May/2023,🟢,https://ai.google/static/documents/palm2techreport.pdf,Dense,"“What we found in our work is that it’s not really the sort of size of model — that the larger is not always better,” Deepmind VP Zoubin Ghahramani said in a press briefing ahead of today’s announcement. “That’s why we’ve provided a family of models of different sizes. We think that actually parameter count is not really a useful way of thinking about the capabilities of models and capabilities are really to be judged by people using the models and finding out whether they’re useful in the tests that they try to achieve with these models.”"
CoLT5,Google,,5.2,,,,,,,🆀🅰 🕸,Mar/2023,🔴,https://arxiv.org/abs/2303.09752,Dense,up to 64k context window [48k words or about 96 pages -Alan]
RT-1,Google,,0.035,,,,,,,🌋,Dec/2022,🔴,https://robotics-transformer.github.io/assets/rt1.pdf,Dense,
Flan-T5,Google,TS,11,1100,100:1,0.4,,,,🆀🅰 🕸,Oct/2022,🟢,https://arxiv.org/abs/2210.11416,Dense,T5=1T tokens + LM-adapted T5 as 100B tokens
Flan-PaLM,Google,,540,780,2:1,2.2,73.5,,,🆆 📚⬆ 🕸 👥,Oct/2022,🔴,https://arxiv.org/abs/2210.11416,Dense,
U-PaLM,Google,,540,780,2:1,2.2,74.1,,,🆆 📚⬆ 🕸 👥,Oct/2022,🔴,https://arxiv.org/abs/2210.11399,Dense,
PaLI,Google,,17,,,,,,,🌋,Sep/2022,🔴,https://arxiv.org/abs/2209.06794,Dense,"PaLM Vision model, new datasets of 10B multilingual text-image pairs"
‘monorepo-Transformer’,Google,,0.5,,,,,,,🕸,Jul/2022,🔴,https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html,Dense,Unnamed. Writes >3% of internal google code.
Minerva,Google,,540,818.5,2:1,2.2,,,,🆆 📚⬆ 🕸 🌋,Jun/2022,🔴,https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html,Dense,PaLM finetuned on LaTeX/arXiv maths
LIMoE,Google,,5.6,,,,,,,🆆 📚⬆ 🕸 👥,Jun/2022,🔴,https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html,MoE,
UL2 20B,Google,,20,1000,50:1,0.5,39.2,,,🕸,May/2022,🔴,https://arxiv.org/abs/2205.05131,Dense,Unifying Language model. C4 only.
LaMDA 2,Google,YouTube (video only),137,,,,,,,⬆ 🕸 👥,May/2022,🟡,https://arxiv.org/abs/2201.08239,Dense,Chatbot with tiny walled garden demo TBA
PaLM-Coder,Google,,540,780,2:1,2.2,,,,🕸,Apr/2022,🔴,https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf,Dense,
PaLM,Google,,540,780,2:1,2.2,,,,🆆 📚⬆ 🕸 👥,Apr/2022,🔴,https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdff,Dense,
GLaM,Google,,1200,,,,,,,🆆 📚⬆ 🕸 👥,Dec/2021,🔴,https://arxiv.org/abs/2112.06905,MoE,
BERT-480,Google,,480,,,,,,,🆆 📚 🕸,Nov/2021,🔴,https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-v4-pods-for-large-model-training,Dense,Submission to benchmarks. Original dataset was BookCorpus + Wikipedia: https://arxiv.org/pdf/1810.04805.pdf
BERT-200,Google,,200,,,,,,,🆆 📚 🕸,Nov/2021,🔴,https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-v4-pods-for-large-model-training (same as above),Dense,Submission to benchmarks. Original dataset was BookCorpus + Wikipedia: https://arxiv.org/pdf/1810.04805.pdf
FLAN,Google,,137,,,,,,,⬆ 🕸 👥,Sep/2021,🔴,https://arxiv.org/abs/2109.01652,Dense,Fine-tuned LaMDA
LaMDA,Google,YouTube (video only),137,,,,,,,⬆ 🕸 👥,Jun/2021,🔴,https://arxiv.org/abs/2201.08239,Dense,Chatbot
Switch,Google,,1600,576,1:1,3.2,,,,🆀🅰 🕸,Jan/2021,🟢,https://arxiv.org/abs/2101.03961,MoE,
Meena,Google,,2.6,10000,"3,847:1",0.5,,,,👥 🌋,Jan/2020,🔴,https://arxiv.org/abs/2001.09977,Dense,Dialogue model. Trained 61B tokens for 164x epochs to 10T tokens!
T5,Google,,11,1000,91:1,0.3,,,,🆀🅰 🕸,Oct/2019,🟢,https://arxiv.org/abs/1910.10683,Dense,"C4 + NLP language problems. ""compared the following three configurations: First, the standard baseline model, which was pre-trained on 235 ≈ 34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5), which we refer to as “baseline-1T”; and third, T5-Base."""
BERT,Google,Hugging Face,0.34,137,403:1,0,,,,🆆 📚,Oct/2018,🟢,https://arxiv.org/abs/1810.04805,Dense,
Transformer (big),Google,https://github.com/tensorflow/tensor2tensor?tab=readme-ov-file#walkthrough,0.213,0.1,1:1,0,,,,📚,Jun/2017,🟢,https://arxiv.org/abs/1706.03762,Dense,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs... For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."""
Transformer (base),Google,https://github.com/tensorflow/tensor2tensor?tab=readme-ov-file#walkthrough,0.065,0.1,2:1,0,,,,📚,Jun/2017,🟢,https://arxiv.org/abs/1706.03762,Dense,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs... For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."""
Gemini 2.0 Pro,Google DeepMind,https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-pro-exp-02-05,200,20000,100:1,6.7,,79.1,64.7,🆆 📚⬆ 🕸 🌋 ⚛️,Feb/2025,🟢,https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/,MoE,"Context=2M. Disappointing benchmarks, this is the 'pro' (medium) not 'ultra' (large) model."
Gemini 2.0 Flash exp,Google DeepMind,https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-2.0-flash-exp,30,30000,"1,000:1",3.2,87,76.4,62.1,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2,MoE,"Gemini 2.0 Flash was first model released, 11/Dec/2024. ""New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities"" Announce: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/"
Gemini-1.5-Pro-002,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,1500,30000,20:1,22.4,,75.8,59.1,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/,MoE,Sparse MoE. Context window=2M
Data-Gemma,Google DeepMind,https://huggingface.co/google/datagemma-rig-27b-it,27,13000,482:1,2,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf,Dense,"RAG/RIG: ""the LLM is fine-tuned to produce natural language Data Commons queries alongside statistics"""
Gemini 1.5 Flash-8B,Google DeepMind,https://ai.google.dev/,8,8000,"1,000:1",0.8,68.1,,30.8,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2403.05530,Dense,Announce: https://x.com/OfficialLoganK/status/1828480085353234535 1M context for all modalities.
FLAMe,Google DeepMind,,24,1000,42:1,0.5,,,,👥,Jul/2024,🔴,https://arxiv.org/abs/2407.10817v1,Dense,LLM-as-a-Judge autorater. Foundational Large Autorater Models (FLAMe). Uses an instruction-tuned PaLM-2-24B model. Unrelated to Microsoft FLAME Jan/2023.
Gemma 2,Google DeepMind,https://huggingface.co/google/gemma-2-27b-it,27,13000,482:1,2,75.2,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,Dense,Announce: https://blog.google/technology/developers/google-gemma-2/
LearnLM,Google DeepMind,https://learning.google.com/experiments/learn-about/signup,1500,30000,20:1,22.4,,,72,🆆 📚⬆ 🕸 🌋,May/2024,🟡,https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf,MoE,"Fine-tuned + prompted Gemini (Dec/2023). ""The results of LearnLM-Tutor reproduce the performance of Gemini Pro, for example an MMLU score of 0.72 and MATH score of 0.33."""
Gemini 1.5 Flash,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,8,10000,"1,250:1",0.9,78.9,59.1,39.5,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://goo.gle/GeminiV1-5,MoE,1M context length.
Med-Gemini-L 1.0,Google DeepMind,https://twitter.com/alan_karthi/status/1785117450528264216,1500,30000,20:1,22.4,,,,🆆 📚⬆ 🕸 🌋,May/2024,🔴,https://arxiv.org/abs/2404.18416,Dense,"Med-Gemini-M 1.0 and Med-Gemini-L 1.0 (Pro and Ultra finetunes) ""For language tasks that require less complex reasoning, such as summarizing medical notes and creating referral letters, we introduce Med-Gemini-M 1.0 by fine-tuning the Gemini 1.0 Pro model. For other tasks that require more advanced reasoning, we introduce Med-Gemini-L 1.0 by fine-tuning the Gemini 1.0 Ultra model using a self-training method to enable the models to efficiently use web search."""
Hawk,Google DeepMind,,7,300,43:1,0.2,35,,,🆆 📚🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.19427,Dense,MMLU=35. RNN.
Griffin,Google DeepMind,,14,300,22:1,0.2,49.5,,,🆆 📚🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.19427,Dense,MMLU=49.5. RNN.
Gemma,Google DeepMind,https://labs.pplx.ai/,7,6000,858:1,0.7,64.3,33.7,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Dense,"MMLU=64.3 (Llama 2 70B=68.9, ChatGPT 20B=70). Text only. Probably dense. Largest trained dataset (6T) besides frontier models."
Gemini 1.5 Pro,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,1500,30000,20:1,22.4,85.9,69,46.2,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://goo.gle/GeminiV1-5,MoE,Sparse MoE. Context window=1M and 10M for research
MedLM,Google DeepMind,https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/medlm,,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟡,https://cloud.google.com/static/vertex-ai/docs/generative-ai/medlm/MedLM-model-card.pdf,Dense,Available to 'white-listed' orgs only.
Gemini Ultra 1.0,Google DeepMind,https://deepmind.google/technologies/gemini/,1500,30000,20:1,22.4,83.7,,35.7,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Dense,"Original MMLU=83.7. MMLU=90.04 with prompting. Chinchilla (20:1), dense, maybe 600B-2000T."
Q-Transformer,Google DeepMind,https://qtransformer.github.io/,,,,,,,,🌋,Nov/2023,🔴,https://qtransformer.github.io/assets/qtransformer.pdf,Dense,"Robotics, builds on RT-1"
Mirasol3B,Google DeepMind,,3,,,,,,,🌋,Nov/2023,🔴,https://arxiv.org/abs/2311.05698,Dense,Combiner + autoregressive transformer for video/audio/text
PaLI-3,Google DeepMind,,5,,,,,,,🌋,Oct/2023,🔴,https://arxiv.org/abs/2310.09199,Dense,VLM. Next iteration of PaLI via Pathways. https://lifearchitect.ai/pathways/
RT-X,Google DeepMind,https://robotics-transformer-x.github.io/,55,,,,,,,🌋,Oct/2023,🟢,https://robotics-transformer-x.github.io/paper.pdf,Dense,"Robotics using UL2. 'RT-1 model trained using the robotic data mixture as RT-1-X, and the RT-2 model trained using the robotic data mixture as RT-2-X.'"
Med-PaLM M,Google DeepMind,,540,780,2:1,2.2,,,,🆆 📚⬆ 🕸 👥,Jul/2023,🔴,https://arxiv.org/abs/2307.14334,Dense,Uses PaLM 1. Already outperformed by Med-PaLM 2. Med-PaLM Multimodal (Med-PaLM M).
DIDACT,Google DeepMind,,,37900,,,,,,🌋,Jun/2023,🔴,https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html,Dense,Iterative coding model trained on Google's monorepo. Jacob: https://twitter.com/jacobaustin132/status/1663972128176128002
Med-PaLM 2,Google DeepMind,,340,3600,11:1,3.7,,,,🆆 📚⬆ 🕸 👥,Mar/2023,🔴,https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/,Dense,"Recently, our next iteration, Med-PaLM 2, consistently performed at an “expert” doctor level on medical exam questions, scoring 85%. This is an 18% improvement from Med-PaLM’s previous performance and far surpasses similar AI models."
Med-PaLM 1,Google DeepMind,,540,780,2:1,2.2,,,,🆆 📚⬆ 🕸 👥,Dec/2022,🔴,https://arxiv.org/abs/2212.13138,Dense,Collab between Google & DeepMind. Makes 1% less errors than humans
H2O-Danube3-4B,H2O.ai,https://h2o.ai/platform/danube/personal-gpt/,4,6000,"1,500:1",0.5,55.18,,,🆆 📚⬆ 🕸 🌋 ⚛️,Jul/2024,🟢,https://arxiv.org/abs/2407.09276,Dense,"Runs natively and fully offline on mobile phone. ""H2O-Danube3 is a family of decoder only LLM models that use the general Llama model architecture adopting core principles from Llama 2 and Mistral with custom parameters determining the shape of each layer and total parameter count. We use the Mistral tokenizer..."" MMLU for chat=54.74, base=55.18 via https://huggingface.co/h2oai/h2o-danube3-4b-base"
LeoLM,Hessian AI/LAION,https://huggingface.co/LeoLM/leo-hessianai-13b,13,2065,159:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://laion.ai/blog/leo-lm/,Dense,Llama 2 'extended' and pretrained on 2000B Llama 2 tokens + 65B tokens of German
Cosmo-1B,HF,https://huggingface.co/HuggingFaceTB/cosmo-1b,1.8,180,100:1,0.1,,,,⚛️,Feb/2024,🟢,https://huggingface.co/blog/cosmopedia,Dense,Synthetic data (25B tokens of synthetic data for 6 epochs + code). MMLU=32.4
StarCoder 2,HF/ServiceNow,,15,4300,287:1,0.8,,,,🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.19173,Dense,"The Stack v2=900B tokens, 5 epochs to 4.3T tokens"
StarCoder,HF/ServiceNow,https://huggingface.co/bigcode/starcoderbase,15.5,1000,65:1,0.4,,,,🌋,May/2023,🟢,https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view,Dense,
PanGu 5.0 Super,Huawei,https://www.huaweicloud.com/intl/en-us/product/modelarts.html,1000,20000,20:1,14.9,,,,🌋,Jun/2024,🟡,https://www.huaweicentral.com/huawei-cloud-unveils-pangu-large-model-5-0/,MoE,https://x.com/faridofanani96/status/1804079517193113850/photo/1
YunShan,Huawei,,7,1748,250:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🔴,https://arxiv.org/abs/2312.17276,Dense,Finance + law fine-tune of PanGu-π
PanGu-Pi,Huawei,,7,1600,229:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🔴,https://arxiv.org/abs/2312.17276,Dense,"Dense, named PanGu-π"
PanGu-Sigma,Huawei,,1085,,,,,,,🌋,Mar/2023,🔴,https://arxiv.org/abs/2303.10845,MoE,Sparse. 1.085T parameters named PanGu-Σ.
PanGu-Coder,Huawei,,2.6,,,,,,,🌋,Jul/2022,🔴,https://arxiv.org/abs/2207.11280,Dense,Python via GH
ruGPT-3,Huawei/Sberbank,Sber Cloud,1.3,,,,,,,"🕸 ""170GB data""",Feb/2021,🟢,https://github.com/sberbank-ai/ru-gpts,Dense,Russian GPT-3 with input from Huawei
SmolLM2,Hugging Face,https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9,1.7,1000,589:1,0.1,42.3,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9,Dense,"Base and instruct versions, with Apache 2.0 license"
SmolLM,Hugging Face,https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966,1.7,1000,589:1,0.1,39.97,,,🆆 📚⬆ 🕸 🌋 ⚛️,Jul/2024,🟢,https://huggingface.co/blog/smollm,Dense,"Dataset includes new Cosmopedia v2 synthetic data. 135M and 360M models,each trained on 600B tokens from Smollm-Corpus. 1.7B model trained on 1T tokens from Smollm-Corpus."
Idefics2,Hugging Face,https://huggingface.co/HuggingFaceM4/idefics2-8b,8.4,,,,,,,🆆 🕸,Apr/2024,🟢,https://huggingface.co/blog/idefics2,Dense,Clone of Flamingo now using Mistral 7B. Named after Asterix and Obelix's dog Idefix (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS)
IDEFICS,Hugging Face,https://huggingface.co/spaces/HuggingFaceM4/idefics_playground,80,,,,,,,🆆 🕸,Aug/2023,🟢,https://huggingface.co/blog/idefics,Dense,Clone of Flamingo using Llama-1 65B. Named after Asterix and Obelix's dog Idefix (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS)
Tk-Instruct,Hugging Face,https://instructions.apps.allenai.org/demo,11,,,,,,,🆀🅰 🕸,Apr/2022,🟢,https://arxiv.org/abs/2204.07705,Dense,Based on T5.
Zephyr 141B-A35B,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,35,2000,58:1,0.9,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2403.07691,MoE,mixtral-8x22b finetune using Odds Ratio Preference Optimization (ORPO).
Zephyr,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,7.3,800,110:1,0.3,,33,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,Dense,Mistral with 'aligned' data removed from dataset
Granite 3.1 8B,IBM,https://huggingface.co/ibm-granite/granite-3.1-8b-instruct,8,12000,"1,500:1",1,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://github.com/ibm-granite/granite-3.1-language-models?tab=readme-ov-file,Dense,
Granite 3.0 8B,IBM,https://huggingface.co/ibm-granite/granite-3.0-8b-base,8,12000,"1,500:1",1,65.54,33.27,32.13,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,http://ibm.biz/granite-report,Dense,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
Granite-3.0-3B-A800M-Instruct,IBM,https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct,3,10000,"3,334:1",0.6,50.16,20.51,26.85,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,http://ibm.biz/granite-report,MoE,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
Granite Code,IBM,https://github.com/ibm-granite/granite-code-models,34,3500,103:1,1.1,50,,,🌋,May/2024,🟢,https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf,Dense,"MMLU=50 for 8B model only. Dataset: publicly available datasets (e.g., GitHub Code Clean, Starcoder data), public code repositories, and issues from GitHub."
MoLM,IBM,https://github.com/ibm/moduleformer,8,300,38:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2306.04640,MoE,ModuleFormer is based on the Sparse Mixture of Experts (MoE).
Granite,IBM,https://www.ibm.com/granite,13,2500,193:1,0.6,57,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://www.ibm.com/downloads/cas/X9W4O6BM,,"Original trained on 1T tokens, update 15/Feb/2024 trained on 2.5T tokens: granite-13b-chat-v2 (v2.1.0). ""At IBM, we curated 6.48TB of data to train our LLM Granite.13B. This was reduced to 2.07 TB after pre-processing, a 68% decrease."""
LongLLaMA,IDEAS/DeepMind,https://github.com/CStanKonrad/long_llama,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://arxiv.org/abs/2307.03170,Dense,256k context length
Yuan 2.0,IEIT,https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/README-EN.md,102.6,288,3:1,0.6,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://arxiv.org/abs/2311.15786,Dense,"Chinese + EN dataset include The Pile: DM, arxiv, wikipedia, book3, stack exchange, Freelaw and medical"
iFlytekSpark-13B,iFlyTek,https://gitee.com/iflytekopensource/iFlytekSpark-13B,13,3000,231:1,0.7,63.02,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://www.ithome.com/0/748/030.htm,Dense,"pre-trained on a massive high-quality data set with a total of more than 3 trillion tokens, and then fine-tuned on fine-tuned diversified alignment data.'"
Xinghuo 3.5 (Spark),iFlyTek,,200,4000,20:1,3,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://www.laitimes.com/en/article/6f50u_6vhbm.html,Dense,GPT-4 competitor. https://www.shine.cn/biz/tech/2401304331/
Jais,Inception,https://huggingface.co/inception-mbzuai,13,395,31:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Aug/2023,🟢,https://arxiv.org/abs/2308.16149,Dense,"Arabic, trained in Abu Dhabi, UAE using Cerebras."
ChuXin,Independent,https://huggingface.co/chuxin-llm/Chuxin-1.6B-Base,1.6,2300,"1,438:1",0.2,41.07,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.04828,Dense,"""results on the ”Needle In A Haystack”(NIAH) tests indicate that ChuXin-1M performs well across all context window lengths up to 1M."""
Parakeet,Independent,https://colab.research.google.com/drive/1gI8CM9Bz9ov0-E6aL2jF808rE56UtZyF?usp=sharing,0.378,3,8:1,0,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://news.ycombinator.com/item?id=39745700#39745702,Dense,Tiny model (378M) for testing
phi-CTNL,Independent,,0.1,0.01,1:1,0,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2309.08632,Dense,Satire. MMLU=100. 'phi-CTNL (pronounced “fictional”) that achieves perfect results across diverse academic benchmarks'
GPT-4chan,Independent,https://huggingface.co/ykilcher/gpt-4chan/discussions/4,6,,,,,,,🆆 📚⬆ 🕸 🌋,Jun/2022,🟢,https://arxiv.org/abs/2001.07487,Dense,Warning for inappropriate content. GPT-J.
Inflection-3 Pi (3.0),Inflection AI,https://developers.inflection.ai/,1200,20000,17:1,16.3,,,,🆆 📚 ⬆ 🕸,Oct/2024,🟢,https://developers.inflection.ai/docs,Dense,"Inference via Intel Gaudi® 3 128 GB, on-premise available. Minimum spend $100 credits."
Inflection-3 Productivity (3.0),Inflection AI,https://developers.inflection.ai/,1200,20000,17:1,16.3,,,,🆆 📚 ⬆ 🕸,Oct/2024,🟢,https://developers.inflection.ai/docs,Dense,"Inference via Intel Gaudi® 3 128 GB, on-premise available. Minimum spend $100 credits."
Inflection-2.5,Inflection AI,https://inflection.ai/inflection-2,1200,20000,17:1,16.3,85.5,,38.4,🆆 📚 ⬆ 🕸,Mar/2024,🟢,https://inflection.ai/inflection-2-5,Dense,
Inflection-2,Inflection AI,https://inflection.ai/inflection-2,1200,20000,17:1,16.3,,,,🆆 📚 ⬆ 🕸,Nov/2023,🟢,https://inflection.ai/inflection-2,Dense,"“now the 2nd best LLM in the world”. Finished training 19/Nov/2023, waiting for fine-tuning and release."
Inflection-1,Inflection AI,https://docs.google.com/forms/d/e/1FAIpQLScM9Iz1KzaRlfgDrYrldoPDnXbhO5LW3-hqmQCd56YpheEN7g/viewform,120,2000,17:1,1.6,,,,🆆 📚 ⬆ 🕸,Jun/2023,🟢,https://inflection.ai/assets/Inflection-1_0622.pdf,Dense,"Comparable with benchmarking results from InternLM 104B, 1-2% better. ‘Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.’"
Pi,Inflection AI,https://pi.ai/talk,60,,,,,,,🆆 📚⬆ 🕸 👥,May/2023,🟢,https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2022/03/08/reid-hoffman-has-set-up-a-new-ai-company-with-deepminds-co-founder.html,Dense,No indication of params/tokens. Devs from DeepMind.
ModernBERT,International,https://huggingface.co/blog/modernbert,0.395,2000,"5,064:1",0.1,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://arxiv.org/abs/2412.13663v1,Dense,"""a proper workhorse model, for retrieval, classification, etc."" https://bsky.app/profile/howard.fm/post/3ldod2afps62x"
Moxin-7B,International,https://github.com/moxin-org/Moxin-LLM,7,2000,286:1,0.4,60.97,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://arxiv.org/abs/2412.06845,Dense,"""Fully Open Source"" with pre-training code, configurations, training and fine-tuning datasets, and intermediate checkpoints."
DCLM-Baseline 7B 2.6T,International,https://huggingface.co/apple/DCLM-Baseline-7B,7,2600,372:1,0.4,63.7,,,🕸 🌋,Jun/2024,🟡,https://arxiv.org/abs/2406.11794,Dense,"New dataset: 240T tokens: 8× larger than previous SOTA dataset. DCLM-Pool is 240T, DCLM-Baseline is 3.8T: ""we combine our 3.8T DCLM-BASELINE with the StarCoder and ProofPile2 data to arrive at a 4.1T token dataset. We train a 7B model for 2.5T tokens"" and ""We release the DCLM benchmark, framework, models, and datasets at https://datacomp.ai/dclm."""
MAP-Neo,International,https://map-neo.github.io/,7,4500,643:1,0.6,58.14,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.19327,Dense,"""first fully open-sourced bilingual LLM with comparable performance to existing state-of-the-art LLMs... we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided."""
Aurora-M,International,https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407,15.5,2035,132:1,0.6,,,,🌋,Mar/2024,🟢,https://arxiv.org/abs/2404.00399,Dense,
Reader-LM,Jina AI,https://huggingface.co/jinaai/reader-lm-1.5b,1.54,2.5,2:1,0,,,,🕸,Sep/2024,🟢,https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/,Dense,"HTML->Markdown. Specialist small model; outperforms GPT-4o general model, does not outperform Gemini Pro 1.5."
jina-embeddings-v2,Jina AI,https://huggingface.co/jinaai/jina-embeddings-v2-base-en,0.435,,,,,,,🕸,Oct/2023,🟢,https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/,Dense,Alternative to text-embedding-ada-002. Related v1 paper: https://arxiv.org/abs/2307.11224
DocLLM,JPMorgan,,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🔴,https://arxiv.org/abs/2401.00908,Dense,Document spatial layout structure.
AceGPT,KAUST/Shenzhen,https://huggingface.co/FreedomIntelligence/AceGPT-13B,13,2010,155:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://github.com/FreedomIntelligence/AceGPT/tree/main,Dense,Arabic. Llama 2 + RLAIF
Skywork MoE 16x13B,Kunlun Tech,https://huggingface.co/Skywork/Skywork-MoE-Base,146,,,,77.4,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf,MoE,"CN + EN. ""(MoE) model with 146 billion parameters, 16 experts, and 22 billion activated parameters. This model is initialized from the pre-existing dense checkpoints of our Skywork-13B model."""
Skywork-13B,Kunlun Tech,,13,3200,247:1,0.7,62.7,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://arxiv.org/abs/2310.19341,Dense,CN + EN.
Helium-1,Kyutai,https://huggingface.co/kyutai/helium-1-preview-2b,2,2500,"1,250:1",0.2,51.2,,,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://kyutai.org/2025/01/13/helium.html,Dense,"""Helium-1 preview, an initial version of our new backbone language model with 2B parameters, targeting edge and mobile devices... We use token level distillation of a 7B parameters model to train Helium-1 preview."""
Helium 7B,Kyutai,https://moshi.chat/,7,1000,143:1,0.3,,,,⚛️,Jul/2024,🟢,https://youtu.be/hm2IJSKcYvo,Dense,"""1. The model is fine-tuned on 100K transcripts generated by Helium itself. 2. These transcripts are highly detailed, heavily annotated with emotion and style, and conversational. 3. Text to Speech Engine is further fine-tuned on 20 hours of audio recorded by Alice and licensed."""
OpenFlamingo-9B,LAION,https://huggingface.co/openflamingo/OpenFlamingo-9B,8.3,1000,121:1,0.3,,,,🕸 🌋,Mar/2023,🟢,https://laion.ai/blog/open-flamingo/,Dense,Uses LLaMA-7B. Demo: https://7164d2142d11.ngrok.app/
EXAONE-3.5,LG,https://huggingface.co/collections/LGAI-EXAONE/exaone-35-674d0e1bb3dcd2ab6f39dbb4,32,6500,204:1,1.5,78.3,,39.7,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://arxiv.org/abs/2412.04862,Dense,“EXAONE”=“EXpert AI for EveryONE”. Training tokens/ratio dropped from EXAONE-3 7.8B with 8T (Aug/2024) to this (Dec/2024) 7.8B with 9T to 32B with 6.5T.
EXAONE 3.0,LG,https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,7.8,8000,"1,026:1",0.8,,27.4,10.1,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2408.03541,Dense,“EXAONE”=“EXpert AI for EveryONE”
Alfred-40B-0723,LightOn,https://huggingface.co/lightonai/alfred-40b-0723,40,1000,25:1,0.7,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://www.lighton.ai/blog/lighton-s-blog-4/introducing-alfred-40b-0723-38,Dense,First finetuned version of Falcon with RLHF. Enterprise: https://www.lighton.ai/paradigm
VLM-4,LightOn,Muse,10,,,,,,,🕸,Mar/2022,🟢,https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2022/03/08/reid-hoffman-has-set-up-a-new-ai-company-with-deepminds-co-founder.html,Dense,Params corrected 25/Apr/2022
EON-8B,LinkedIn,https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform,8,15000,"1,875:1",1.2,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🔴,https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform,Dense,"""We found the EON-8B model (a domain-adapted Llama 3.1-8B variant) to be 75x and 6x cost effective in comparison to GPT-4 and GPT-4o respectively (Figure 4)... On tasks seen during training, the EON-8B model outperformed base Llama-3-8B-Instruct and its performance was comparable to SOTA GPT models."""
LFM-40B,Liquid AI,https://labs.perplexity.ai/,40,2000,50:1,0.9,78.76,55.63,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://www.liquid.ai/liquid-foundation-models,MoE,"40BA12B. Some controversy/concern over company. Liquid Foundation Models (LFM). ""Human preference optimization techniques have not been applied extensively to our models yet."""
K2,LLM360,https://huggingface.co/LLM360/K2,65,1400,22:1,1,64.8,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.llm360.ai/blog/several-new-releases-to-further-our-mission.html,Dense,"""K2-65B is a fully reproducible LLM outperforming Llama 2 70B using 35% less compute."""
MaLA-500,LMU,https://huggingface.co/MaLA-LM/mala-500,10,2000,200:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.13303,Dense,Extends Llama 2 7B to 10B using 534 languages.
LTM-2-mini,Magic,https://magic.dev/blog/100m-token-context-windows,20,2000,100:1,0.7,,,,🌋,Aug/2024,🔴,https://magic.dev/blog/100m-token-context-windows,Dense,Context=100M tokens equals ~10 million lines of code or ~750 novels.
LTM-1,Magic,https://magic.dev/blog/ltm-1,,,,,,,,🌋,Jun/2023,🔴,https://magic.dev/blog/ltm-1,Dense,Context window=5M
Llama 4,Meta AI,https://x.com/Ahmad_Al_Dahle/status/1851822285377933809,,,,,,,,,TBA,,,Dense,"Training Oct/2024-Feb/2025 on 100,000 H100s. Due 2025."
BLT,Meta AI,https://github.com/facebookresearch/blt,8,4500,563:1,0.6,57.4,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/,Dense,"Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance"
Large Concept Model,Meta AI,https://github.com/facebookresearch/large_concept_model?tab=readme-ov-file,7,2700,386:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/,Dense,"""autoregressive sentence prediction in an embedding space."" 7.7T tokens is a misprint, should be 2.2T as in paper."
Llama 3.3,Meta AI,https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct,70,15000,215:1,3.4,86,68.9,50.5,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,Dense,"Drop-in replacement for Llama 3.1 70B, comparable performance to Llama 3.1 405B."
Llama 3.2 90B,Meta AI,https://www.llama.com/,90,9000,100:1,3,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://www.llama.com/,Dense,Vision (VLM)
Llama 3.2 3B,Meta AI,https://www.llama.com/,3.21,9000,"2,804:1",0.6,63.4,,32.8,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://www.llama.com/,Dense,"Text (LLM). ""Pre-training. [For Llama 3.2 3B] We prune the models from their 8B siblings and use logits from the 8B and 70B models as token-level targets (token-level distillation). We then use knowledge distillation to recover performance."""
Llama 3.1 405B,Meta AI,https://www.meta.ai/,405,15600,39:1,8.4,88.6,73.3,51.1,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,Dense,Announce: https://ai.meta.com/blog/meta-llama-3-1/ Model card: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md
Chameleon,Meta AI,https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live,34,9200,271:1,1.9,65.8,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.09818,Dense,Multimodal
Llama 3 70B,Meta AI,https://meta.ai/,70,15000,215:1,3.4,82,52.8,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://ai.meta.com/blog/meta-llama-3/,Dense,Instruct MMLU-Pro=56.2
MobileLLM,Meta AI,https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95,1,1000,"1,000:1",0.1,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.14905,Dense,Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
CodeLlama-70B,Meta AI,https://huggingface.co/codellama/CodeLlama-70b-hf,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/,Dense,Paper link is to 34B from Aug/2023. This 70B model finished training Jan/2024.
SeamlessM4T-Large v2,Meta AI,https://seamless.metademolab.com/expressive/,2.3,,,,,,,🌋,Nov/2023,🟢,https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/,Dense,Based on NLLB and older models. https://github.com/facebookresearch/seamless_communication
Llama 2 Long,Meta AI,,70,2400,35:1,1.4,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🔴,https://arxiv.org/abs/2309.16039,Dense,"Unreleased to date. Context window=32,768 tokens (compare to Llama 2=4096 tokens)"
Code Llama 34B,Meta AI,https://github.com/facebookresearch/codellama,34,2600,77:1,1,,,,🆆 🕸,Aug/2023,🟢,https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/,Dense,"Outperforms GPT-3.5. Initial Llama 2 (2T tokens) trained on 500B tokens of code, 100B tokens of python"
Llama 2,Meta AI,https://www.llama2.ai/,70,2000,29:1,1.2,68.9,37.5,26.26,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,Dense,"Context window=4096. MMLU=68.9 (GPT-3.5=70.0, GPT-4=86.4)"
BlenderBot 3x,Meta AI,https://parl.ai/projects/bb3x/,175,300,2:1,0.8,,,,🆆 📚 ⬆ 🕸,Jun/2023,🟢,https://arxiv.org/abs/2306.04707,Dense,OPT-175B with new dialogue data
LIMA,Meta AI,,65,,,,,,,🆆 📚⬆ 🕸 🌋,May/2023,🔴,https://arxiv.org/abs/2305.11206,Dense,"LLaMA-65B with nearly no fine-tuning, no RLHF"
LLaMA-65B,Meta AI,Weights leaked: https://github.com/facebookresearch/llama/pull/73/files,65,1400,22:1,1,68.9,,,🆆 📚⬆ 🕸 👥,Feb/2023,🟢,https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/,Dense,"Researchers only, noncommercial only. 'LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B.'"
Toolformer+Atlas 11B+NLLB 54B,Meta AI,Replicated: https://github.com/conceptofmind/toolformer,6.7,402,60:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Feb/2023,🔴,https://arxiv.org/abs/2302.04761,Dense,Based on GPT-J 6.7B + access to other models via API
OPT-IML,Meta AI,https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML,175,300,2:1,0.8,,,,🆆 📚⬆ 🕸,Dec/2022,🟢,https://arxiv.org/abs/2212.12017,Dense,Instruct
Galactica,Meta AI,https://galactica.org/,120,450,4:1,0.8,52.6,,,📚,Nov/2022,🟢,https://galactica.org/static/paper.pdf,Dense,scientific only
Atlas,Meta AI,,11,40,4:1,0.1,47.9,,,🆆 🕸,Aug/2022,🟢,https://arxiv.org/abs/2208.03299,Dense,
BlenderBot 3,Meta AI,blenderbot.ai (US only),175,300,2:1,0.8,,,,🆆 📚 ⬆ 🕸,Aug/2022,🟢,https://github.com/facebookresearch/ParlAI/blob/main/projects/bb3/BB3_main_tech_report.pdf,Dense,
NLLB,Meta AI,Github (train/deploy),54.5,,,,,,,🌋,Jul/2022,🟢,https://research.facebook.com/publications/no-language-left-behind/,MoE,"54.5B MOE, 3.3B dense. 200+ languages"
OPT-175B,Meta AI,HF (train/deploy),175,300,2:1,0.8,,,,🆆 📚 ⬆ 🕸,May/2022,🟢,https://arxiv.org/abs/2205.01068,Dense,Only 30B available (Jun/2022)
InCoder,Meta AI,https://huggingface.co/spaces/facebook/incoder-demo,6.7,,,,,,,"GH, StackOverflow",Apr/2022,🟢,https://arxiv.org/abs/2204.05999,Dense,Python and JavaScript
SeeKeR,Meta AI,,2.7,,,,,,,🆆 📚 ⬆ 🕸,Mar/2022,🟢,https://arxiv.org/abs/2203.13224,Dense,BART and compared to GPT-2
CM3,Meta AI,,13,,,,,,,🆆 🕸,Jan/2022,🟢,https://arxiv.org/abs/2201.0752,Dense,LLM with multimodal capabilities
XGLM,Meta AI,,7.5,,,,,,,🕸,Dec/2021,🟢,https://arxiv.org/abs/2112.10668,Dense,"Multilingual: 30 languages, 16 families."
Fairseq,Meta AI,"TS, Goose",1100,,,,,,,🆆 📚⬆ 🕸 🕸 🕸,Dec/2021,🟢,https://arxiv.org/abs/2112.10684,Dense,13B & 1100B param models.
BlenderBot 2.0,Meta AI,,9.4,,,,,,,👥 🌋,Jul/2021,🟢,https://parl.ai/projects/blenderbot2/,Dense,Chatbot
Megatron-11B,Meta AI,InferKit,11,2200,200:1,0.5,,,,🆆 📚 ⬆ 🕸,Apr/2020,🟢,https://github.com/pytorch/fairseq/tree/main/examples/megatron_11b,Dense,My favourite model until GPT-3 and GPT-4 came along: https://github.com/facebookresearch/fairseq/blob/main/examples/megatron_11b/README.md
RoBERTa,Meta AI,Hugging Face,0.355,2200,"6,198:1",0.1,27.9,,,🆆 📚 ⬆ 🕸,Jul/2019,🟢,https://arxiv.org/abs/1907.11692,Dense,"calcs: ""In total, this batch size and number of steps corresponds to pre-training on 235 ≈ 34B tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2 35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training."" https://arxiv.org/pdf/1910.10683.pdf MMLU shows RoBERTa-base 125M only=27.9 (not 355M)"
MAI-1,Microsoft,https://arstechnica.com/information-technology/2024/05/microsoft-developing-mai-1-language-model-that-may-compete-with-openai-report/,500,10000,20:1,7.5,,,,,TBA,,https://www.reuters.com/technology/microsoft-readies-new-ai-model-compete-with-google-openai-information-reports-2024-05-06/,Dense,Potential failed training run 2024. MAI=Microsoft artificial intelligence. MSFT CTO statement: https://archive.md/XRSgS
phi-4,Microsoft,https://huggingface.co/microsoft/phi-4,14,10000,715:1,1.2,84.8,70.4,56.1,⚛️,Dec/2024,🟢,https://arxiv.org/abs/2412.08905,Dense,Use unsloth: https://huggingface.co/unsloth/phi-4-GGUF & https://www.reddit.com/r/singularity/comments/1i0kso4/i_fixed_4_bugs_in_microsofts_opensource_phi4_model/
GRIN MoE,Microsoft,https://huggingface.co/microsoft/GRIN-MoE,60,4025,68:1,1.6,79.4,,,⚛️,Sep/2024,🟢,https://huggingface.co/microsoft/GRIN-MoE/blob/main/GRIN_MoE.pdf,MoE,"16x3.8B ""only 6.6B activate parameters"". GRIN=GRadient-INformed. ""GRIN MoE is pre-trained on 4T tokens as a Causal Language Model. The same training dataset has been used to train Phi-3 dense models"""
phi-3.5-MoE,Microsoft,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,60,4900,82:1,1.8,78.9,54.3,36.8,⚛️,Aug/2024,🟢,https://arxiv.org/abs/2407.13833https://arxiv.org/abs/2407.13833,MoE,
phi-3.5-mini,Microsoft,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,3.8,3400,895:1,0.4,69,47.4,30.4,⚛️,Aug/2024,🟢,https://arxiv.org/abs/2407.13833,Dense,
SpreadsheetLLM,Microsoft,,1760,13000,8:1,15.9,,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🔴,https://arxiv.org/abs/2407.09025v1,Dense,"Notable finetune of GPT4-0125-preview ""outperforming the vanilla approach by 25.6% in GPT4’s in-context learning setting"""
Causal Axioms,Microsoft,,0.067,1.2,18:1,0,,,,⚛️,Jul/2024,🔴,https://arxiv.org/abs/2407.07612v1,Dense,"""the training dataset follows a specific structure, we develop a custom tokenizer. Alphanumeric node names are tokenized at a character level, while special terms such as ‘causes’, ‘Does’, ‘cause’, ‘Yes’, and ‘No’ are tokenized at the word level... Our training setup consists of around 175k instances of sequential chains with size of chains ranging from 3 to 6 nodes... All models are trained for 100 epochs. [LifeArchitect.ai estimate is 12 tokens per node x 6 nodes x 175,000 instances x 100 epochs = 1.26B tokens]"" Based on GPT-2 arch."
YOCO,Microsoft,https://github.com/microsoft/unilm/tree/master/YOCO,3,1600,534:1,0.2,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://arxiv.org/abs/2405.05254,Dense,"With Tsingua. You Only Cache Once (YOCO). Long context ""1M context length with near-perfect needle retrieval accuracy"""
TinyStories,Microsoft,https://huggingface.co/roneneldan/TinyStories-33M,0.033,50,"1,516:1",0,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2305.07759,Dense,Precursor to phi.
phi-3-medium,Microsoft,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,14,4800,343:1,0.9,78.2,55.7,,⚛️,Apr/2024,🟢,https://arxiv.org/abs/2404.14219,Dense,"Preview only, benchmarks being investigated as of May/2024."
phi-3-mini,Microsoft,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,3.8,3300,869:1,0.4,68.8,45.7,,⚛️,Apr/2024,🟢,https://arxiv.org/abs/2404.14219,Dense,"""phi3-mini can be quantized to 4-bits so that it only occupies ≈ 1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running natively on-device and fully offline achieving more than 12 tokens per second."""
WizardLM-2-8x22B,Microsoft,https://huggingface.co/MaziyarPanahi/WizardLM-2-8x22B-GGUF,141,2000,15:1,1.8,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://wizardlm.github.io/WizardLM2/,MoE,Base model = mistral-8x22b.
BitNet b1.58,Microsoft,https://huggingface.co/1bitLLM/bitnet_b1_58-xl,70,2000,29:1,1.2,,,,🆆 📚🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.17764,Dense,
WaveCoder-DS-6.7B,Microsoft,,6.7,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🔴,https://arxiv.org/abs/2312.14187,Dense,"To obtain WaveCoder models, We choose StarCoder-15B, CodeLLaMa (7B and 13B), DeepseekCoder-6.7B as the base model and fine-tune all the base model for 3 epochs"
Transformers-Arithmetic,Microsoft,,0.1,0.3,3:1,0,,,,⬆,Nov/2023,🔴,https://arxiv.org/abs/2311.14737,Dense,Proving maths is not memorized. Uses GPT-2-style model. Sébastien Bubeck
Orca 2,Microsoft,,13,2001,154:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟡,https://arxiv.org/abs/2311.11045,Dense,"Llama 2 13B (2T) -> Orca 2 (GPT-4 finetune). Still an imitation model, overhyped: The False Promise of Imitating Proprietary LLMs https://arxiv.org/abs/2305.15717"
Phi-2,Microsoft,https://replicate.com/lucataco/phi-2,2.7,1400,519:1,0.2,,,,⚛️,Nov/2023,🟢,https://huggingface.co/microsoft/phi-2,Dense,https://twitter.com/SebastienBubeck/status/1724854157004190095
Florence-2,Microsoft,https://huggingface.co/microsoft/Florence-2-large,0.771,,,,,,,🌋,Nov/2023,🟢,https://arxiv.org/abs/2311.06242,Dense,"VLM, Flamingo alt"
Kosmos-2.5,Microsoft,,1.3,,,,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🔴,https://arxiv.org/abs/2309.11419,Dense,
Phi-1.5,Microsoft,https://huggingface.co/microsoft/phi-1_5,1.3,150,116:1,0,,,,📚⚛️,Sep/2023,🟢,https://arxiv.org/abs/2309.05463,Dense,Textbooks only. 30B-token dataset
WizardLM,Microsoft,https://huggingface.co/WizardLM/WizardLM-70B-V1.0,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 👥,Aug/2023,🟢,https://github.com/nlpxucan/WizardLM,Dense,Assume Llama-2 fine-tune. Outperforms text-davinci-003. May merge this entry with the Apr/2023 7B release
Kosmos-2,Microsoft,https://44e505515af066f4.gradio.app/,1.6,360,225:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Jun/2023,🟢,https://arxiv.org/abs/2306.14824,Dense,Proto-AGI. Multimodal large language model (MLLM). a multimodal large language model with grounding capability built upon KOSMOS-1
Phi-1,Microsoft,,1.3,51,40:1,0,,,,📚⚛️,Jun/2023,🔴,https://arxiv.org/abs/2306.11644,Dense,"Code model. ‘breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than 200M tokens.’"
Orca,Microsoft,https://aka.ms/orca-lm,13,1000,77:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jun/2023,🟡,https://arxiv.org/abs/2306.02707,Dense,"LLaMA -> Vicuna -> Orca (GPT-4 finetune). Still an imitation model, overhyped: The False Promise of Imitating Proprietary LLMs https://arxiv.org/abs/2305.15717"
WizardLM,Microsoft,https://6f8173a3550ed441ab.gradio.live/,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 👥,Apr/2023,🟢,https://arxiv.org/abs/2304.12244,Dense,LLaMA 7B self-instructed fine-tune.
Kosmos-1,Microsoft,,1.6,360,225:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Feb/2023,🔴,https://arxiv.org/abs/2302.14045,Dense,"Proto-AGI. Multimodal large language model (MLLM). Raven’s Progressive Matrices as real images, not digits as in testing of text-davinci-003 at https://lifearchitect.ai/ravens/"
FLAME,Microsoft,,0.06,9,150:1,0,,,,🌋,Jan/2023,🔴,https://arxiv.org/abs/2301.13779,Dense,"T5 for Excel formulas, very small 60M params, ""We start from a dataset of 927M formulas"" estimate 10x multiplier for 9B tokens"
PACT,Microsoft,https://github.com/microsoft/PACT,,0.03,,0,,,,🌋,Oct/2022,🟢,https://arxiv.org/abs/2209.11133,Dense,"Trained on ~5TB data, 2GB model download. 'In general we see an improvement in model performance as we increase the number of training tokens. Interestingly, larger models did not necessarily result in better performance for robot navigation. Even though larger models consistently presented better loss values for action prediction on a static dataset, (Fig. 7 b), when it comes to real-time deployment the larger network capacity introduces inference delays that become a disadvantage and lead to earlier crashes. For example, while LiDAR perception measurements arrive to the vehicle every 0.077s (13Hz), the largest model of 24 layers takes on average 0.023s for inference with a RTX3090 GPU, roughly 40% longer the 3 layer model (0.016s). These time differences can amount to even larger performance gaps in small embedded systems, and further emphasize the importance of multiple downstream task architectures sharing a common representation branch for real-time robotics applications.'"
Z-Code++,Microsoft,,0.71,500,705:1,0.1,,,,🆆 🕸,Aug/2022,🔴,https://arxiv.org/abs/2208.09770v1,Dense,"abstractive text summarization, 710M, outperforms PaLM 540B. ""Due to the limited computational resource, Z-Code++LARGE is trained with only 500B tokens instead of 1T tokens as that for mT5 training."""
GODEL-XL,Microsoft,,2.7,,,,,,,🆆 📚⬆ 🕸 👥,Jun/2022,🟢,https://arxiv.org/abs/2206.11309#microsoft,Dense,"XL: GPT-3 175B in paper, GPT-J 2.7B released"
DeBERTaV3,Microsoft,,1.5,162,108:1,0.1,,,,🆆 📚 ⬆ 🕸,Nov/2021,🟢,https://arxiv.org/abs/2111.09543,Dense,RoBERTa=162B token dataset.
MT-NLG,Microsoft/NVIDIA,,530,270,1:1,1.3,,,,🆆 📚⬆ 🌋 🕸 🕸,Oct/2021,🔴,https://arxiv.org/abs/2201.11990,Dense,
MiniMax-Text-01,MiniMax,https://github.com/MiniMax-AI/MiniMax-01,456,7200,16:1,6,88.5,75.7,54.4,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://arxiv.org/abs/2501.08313,MoE,"A45.9B. ""The pre-training corpus for MiniMax-Text-01 encompasses a comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and programming code... repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times... Our findings indicate that low-quality data suffer a substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs"" Login playground: https://www.hailuo.ai/"
Mistral Small 3,Mistral,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,24,8000,334:1,1.5,80.73,54.37,45.3,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,Dense,"MMLU=base, -Pro=base, GPQA=instruct. ""When quantized, Mistral Small 3 can be run privately on a single RTX 4090 or a Macbook with 32GB RAM."" ""Mistral Small 3 is neither trained with RL nor synthetic data"""
Pixtral Large,Mistral,https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411,124,6000,49:1,2.9,,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://mistral.ai/news/pixtral-large/,Dense,Open-weights multimodal model built on top of Mistral Large 2.
Ministral 8B,Mistral,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,8,6000,750:1,0.7,65,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://mistral.ai/news/ministraux/,Dense,"""Introducing the world’s best edge models"""
Pixtral-12b-240910,Mistral,https://huggingface.co/mistralai/Pixtral-12B-2409,12,6000,500:1,0.9,69.2,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://mistral.ai/news/pixtral-12b/,Dense,"""Pixtral was trained to be a drop-in replacement for Mistral Nemo 12B."""
Mistral Large 2,Mistral,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,123,8000,66:1,3.3,84,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://mistral.ai/news/mistral-large-2407/,Dense,Fits on a single node for inference.
NeMo,Mistral,https://huggingface.co/mistralai/Mistral-Nemo-Base-2407,12,2000,167:1,0.5,68,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://mistral.ai/news/mistral-nemo/,Dense,"With NVIDIA. ""Drop-in replacement of Mistral 7B"". ""trained using Megatron-LM, part of NVIDIA NeMo, with 3,072 H100 80GB Tensor Core GPUs"" https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/"
Codestral Mamba,Mistral,https://huggingface.co/mistralai/mamba-codestral-7B-v0.1,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://mistral.ai/news/codestral-mamba/,Dense,"""Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length."""
Mathstral,Mistral,https://huggingface.co/mistralai/mathstral-7B-v0.1,7,2000,286:1,0.4,63.47,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://mistral.ai/news/mathstral/,Dense,"""We’re contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning."""
Codestral,Mistral,https://huggingface.co/mistralai/Codestral-22B-v0.1,22,2000,91:1,0.7,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://mistral.ai/news/codestral/,Dense,Fluent in 80+ programming languages
mixtral-8x22b,Mistral,https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1,141,2000,15:1,1.8,77.75,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://mistral.ai/news/mixtral-8x22b/,MoE,"MoE=22Bx8, seq=65536."
Mistral Small,Mistral,https://chat.mistral.ai/chat,7,3000,429:1,0.5,72.2,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://mistral.ai/news/mistral-large/,Dense,Optimised for latency and cost.
Mistral Large,Mistral,https://poe.com/Mistral-Large,300,8000,27:1,5.2,81.2,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://mistral.ai/news/mistral-large/,Dense,"MMLU=81.2 (same as Flan-PaLM 2 340B, higher than PaLM 2 340B MMLU=78.3), 32k context window. API only (not open source)."
miqu 70b,Mistral,https://huggingface.co/miqudev/miqu-1-70b,70,3000,43:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://huggingface.co/miqudev/miqu-1-70b,Dense,"Leaked, proper version soon: https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/"
Mistral-medium,Mistral,https://poe.com/,180,3500,20:1,2.6,75.3,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://mistral.ai/news/la-plateforme/,Dense,"MMLU=75.3% (GPT-3.5-turbo 20B=70%, Llama 2 70B=68.9%)"
mixtral-8x7b-32kseqlen,Mistral,https://www.together.ai/blog/mixtral,46.7,8000,172:1,2,70.6,43.3,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2401.04088,MoE,"MoE=7Bx8, aka mistral-small. 'Concretely, Mixtral has 45B total parameters but only uses 12B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12B model.'"
Mistral 7B,Mistral,https://huggingface.co/mistralai,7.3,800,110:1,0.3,,30.9,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://mistral.ai/news/announcing-mistral-7b/,Dense,"Apache 2.0, Sliding Window Attention (SWA) to handle longer sequences at smaller cost"
JetMoE-8B,MIT,https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat,8,1250,157:1,0.3,49.2,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://huggingface.co/jetmoe/jetmoe-8b,MoE,
Kimi k1.5,Moonshot AI,https://github.com/MoonshotAI/kimi-k1.5?tab=readme-ov-file,500,15000,30:1,9.1,87.4,,51.5,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://arxiv.org/abs/2501.12599,Dense,"Reasoning. ""our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities---e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista---matching OpenAI's o1"". GPQA score is my estimate from pp13–14, noting that ""the scores above come from an internal long-cot model with much smaller model size than k1.5 long-CoT model."""
k0-math,Moonshot AI,https://kimi.moonshot.cn/,100,2000,20:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://www.globaltimes.cn/page/202411/1323248.shtml,Dense,"Reasoning, maths only. Very little info available. Chinese. Long context. No paper."
Kimi Chat,Moonshot AI,https://kimi.moonshot.cn/,100,2000,20:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://www.chinadaily.com.cn/a/202403/22/WS65fce476a31082fc043be1b1.html,Dense,Chinese. Long context. No paper.
DBRX,MosaicML,https://huggingface.co/spaces/databricks/dbrx-instruct,132,12000,91:1,4.2,73.7,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,MoE,"MoE. Trained for $10M on 3,072 NVIDIA H100s connected by 3.2Tbps Infiniband."
MPT,MosaicML,https://huggingface.co/mosaicml/mpt-7b,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://twitter.com/NaveenGRao/status/1654496162492084227,Dense,Llongboi' -Apache 2.0 license suitable for commercial use. -Base 7B LLM trained on 1T tokens outperforms LLaMA and GPT3. -64K+ context length. -$200k to train from scratch.
MPT,MosaicML,https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly,1.3,200,154:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://twitter.com/jefrankle/status/1649060478910357504,Dense,More 1B models coming with different datasets. Many more.
NexusRaven-V2 13B,Nexusflow.ai,https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo,,,,,,,,🌋,Dec/2023,🟢,https://github.com/nexusflowai/NexusRaven-V2/tree/master,Dense,Based on CodeLlama. 'surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.'
GPT4All-LoRa,Nomic,https://github.com/nomic-ai/gpt4all,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Mar/2023,🟢,https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf,Dense,chatbot trained on ~800k GPT-3.5-Turbo Generations based on LLaMa
DisTrO 15B,Nous Research,https://distro.nousresearch.com/,15,100,7:1,0.1,23.48,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://github.com/NousResearch/DisTrO?tab=readme-ov-file,Dense,"""About 14 DGXes scattered around the globe. Sometimes more sometimes less, it varies depending on availability. On average, around 112 H100s."" https://x.com/bloc97_/status/1863675225810043331 ""we introduce DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware."""
OLMo-Bitnet-1B,Nous Research,https://huggingface.co/NousResearch/OLMo-Bitnet-1B,1,60,60:1,0,,,,🌋,Apr/2024,🟢,https://arxiv.org/abs/2402.17764,Dense,1.58-bit quantized (ternary weights) means we can run a 70B model in ~14GB VRAM. See also BitNet b1.58
OtterHD-8B,NTU,https://github.com/Luodian/Otter,8,737,93:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://arxiv.org/abs/2311.04219,Dense,Evolution of Persimmon-9.3B and Fuyu 8B
Cosmos Nemotron 34B,NVIDIA,https://build.nvidia.com/nvidia/cosmos-nemotron-34b,34,400000,"11,765:1",12.3,,,,🌋,Jan/2025,🟢,https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai,Dense,"VLM. MMMU=47.33. ""VILA project becomes part of Cosmos Nemotron family"" https://github.com/NVlabs/Cosmos-Nemotron Vision Encoder: SigLIP-400M, Language Encoder: Yi-34B https://blogs.nvidia.com/blog/nemotron-model-families/"
Cosmos 1.0,NVIDIA,https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World,14,400000,"28,572:1",7.9,,,,🌋,Jan/2025,🟢,https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai,Dense,"WFM (world foundation model). ""The models range in size from 4 billion to 14 billion parameters, with Nano being the smallest and Ultra being the largest... ""Cosmos WFM models, were trained on 9,000 trillion tokens [9,000T] from 20 million hours of real-world human interactions, environment, industrial, robotics, and driving data..."" https://techcrunch.com/2025/01/06/nvidia-releases-its-own-brand-of-world-models/ Actual working: https://lifearchitect.ai/cosmos/"
Llama-3.1-Nemotron-70B,NVIDIA,https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct,70,15000,215:1,3.4,,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct/modelcard,Dense,Related paper: https://arxiv.org/abs/2410.01257
nGPT,NVIDIA,https://github.com/lucidrains/nGPT-pytorch,1,400,400:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://arxiv.org/abs/2410.01131,Dense,"""a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized...reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length."""
NLVM 1.0,NVIDIA,https://huggingface.co/nvidia/NVLM-D-72B,72,18000,250:1,3.8,82,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🟢,https://arxiv.org/abs/2409.11402,Dense,"Flamingo clone. ""we use Qwen2-72B-Instruct as the default text-only LLM backbone. We also employ Nous-Hermes-2-Yi-34B for ablation study and faster experimentation... we use InternViT-6B as the default vision encoder"""
Minitron-4B,NVIDIA,https://huggingface.co/nvidia/Minitron-4B-Base,4,94,24:1,0.1,58.6,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://arxiv.org/abs/2407.14679,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
Minitron-8B,NVIDIA,https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base,4,94,24:1,0.1,63.8,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
Nemotron-4-340B,NVIDIA,https://build.nvidia.com/nvidia/nemotron-4-340b-instruct,340,9000,27:1,5.8,81.1,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf,Dense,"Open-source equiv of Mar/2023 GPT-4 (1760MoE≈340B, 13T), same param count but 2x the tokens of May/2023 PaLM 2 (340B, 3.6T), competitor to Nov/2023 Grok-1 (314B, 6T). Trained on 6,144 H100s. ~1.3TB for inference. 50+ natural and 40+ coding languages. Trained between December 2023 and May 2024. MMLU 0-shot for instruct=78.7, 5-shot for base=81.1. Permalink for paper: https://research.nvidia.com/publication/2024-06_nemotron-4-340b"
Nemotron-4 15B,NVIDIA,,15,8000,534:1,1.2,64.2,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.16819,Dense,
Audio Flamingo,NVIDIA,https://huggingface.co/spaces/nvidia/audio-flamingo-demo,1,20,20:1,0,,,,🌋,Feb/2024,🟡,https://arxiv.org/abs/2402.01831,Dense,Project page: https://audioflamingo.github.io/
Nemotron-3 22B,NVIDIA,https://huggingface.co/nvidia/nemotron-3-8b-base-4k,22,3800,173:1,1,54.4,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/,Dense,"8B released, 22B internal."
Nemotron-2 43B,NVIDIA,,43,3800,89:1,1.3,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🔴,https://arxiv.org/abs/2311.09528,Dense,Used to train HelpSteer (16/Nov/2023): https://arxiv.org/abs/2311.09528
Retro 48B,NVIDIA,,48,1200,25:1,0.8,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://arxiv.org/abs/2310.07713,Dense,the largest LLM pretrained with retrieval before instruction tuning.'
GPT-2B-001,NVIDIA,https://huggingface.co/nvidia/GPT-2B-001,2,1100,550:1,0.2,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://huggingface.co/nvidia/GPT-2B-001,Dense,No paper yet
VIMA,NVIDIA,Open: https://vimalabs.github.io/,0.2,,,,,,,🌋,Oct/2022,🟢,https://arxiv.org/abs/2210.03094,Dense,
NeMo Megatron-GPT 20B,NVIDIA,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,20,,,,,,,🆆 📚⬆ 🕸 🌋,Sep/2022,🟢,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,Dense,
Megatron-LM,NVIDIA,,8.3,800,97:1,0.3,,,,🆆 📚 ⬆ 🕸,Sep/2019,🟢,https://arxiv.org/abs/1909.08053,Dense,
GPT-5,OpenAI,https://lifearchitect.ai/whats-in-gpt-5/,5400,114000,22:1,,,,,,TBA,,,MoE,"Due 2025. Showing dense param count, but will be MoE model."
GPT-6,OpenAI,https://lifearchitect.ai/gpt-6/,,,,,,,,,TBA,,,,Due 2025.
o4,OpenAI,https://lifearchitect.ai/o4/,,,,,,,,,TBA,,,,Due 2025.
o3-mini,OpenAI,https://chatgpt.com/?model=o3-mini,20,13000,650:1,1.7,,,77,🆆 📚⬆ 🕸 🌋 ⚛️,Jan/2025,🟢,https://openai.com/index/o3-mini-system-card/,Dense,"Reasoning. GPQA=79.7 for 'high' thinking. ALPrompt 2025H1=1/5. My analysis is that this model’s performance is very poor, with responses often becoming disordered and illogical. OpenAI compared o3-mini to OpenAI’s software engineers, and it performed very poorly (o3-mini=0%, o1=12%). ""o3-mini models have the lowest performance, with scores of 0%… We suspect o3-mini’s low performance is due to poor instruction following and confusion about specifying tools in the correct format. The model often attempts to use a hallucinated bash tool rather than python despite constant, multi-shot prompting and feedback that this format is incorrect. This resulted in long conversations that likely hurt its performance."" (o3-mini paper, p31)"
GPT-4b,OpenAI,,8,4000,500:1,0.6,,,,🌋,Jan/2025,🔴,https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/,Dense,"Protein sequence model. ""The model was trained on examples of protein sequences from many species, as well as information on which proteins tend to interact with one another. While that’s a lot of data, it’s just a fraction of what OpenAI’s flagship chatbots were trained on, making GPT-4b an example of a “small language model” that works with a focused data set."" https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/"
o3,OpenAI,https://lifearchitect.ai/o3/,5000,100000,20:1,74.5,,,87.7,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://lifearchitect.ai/o3/,MoE,Reasoning. SoTA model for Dec/2024. Parameter estimate is very rough centrepoint for range 400B-52T.
o1-2024-12-17,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,91.8,,75.7,🆆 📚⬆ 🕸 🌋,Dec/2024,🔴,https://openai.com/index/o1-and-new-tools-for-developers/,MoE,"""o1-2024-12-17 sets new state-of-the-art results on several benchmarks, improving cost-efficiency and performance."""
o1,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,92.3,91,79,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://openai.com/index/introducing-chatgpt-pro/,MoE,"Reasoning. ""a version of our most intelligent model that thinks longer for the most reliable responses"" System card about safety only: https://cdn.openai.com/o1-system-card-20241205.pdf"
gpt-4o-2024-11-20,OpenAI,https://chat.com/,200,20000,100:1,6.7,85.7,,46,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://platform.openai.com/docs/models#gpt-4o,MoE,"Material decrease in benchmark scores (GPQA: -13.37%, MMLU: -3.38%) compared to Aug/2024. Pruned? Quantized? https://github.com/openai/simple-evals"
o1-preview,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,92.3,91,78.3,🆆 📚⬆ 🕸 🌋,Sep/2024,🔴,https://openai.com/index/introducing-openai-o1-preview/,MoE,Reasoning.
GPT-4o mini,OpenAI,https://chatgpt.com/,8,13000,"1,625:1",1.1,82,,40.2,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,MoE,"Omnimodel. ""OpenAI would not disclose exactly how large GPT-4o mini is, but said it’s roughly in the same tier as other small AI models, such as Llama 3 8b, Claude Haiku and Gemini 1.5 Flash."" https://techcrunch.com/2024/07/18/openai-unveils-gpt-4o-mini-a-small-ai-model-powering-chatgpt/ ""tested GPT-4o to identify potential risks, which we have addressed and plan to share the details of in the forthcoming GPT-4o system card and Preparedness scorecard."" And related paper about instruction hierarchy: https://arxiv.org/abs/2404.13208"
CriticGPT,OpenAI,,3,1000,334:1,0.2,,,,👥,Jun/2024,🔴,https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf,Dense,"""LLM Critics Help Catch LLM Bugs"" Announce: https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"
GPT-4o,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,88.7,72.6,53.6,🆆 📚⬆ 🕸 🌋,May/2024,🔴,https://openai.com/index/gpt-4o-system-card/,MoE,"gpt-4o-2024-05-13 no longer easily available, so hidden in the Model Table rankings. Omnimodel. ‘[GPT-4o is] likely an early checkpoint of GPT-5’. https://twitter.com/drjimfan/status/1790089671365767313 ELO: https://twitter.com/LiamFedus/status/1790064963966370209 Demo: https://youtu.be/DQacCB9tDaw"
gpt-4-turbo-2024-04-09,OpenAI,https://chat.openai.com/,70,13000,186:1,3.2,86.5,63.7,49.1,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,"This is such a significantly better model that I've added it here. This GPQA=46.5%, old GPT-4 GPQA=36%. https://twitter.com/EpochAIResearch/status/1778463039932584205 MMLU scores are unclear, but may have improved by 1%: https://twitter.com/OpenAI/status/1778602770784002136. Final benchmarks are here: https://archive.md/6Cc0Z"
GPT-4 Turbo,OpenAI,https://chat.openai.com/,70,13000,186:1,3.2,86.4,,46.5,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,https://openai.com/blog/new-models-and-developer-products-announced-at-devday
GPT-4 MathMix,OpenAI,,1760,13000,8:1,15.9,,,,🆆 📚⬆ 🕸 🌋,May/2023,🔴,https://arxiv.org/abs/2305.20050,MoE,"Unreleased, includes step by step research"
"GPT-4 Classic (gpt-4-0314 & gpt-4-0613, non-Turbo)",OpenAI,https://chat.openai.com/,1760,13000,8:1,15.9,86.4,,35.7,🆆 📚⬆ 🕸 🌋,Mar/2023,🟢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,Original MMLU=86.4. MMLU=90.1 with prompting. Proto-AGI. 1.76T parameters MoE.
ChatGPT (gpt-3.5-turbo),OpenAI,https://chat.openai.com/,20,,,,70,,28.1,🆆 📚 ⬆ 🕸,Nov/2022,🟢,https://openai.com/blog/chatgpt,Dense,"Instruct with strict policies (""extremely limited"")"
text-davinci-003,OpenAI,https://chat.openai.com/,,,,,,,,🆆 📚 ⬆ 🕸,Nov/2022,🟢,https://openai.com/blog/chatgpt,Dense,
6.9B FIM,OpenAI,,6.9,100,15:1,0.1,,,,🆆 📚 ⬆ 🕸,Jul/2022,🔴,https://arxiv.org/pdf/2207.14255.pdf,Dense,"Several models: 8 sizes, NLP, Code, FIM/non-FIM. 100B tokens for 6.9B params... beyond chinchilla"
Codex,OpenAI,Playground,12,,,,,,,🕸,Aug/2021,🟢,https://arxiv.org/abs/2107.03374,Dense,Code
GPT-3,OpenAI,Sunset/deprecated :-(,175,300,2:1,0.8,43.9,,,🆆 📚 ⬆ 🕸,May/2020,🟢,https://arxiv.org/abs/2005.14165,Dense,No RLHF (base only). Popular: 3.1M wpm. Dataset: https://lifearchitect.ai/whats-in-my-ai/
GPT-2,OpenAI,Hugging Face,1.5,10,7:1,0,32.4,,,⬆,Feb/2019,🟢,https://openai.com/blog/better-language-models/,Dense,Reddit outbound only
GPT-1,OpenAI,Hugging Face,0.117,0.003,1:1,0,,,,📚,Jun/2018,🟢,https://openai.com/blog/language-unsupervised/,Dense,"Books only. ""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens."" =3,276,800"
Teuken-7B,OpenGPT-X,https://huggingface.co/openGPT-X/Teuken-7B-instruct-research-v0.4,7,4000,572:1,0.6,50,,,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2410.03730,Dense,"24 EU languages (60% non-English): bg, cs, da, de, el, en, es, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv. https://opengpt-x.de/models/teuken-7b-de/ & paper date is Sep/2024."
Orion-14B,OrionStar,https://github.com/OrionStarAI/Orion,14,2500,179:1,0.6,69.6,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.12246,Dense,"English, Chinese, Japanese, Korean, and other languages."
pplx-70b-online,Perplexity,https://labs.perplexity.ai/,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://blog.perplexity.ai/blog/introducing-pplx-online-llms,Dense,Web access. Higher 'freshness' and 'truth' scores.
Pleias 1.0,PleIAs,https://huggingface.co/PleIAs/Pleias-3b-Preview,3,1086,362:1,0.2,,,,🕸 ⚛️,Dec/2024,🟢,https://huggingface.co/blog/Pclanglais/common-models,Dense,"Trained on the Jean Zay supercomputer, 192x H100s for 20 days. Dataset is new CC + Synthetic: https://huggingface.co/datasets/PleIAs/common_corpus"
METAGENE-1,Prime Intellect,https://huggingface.co/metagene-ai,7,370,53:1,0.2,,,,🌋,Jan/2025,🟢,https://metagene.ai/metagene-1-paper.pdf,Dense,"Llama-2-7B base. ""METAGENE-1 is a 7B parameter metagenomic foundation model designed for pathogen detection and pandemic monitoring, trained on over 1.5 trillion base pairs [∼370 billion tokens (≈1.69 trillion base pairs)] of DNA and RNA collected via metagenomic sequencing of wastewater."""
INTELLECT-1,Prime Intellect,https://huggingface.co/PrimeIntellect/INTELLECT-1,10,1000,100:1,0.3,49.89,,28.32,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf,Dense,"Training complete 22/Nov/2024. Fully distributed training: ""the first decentralized training run of a 10-billion-parameter model, inviting anyone to contribute compute and participate. This brings us one step closer towards open source AGI."""
RakutenAI-7B,Rakuten Group,https://huggingface.co/Rakuten/RakutenAI-7B,7,3000,429:1,0.5,61.31,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.15484,Dense,Japanese. Mistral 7B derivative.
202305-refact2b-mqa-lion,Refact,https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/,1.6,,,,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟡,https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/,Dense,"LiON vs Adam, code, RedPajama+The Stack"
Reka Core,Reka AI,https://poe.com/RekaCore,300,10000,34:1,5.8,83.2,,38.2,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model
Reka Edge,Reka AI,https://chat.reka.ai/,7,4500,643:1,0.6,63.1,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,
Reka Flash,Reka AI,https://poe.com/RekaFlash,21,5000,239:1,1.1,73.5,,34,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,My testing shows very poor performance equiv with tiny model
Yasa-1,Reka AI,https://reka.ai/announcing-our-multimodal-ai-assistant/,,,,,,,,🌋,Oct/2023,🟡,https://reka.ai/product/,Dense,"Multi-modal. No public arch info. Researchers from DeepMind, Google, Baidu and Meta building enterprise models"
Yasa,Reka AI,https://reka.ai/product/,,,,,,,,,Jun/2023,🟡,https://reka.ai/product/,Dense,"No public arch info. Researchers from DeepMind, Google, Baidu and Meta building enterprise models"
Hanooman,Reliance,,40,,,,,,,🌋,Feb/2024,🟢,https://www.hanooman.ai/,Dense,"11 Indian languages like Hindi, Tamil, and Marathi"
YuLan-Mini,Renmin,https://github.com/RUC-GSAI/YuLan-Mini,2.4,1080,450:1,0.2,51.79,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://arxiv.org/abs/2412.17743,Dense,"""1.08T tokens for training. Among them are 481B English web data, 138B general English knowledge, 227B code pre-training data, 16.7B code instruction data, 93.8B mathematics pre-training data, 15.5B mathematics instruction data, and 108B Chinese data."""
YuLan-Base-12B,Renmin,https://github-com.translate.goog/RUC-GSAI/YuLan-Chat?_x_tr_sl=zh-CN&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=sc,12,1700,142:1,0.5,55.7,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://arxiv.org/abs/2406.19853,Dense,"""YuLan's training is finished on Jan, 2024 and has achieved performance on par with state-of-the-art LLMs across various English and Chinese benchmarks."""
Sonus-1 Reasoning,Rubik's AI,https://chat.sonus.ai/sonus/,405,15000,38:1,8.2,90.15,73.1,67.3,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://sonus.ai/blog/sonus-1,Dense,"Likely a Llama 3.1 405B wrapper. ALPrompt 2024H1=5/5. ALPrompt 2024H2=2/5. ALPrompt 2025H1=1/5. This is a strange model: slow and smart, but not as performant as o1. No arch details at all."
Deepthought-8B,Ruliad,https://chat.ruliad.co/,8,15000,"1,875:1",1.2,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha,Dense,Reasoning. No evals. Llama 3.1 8B base.
RWKV-7 Goose,RWKV,https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7,0.4,332,830:1,0,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7,Dense,"RWKV (pronounced RwaKuv) is an RNN: ""multilingual, supporting over 100 languages and code."". Full run is 332B tokens of 3.1T dataset."
RWKV-v6 Finch,RWKV,https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2,7.63,2500,328:1,0.5,,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://huggingface.co/BlinkDL/rwkv-6-world,Dense,RWKV (pronounced RwaKuv) is an RNN: https://twitter.com/BlinkDL_AI/status/1787834625211158562
RWKV-v5 EagleX,RWKV,https://huggingface.co/recursal/EagleX_1-7T,7.52,1700,227:1,0.4,40.14,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b,Dense,RWKV (pronounced RwaKuv) is an RNN: Built on the RWKV-v5 architecture (a linear transformer with 10-100x+ lower inference cost)
RWKV-v5 Eagle 7B,RWKV,https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2,7.52,1100,147:1,0.3,33.21,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers,Dense,"RWKV (pronounced RwaKuv) is an RNN: Built on the RWKV-v5 architecture (a linear transformer with 10-100x+ lower inference cost), Trained on 1.1 Trillion Tokens across 100+ languages. Original paper: https://arxiv.org/abs/2305.13048"
RWKV-4,RWKV,https://huggingface.co/BlinkDL,14,332,24:1,0.2,,,,🆆 📚 ⬆ 🕸 🌋,Nov/2022,🟢,https://arxiv.org/abs/2305.13048,Dense,RWKV (pronounced RwaKuv) is an RNN: https://www.reddit.com/r/MachineLearning/comments/yxt8sa/r_rwkv4_7b_release_an_attentionfree_rnn_language/
Sailor2,Sail,https://huggingface.co/spaces/sail/Sailor2-20B-Chat,20,18510,926:1,2,,,,🆆 📚⬆ 🕸 🌋,Dec/2024,🟢,https://github.com/sail-sg/sailor2,Dense,SEA languages. Continual pretraining based on Qwen2.5. Project page: https://sea-sailor.github.io/blog/sailor2/
Sailor,Sail,https://huggingface.co/sail,7,200,29:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2404.03608v1,Dense,"SEA languages. Based on Qwen-1.5. https://github.com/sail-sg/sailor-llm ""Generally Sailor models consume around 200B tokens, completing a full pass through the SailCraft corpus once. However, the Sailor-0.5B model undergoes training with 400B tokens, equivalent to 2 epochs."""
EvoLLM-JP,Sakana AI,https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B,10,800,80:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.13187,Dense,"Japanese. Model merge 'our EvoLLM-JP-A is a merge of shisa-gamma-7b-v1, Arithmo2-Mistral-7B, and Abel7B-002' https://sakana.ai/evolutionary-model-merge/"
SFR-LLaMA-3.1-70B-Judge,Salesforce,https://blog.salesforceairesearch.com/sfr-judge/,70,15000,215:1,3.4,,,,🆆 📚⬆ 🕸 🌋,Sep/2024,🔴,https://arxiv.org/abs/2409.14664,Dense,"Code coming soon: https://github.com/SalesforceAIResearch/SFRJudge ""we opt to focus on datasets that evaluate modern (2023 and beyond) LLM responses, as older datasets likely contain lower quality responses from less capable models, with correspondingly stale annotations. We supplement human-annotated data with synthetically generated data to endow our judge models with specific capabilities (e.g., following fine-grained rubrics in evaluation)"""
xLAM,Salesforce,https://huggingface.co/Salesforce/xLAM-8x22b-r,141,,,,,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://huggingface.co/Salesforce/xLAM-8x22b-r,MoE,64K sequence length. Released under Apache-2.0.
XGen,Salesforce,https://github.com/salesforce/xgen,7,1500,215:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://blog.salesforceairesearch.com/xgen/,Dense,8K sequence length. Released under Apache-2.0.
CodeT5+,Salesforce,https://huggingface.co/Salesforce/codet5p-16b,16,,,,,,,🕸 🌋,May/2023,🟢,https://arxiv.org/abs/2305.07922,Dense,"InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001'"
CodeGen,Salesforce,"TS, Goose",16,,,,,,,"🕸 BigQuery, BigPython",Mar/2022,🟢,https://arxiv.org/abs/2203.13474,Dense,Code
CodeT5,Salesforce,,0.7,,,,,,,"🕸 BigQuery, BigPython",Mar/2022,🟢,https://arxiv.org/abs/2109.00859,Dense,Code. Large introduced in https://arxiv.org/pdf/2207.01780.pdf
EvaByte,SambaNova,https://huggingface.co/EvaByte/EvaByte,6.5,1500,231:1,0.3,50.6,,,🆆 📚⬆ 🕸 🌋,Jan/2025,w,https://hkunlp.github.io/blog/2025/evabyte/,Dense,"""efficient byte-level processing at scale... [compared to tokenizer-based LMs:] 5x less training data, excelling in coding tasks, and decoding up to 2x faster. Its token-free design also brings added flexibility, avoiding tokenizer quirks while naturally extending to multimodal applications without any architecture tweaks."""
Samba-1,SambaNova,https://trysambanova.ai/,1400,20000,15:1,17.6,,,,🌋,Feb/2024,🟡,https://sambanova.ai/press/secure-one-trillion-parameter-generative-ai-model-for-the-enterprise,CoE,CoE: Collection of experts: Llama2 7B / 13B / 70B Mistral 7B DeepSeek Coder 1.3B / 6.7B / 33B Falcon 40B DePlot CLIP Llava
Gauss,Samsung,https://koreajoongangdaily.joins.com/news/2023-11-08/business/tech/Samsung-unveils-generative-AI-model-Gauss/1908889,7,,,,,,,,Nov/2023,🟡,https://koreajoongangdaily.joins.com/news/2023-11-08/business/tech/Samsung-unveils-generative-AI-model-Gauss/1908889,Dense,"Gauss Language specializing in generating texts, Gauss Code on software and code description and Gauss Image for image creation."
sarvam-2b,Sarvam AI,https://huggingface.co/sarvamai/sarvam-2b-v0.5,2,4000,"2,000:1",0.3,,,,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://huggingface.co/sarvamai/sarvam-2b-v0.5,Dense,"Indic languages supported are: Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu."
mGPT,Sber,Hugging Face,13,,,,,,,🆆 🕸,Apr/2022,🟡,https://arxiv.org/abs/2204.07580,Dense,60 languages. Only 1.3B model available
SenseNova 5.5,SenseTime,https://platform.sensenova.cn/home#/home,600,10000,17:1,8.2,,,,⚛️,Jul/2024,🟢,https://www.sensetime.com/en/news-detail/51168278?categoryId=1072,MoE,"""The model training was based on over 10TB tokens [sic, taken as 10T tokens instead of 10TB=2T tokens] of high-quality training data, including a large amount of synthetically-generated reasoning chain data, which help to enhance its reasoning capabilities."" & ""The updates include SenseNova 5o, the first real-time multimodal model in China, which provides a new AI interaction model on par with GPT-4o’s streaming interaction capabilities"""
SenseNova 5.0,SenseTime,,600,10000,17:1,8.2,84.78,,42.93,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://news.futunn.com/en/post/41290101/a-large-shangtang-multi-modal-model-with-600-billion-parameters,MoE,GPT-4 scale; low media coverage; no demo in Western world. https://www.techinasia.com/sensetime-pauses-trading-stock-rises-30-model-launch
Meta-Transformer,Shanghai AI Laboratory/CUHK,https://github.com/invictus717/MetaTransformer,2,,,,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://arxiv.org/abs/2307.10802,Dense,"Proto-AGI. 12 modalities (text, image, point cloud, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, Inertial Measurement Unit (IMU), and graph data)."
OREAL-32B,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/OREAL-32B,32,4000,125:1,1.2,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Feb/2025,🟢,https://arxiv.org/abs/2502.06781,Dense,Reasoning. OREAL=Outcome REwArd-based reinforcement Learning.
InternLM3,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm3-8b-instruct,8,4000,500:1,0.6,76.6,57.6,37.4,🆆 📚⬆ 🕸 🌋,Jan/2025,🟢,https://huggingface.co/internlm/internlm3-8b-instruct,Dense,"""InternLM3 is trained on only 4 trillion high-quality tokens, saving more than 75% of the training cost compared to other LLMs of similar scale."" Playground: https://internlm-chat.intern-ai.org.cn/"
InternVL 2.5,Shanghai AI Laboratory/SenseTime,https://huggingface.co/spaces/OpenGVLab/InternVL,78,18120,233:1,4,86.1,71.1,49,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://arxiv.org/abs/2412.05271,Dense,"Reasoning. Benchmarks are estimates based on Qwen2.5 72B Instruct as the base LLM (InternVL 2.5=InternViT-6B-448px-V2.5 5.5B + Qwen2.5-72B-Instruct). ""Notably, Qwen2-VL processed a cumulative total of 1.4T tokens, while our InternVL2.5-78B is trained on just ∼120B tokens [of vision].""Dataset... we identify repetitive generation as one of the most detrimental issues. In many open-source or synthetic datasets, a small number of repetitive samples—comprising merely thousands of examples in our Stage 2 data mixture—can cause the model to spiral into repetitive loops, particularly in long-form outputs or CoT reasoning tasks. This phenomenon undermines the effectiveness of test-time scaling strategies. To address this challenge and support future research, we designed an efficient data filtering pipeline to remove low-quality samples, thereby minimizing the risk of repetitive generation."" Repo: https://github.com/OpenGVLab/InternVL"
InternLM2.5,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm2_5-20b-chat,20,2600,130:1,0.8,73.5,,38.4,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_7b.md,Dense,"""The release of InternLM2.5 series contains 7B model size for now and we are going to release the 1.8B and 20B versions soon"" [20B released around 1/Aug/2024]"
InternLM2,Shanghai AI Laboratory/SenseTime,https://github.com/InternLM/InternLM,20,2600,130:1,0.8,67.7,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2403.17297,Dense,
InternLM,Shanghai AI Laboratory/SenseTime,https://internlm-org.translate.goog/?_x_tr_sl=zh&_x_tr_tl=en,104,1600,16:1,1.4,,,,🆆 📚⬆ 🕸 🌋,Jun/2023,🔴,https://github.com/InternLM/InternLM-techreport,Dense,"Outperforms ChatGPT, LLaMA on RACE-h, Chinese + English"
Viking,Silo AI,,33,2000,61:1,0.9,,,,🌋,Apr/2024,🟢,https://www.silo.ai/blog/viking-7b-13b-33b-sailing-the-nordic-seas-of-multilinguality,Dense,"Viking uses an architecture similar to Llama 2, with flash attention, rotary embeddings, grouped query attention and supports a 4k sequence length'"
Poro,Silo AI,https://huggingface.co/LumiOpen/Poro-34B,34.2,1000,30:1,0.6,,,,🌋,Feb/2024,🟢,https://www.silo.ai/blog/viking-7b-13b-33b-sailing-the-nordic-seas-of-multilinguality,Dense,"Uses a BLOOM architecture with ALiBi embeddings to allow for context window extrapolation. While model architecture for the initial model has been kept simple, future models under progress will support additional capabilities, such as flash attention, rotary embeddings and grouped query attention.'"
NExT-GPT,Singapore,https://next-gpt.github.io/,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2309.05519,Dense,Multimodal. Vicuna 7B + other modalities
Arctic,Snowflake AI Research,https://arctic.streamlit.app/,480,3500,8:1,4.3,67.3,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/,Hybrid,"""Arctic uses a unique Dense-MoE Hybrid transformer architecture. It combines a 10B dense transformer model with a residual 128×3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating."""
Apollo,SRIBD/CUHK,https://apollo.llmzoo.com/,7,2500,358:1,0.4,,,,🆆 📚🕸 🌋,Mar/2024,🟢,https://arxiv.org/abs/2403.03640,Dense,Qwen 1.8B as base. Medical focus.
Stable Code Instruct 3B,Stability AI,https://huggingface.co/stabilityai/stable-code-instruct-3b,2.7,560,208:1,0.1,,,,🌋,Mar/2024,🟢,https://stability.ai/news/introducing-stable-code-instruct-3b,Dense,"Context window=16,384. Trained on The Stack dataset."
Stable Beluga 2.5,Stability AI,,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://stability.ai/news/putting-the-ai-supercomputer-to-work,Dense,"Mentioned in Stability release about Intel chips 11/Mar/2024, availablity unknown"
Japanese StableLM Alpha 7B,Stability AI,https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b,7,750,108:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Aug/2023,🟢,https://stability.ai/blog/stability-ai-new-jplm-japanese-language-model-stablelm,Dense,Best-performing openly available language model for Japanese speakers.
Stable Code 3B,Stability AI,https://huggingface.co/stabilityai/stablecode-completion-alpha-3b-4k,2.7,560,208:1,0.1,,,,🌋,Aug/2023,🟢,https://stability.ai/blog/stablecode-llm-generative-ai-coding,Dense,"Context window=16,384. Trained on The Stack dataset."
Stable Beluga 2,Stability AI,https://huggingface.co/stabilityai/FreeWilly2,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models,Dense,Fine-tuned Llama 2. Non-commercial use license. Codename was FreeWilly2
Stable Beluga 1,Stability AI,https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor,65,1400,22:1,1,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models,Dense,Fine-tuned LLaMA-1. Non-commercial use license. Codename was FreeWilly1
StableLM,Stability AI,https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat,65,1500,24:1,1,,,,🆆 📚⬆ 🕸 🌋,Apr/2023,🟢,https://github.com/stability-AI/stableLM/,Dense,"contains 1.5 trillion tokens, roughly 3x the size of The Pile. These models will be trained on up to 1.5 trillion tokens. The context length for these models is 4096 tokens."
s1-32B,Stanford,https://github.com/simplescaling/s1,32,18000,563:1,2.5,,,59.6,🆆 📚⬆ 🕸 🌋 ⚛️,Feb/2025,🟢,https://arxiv.org/abs/2501.19393,Dense,"Reasoning. Based on Qwen2.5-32B-Instruct. ""we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end. This can lead the model to doublecheck its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24)."""
TTT-Linear,Stanford,https://github.com/test-time-training/ttt-lm-jax,1.3,26,20:1,0,,,,📚,Aug/2024,🟢,https://arxiv.org/abs/2407.04620,Dense,"Test-Time Training (TTT) layers. Real-time learning by Stanford, UC, and Meta. Potential for frontier models in 2025+."
Med-Flamingo,Stanford,https://github.com/snap-stanford/med-flamingo,8.3,1000,121:1,0.3,,,,🕸 🌋,Jul/2023,🟢,https://arxiv.org/abs/2307.15189,Dense,"Uses LAION OpenFlamingo 9B, based on LLaMA-7B text + 1.3B vision"
Alpaca,Stanford,https://crfm.stanford.edu/alpaca/,7,1000,143:1,0.3,,,,🆆 📚⬆ 🕸 👥,Mar/2023,🟢,https://github.com/tatsu-lab/stanford_alpaca,Dense,Stanford Alpaca: An Instruction-following LLaMA model'
Diffusion-LM,Stanford,Github (train/deploy),0.3,,,,,,,🌋 👥,May/2022,🟢,https://arxiv.org/abs/2205.14217,Dense,GPT-J with synthetic data
Step-2,StepFun,https://platform.stepfun.com/#language-step2,1000,13000,13:1,12,82.9,63,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://platform.stepfun.com/docs/llm/text,MoE,"Launched early Jul/2024: https://pandaily.com/stepfun-releases-three-large-models-of-the-step-series/ ""StepFun, founded in April 2023 with the mission to “Scale-up possibilities for everyone,” unites top talent in artificial intelligence from both domestic and international backgrounds, and is dedicated to advancing toward AGI. The company has already launched the Step series of foundation models, which includes Step-2, a cutting-edge trillion-parameter Mixture of Experts (MoE) language model; Step-1.5V, a powerful multimodal large model; and Step-1V, an innovative image generation model, among others."""
TinyLlama,SUTD/Independent,https://github.com/jzhang38/TinyLlama,1.1,3000,"2,728:1",0.2,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.02385,Dense,"Overtrained' using 2,727 tokens per parameter. Dataset was 1T: 3 epochs to 3T seen. Singapore"
Hunyuan-Large,Tencent,https://huggingface.co/tencent/Tencent-Hunyuan-Large,389,7000,18:1,5.5,89.9,60.2,42.4,🆆 📚⬆ 🕸 🌋 ⚛️,Nov/2024,🟢,https://arxiv.org/abs/2411.02265,MoE,"Hunyuan-Large is pre-trained on 7T tokens, which contains nearly 1.5T tokens of high-quality and diverse synthetic data.' '389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens'"
FuseLLM,Tencent,https://github.com/fanqiwan/FuseLLM,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.10491,Dense,"Fusion of Llama-2-7B (2T tok), OpenLLaMA-7B (2T tok), and MPT-7B (1T tok)."
LLaMA Pro,Tencent,https://huggingface.co/TencentARC/LLaMA-Pro-8B,8.3,2080,251:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://arxiv.org/abs/2401.02415,Dense,We pre-train LLAMA PRO’s expanded blocks on 80B tokens using open-source code and math data for 2830 GPU Hours (16 NVIDIA H800 GPUs for about 7 days).
Hunyuan,Tencent,https://www.tencent.com/en-us/articles/2201685.html,100,2000,20:1,1.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2402.01723v1,Dense,
Fox-1,TensorOpera,https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1,1.6,3005,"1,879:1",0.2,44.99,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://arxiv.org/abs/2411.05281,Dense,Gold standard for dataset documentation
BOLT2.5B,ThirdAI,https://huggingface.co/spaces/thirdai/BOLT2.5B,2.5,40,16:1,0,,,,🕸,Sep/2023,🟢,https://medium.com/thirdai-blog/introducing-the-worlds-first-generative-llm-pre-trained-only-on-cpus-meet-thirdai-s-bolt2-5b-10c0600e1af4,Dense,CPU trained
Falcon 3,TII,https://huggingface.co/tiiuae/Falcon3-10B-Base,10,16000,"1,600:1",1.3,73.1,42.5,34.1,🆆 📚⬆ 🕸 🌋 ⚛️,Dec/2024,🟢,https://huggingface.co/blog/falcon3,Dense,"""We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens... upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data."""
Falcon Mamba 7B,TII,https://falconllm.tii.ae/falcon-models.html,7,6000,858:1,0.7,62.11,14.47,8.05,🕸,Aug/2024,🟢,https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html,Dense,https://huggingface.co/spaces/tiiuae/falcon-mamba-playground
Falcon 2 11B,TII,https://huggingface.co/tiiuae/falcon-11B,11,5500,500:1,0.8,58.37,,,🆆 📚⬆ 🕸 🌋,May/2024,🟢,https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas,Dense,Announce: https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas
Falcon 180B,TII,https://huggingface.co/spaces/tiiuae/falcon-180b-demo,180,3500,20:1,2.6,70.6,,,🆆 📚⬆ 🕸 🌋,Sep/2023,🟢,https://arxiv.org/abs/2311.16867,Dense,Major milestone for open source models (largest open dense model to date).
Falcon,TII,TS,40,1000,25:1,0.7,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model,Dense,Abu Dhabi
NOOR,TII,,10,,,,,,,🆆 📚 🕸 🇦🇪,Apr/2022,🔴,https://www.tii.ae/news/technology-innovation-institute-announces-launch-noor-worlds-largest-arabic-nlp-model,Dense,"Arabic. ""World’s largest high-quality cross-domain Arabic dataset, combining web data with books, poetry, news articles, and technical information"""
StripedHyena 7B,Together,https://api.together.xyz/playground/language/togethercomputer/StripedHyena-Hessian-7B,7.65,,,,,,,🌋,Dec/2023,🟢,https://www.together.ai/blog/stripedhyena-7b,Dense,"RedPajama (C4), new arch beyond just Transformers"
LLaMA-2-7B-32K,Together,https://huggingface.co/togethercomputer/LLaMA-2-7B-32K,7,2000,286:1,0.4,,,,🆆 📚⬆ 🕸 🌋,Jul/2023,🟢,https://together.ai/blog/llama-2-7b-32k,Dense,32k context window instead of 4k (Llama 2)
GPT-NeoX-Chat-Base-20B,Together,https://huggingface.co/spaces/togethercomputer/OpenChatKit,20,,,,33.6,,,🆆 📚 ⬆ 🕸 🌋,Mar/2023,🟢,https://github.com/togethercomputer/OpenChatKit,Dense,"instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai. '"
GPT-JT,Together,https://huggingface.co/spaces/togethercomputer/GPT-JT,6,,,,,,,🆆 📚 ⬆ 🕸 🌋,Nov/2022,🟢,https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai,Dense,
MiniCPM-2.4B,Tsinghua,https://github.com/OpenBMB/MiniCPM/,2.4,1100,459:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/abs/2404.06395,Dense,MoE option=https://huggingface.co/openbmb/MiniCPM-MoE-8x2B
Eurus,Tsinghua,https://huggingface.co/collections/openbmb/eurus-660bc40bec5376b3adc9d1c5,70,2000,29:1,1.2,,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://huggingface.co/collections/openbmb/eurus-660bc40bec5376b3adc9d1c5,Dense,Fine-tune of Mistral-7B and CodeLlama-70B.
xTrimoPGLM,Tsinghua,,100,1000,10:1,1.1,,,,🌋,Jul/2023,🔴,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,Dense,Protein language model
OpenChat,Tsinghua,https://huggingface.co/openchat/openchat_3.5,13,2000,154:1,0.5,,,,🆆 📚⬆ 🕸 🌋,Sep/2022,🟢,https://arxiv.org/abs/2309.11235,Dense,Llama 2 13B -> OpenChat 13B
CodeGeeX,Tsinghua,,13,850,66:1,0.4,,,,🌋,Sep/2022,🟢,https://github.com/THUDM/CodeGeeX,Dense,
GLM-130B,Tsinghua,https://huggingface.co/spaces/THUDM/GLM-130B,130,400,4:1,0.8,,,,🆆 📚 ⬆ 🕸,Aug/2022,🟢,https://arxiv.org/abs/2210.02414,Dense,"50% English (200B tokens), so included here"
MatMul-Free LM,UCSC,https://github.com/ridgerchu/matmulfreellm,2.7,100,38:1,0.1,,,,🆆 📚⬆ 🕸 🌋,Jun/2024,🟢,https://arxiv.org/abs/2406.02528,Dense,"""we explore alternative methods for mixing tokens without relying on matrix multiplications."" Compared with Transformer++ based on Llama-2, not to be confused with the pre-GPT-3 American Express Transformer++ paper from 2/Mar/2020. Instead, Transformer++ is defined in the Mamba paper: 'Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and SwiGLU MLP (Shazeer 2020)'"
Raven,UI/NVIDIA,,11,40,4:1,0.1,,,,🆆 🕸,Aug/2023,🔴,https://arxiv.org/abs/2308.07922,Dense,RAG Atlas
TowerLLM,Unbabel,https://unbabel.com/meet-towerllm/,7,1020,146:1,0.3,,,,🆆 📚⬆ 🕸 🌋,Feb/2024,🟢,https://arxiv.org/abs/2402.17733,Dense,"Commercial product, Llama-2 as base."
SOLAR-10.7B,Upstage AI,https://huggingface.co/upstage/SOLAR-10.7B-v1.0,10.7,,,,,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.15166,Dense,South Korean. Llama-2 arch. SOTA for its size (Dec/2023).
Guanaco,UW,https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi,65,1400,22:1,1,,,,🆆 📚⬆ 🕸 🌋,May/2023,🟢,https://arxiv.org/abs/2305.14314,Dense,LLaMA-65B via QLoRA
Mockingbird,Vectara,https://vectara.com/platform/,9,1000,112:1,0.3,,,,🆆 📚⬆ 🕸 🌋 ⚛️,Jul/2024,🟢,https://vectara.com/blog/mockingbird-a-rag-and-structured-output-focused-llm/,Dense,"""At <10B parameters it's an LLM trained to provide optimal results for RAG and structured outputs."""
MotionLM,Waymo,,0.09,,,,,,,🌋,Sep/2023,🔴,https://arxiv.org/abs/2309.16534,Dense,LLM for autonomous vehicle forecasting. https://youtu.be/jrMMNmN21I8?t=1560
GAIA-1,Wayve,https://wayve.ai/thinking/scaling-gaia-1/,9,,,,,,,🌋,Sep/2023,🔴,https://arxiv.org/abs/2309.17080,Dense,"World model, generates video. Uses T5-large 770M for language + all vision parameters"
WeLM,Wechat,https://welm.weixin.qq.com/docs/playground/,10,300,30:1,0.2,,,,🆆 📚⬆ 🕸 🌋,Sep/2022,🟢,https://arxiv.org/abs/2209.10372,Dense,13% English tokens and 87% Chinese
YAYI 2,Wenge,https://huggingface.co/wenge-research/yayi2-30b,30,2650,89:1,0.9,80.5,,,🆆 📚⬆ 🕸 🌋,Dec/2023,🟢,https://arxiv.org/abs/2312.14862,Dense,Dataset=240TB filtered to 10.6TB for 2.65T tokens
Palmyra-Med-70B,Writer,https://huggingface.co/Writer/Palmyra-Med-70B-32K,70,1200,18:1,1,,,,🌋,Jul/2024,🟢,https://writer.com/blog/palmyra-med-fin-models/,Dense,Medical. MMLU Medical Genetics=94.0
Palmyra-Fin-70B,Writer,https://huggingface.co/Writer/Palmyra-Fin-70B-32K,70,1200,18:1,1,,,,🌋,Jul/2024,🟢,https://writer.com/blog/palmyra-med-fin-models/,Dense,"Financial. ""across a variety of real-world financial use cases. It outperformed popular models like Claude 3.5 Sonnet, GPT-4o, and Mixtral-8x7b"""
Palmyra X,Writer,,72,1200,17:1,1,70.2,,,🌋,Jan/2024,🟢,https://writer.com/blog/palmyra-helm-benchmark/,Dense,"Palmyra X V2, Palmyra X V3, Palmyra X V4. https://venturebeat.com/ai/why-writers-palmyra-llm-is-the-little-ai-model-that-could-for-enterprises/"
Palmyra,Writer,https://huggingface.co/models?search=palmyra,20,300,15:1,0.3,,,,🌋,Feb/2023,🟢,https://writer.com/blog/palmyra/,Dense,"Only up to 5B available open-source 'trained on over 300 billion tokens of text data, and the size of the resulting model is over 20 billion parameters. ' https://writer.com/product/cowrite/"
Grok-3,xAI,https://lifearchitect.ai/whats-in-grok/,928,36200,40:1,,,,,,TBA,,,MoE,"Training Jul-Dec 2024 on 100,000 H100s. Due 2025H1. Showing dense param count, but will be MoE model."
Grok-2,xAI,https://x.com/i/grok,400,15000,38:1,8.2,87.5,75.5,56,🆆 📚⬆ 🕸 🌋,Aug/2024,🟢,https://x.ai/blog/grok-2,Dense,"MMLU-Pro=75.5=SOTA. Claude 3.5S MMLU-Pro=72.83. ""Grok-2 has been tested on the LMSYS leaderboard under the name ""sus-column-r."" At the time of this blog post, it is outperforming both Claude 3.5 Sonnet and GPT-4-Turbo."" [Alan: Grok is Heinlein, Sixth Column is also Heinlein: https://en.wikipedia.org/wiki/Sixth_Column ]"
Grok-1.5,xAI,https://grok.x.ai/,180,6000,34:1,3.5,81.3,,,🆆 📚⬆ 🕸 🌋,Mar/2024,🟢,https://x.ai/blog/grok-1.5,MoE,Context=128k.
Grok-1,xAI,https://grok.x.ai/,314,6000,20:1,4.6,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🟢,https://github.com/xai-org/grok-1,MoE,Context window=8192. UI: https://twitter.com/TobyPhln/status/1721053802235621734
Grok-0,xAI,https://grok.x.ai/,33,2000,61:1,0.9,,,,🆆 📚⬆ 🕸 🌋,Nov/2023,🔴,https://web.archive.org/web/20231105051542/https://x.ai/,Dense,"Announced Nov/2023, trained Jul/2023"
Xmodel-LM,XiaoduoAI,https://github.com/XiaoduoAILab/XmodelLM,1.1,2064,"1,877:1",0.2,25.9,,,🆆 📚⬆ 🕸 🌋,Nov/2024,🟢,https://arxiv.org/abs/2411.10083,Dense,SLM
Lemur,XLANG Lab,https://github.com/OpenLemur/Lemur,70,2090,30:1,1.3,,,,🆆 📚⬆ 🕸 🌋,Oct/2023,🟢,https://arxiv.org/abs/2310.06830,Dense,https://arxiv.org/abs/2310.06830
YaLM 100B,Yandex,Github (train/deploy),100,300,3:1,0.6,,,,🆆 📚⬆ 🕸,Jun/2022,🟢,https://github.com/yandex/YaLM-100B,Dense,"Megatron-LM clone, Russian/English: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6"
GLM-4,Zhipu AI (Tsinghua),https://open.bigmodel.cn/,200,4000,20:1,3,,,,🆆 📚⬆ 🕸 🌋,Jan/2024,🟢,https://pandaily.com/zhipu-ai-unveils-glm-4-model-with-advanced-performance-paralleling-gpt-4/,Dense,Best Chinese model to date based on analysis. Follows OpenAI roadmap. MMLU=81.5. 'hundreds of billions of parameters' https://www.chatglm.cn/
Zamba2-7B,Zyphra,https://huggingface.co/Zyphra/Zamba2-7B,7,3100,443:1,0.5,67.2,,,🆆 📚⬆ 🕸 🌋,Oct/2024,🟢,https://www.zyphra.com/post/zamba2-7b,Dense,"Mamba2 ""trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM"""
Zamba2-small,Zyphra,https://huggingface.co/Zyphra/Zamba2-2.7B,2.7,3100,"1,149:1",0.3,55,,,🆆 📚⬆ 🕸 🌋,Jul/2024,🟢,https://www.zyphra.com/post/zamba2-small,Dense,Mamba2
Zamba 7B,Zyphra,https://huggingface.co/Zyphra/Zamba-7B-v1,7,1050,150:1,0.3,57.72,,,🆆 📚⬆ 🕸 🌋,Apr/2024,🟢,https://arxiv.org/html/2405.16712v1,Dense,Mamba1
,,,,,,,,,,,,,,,
About this sheet,,,,,,,,,,,,,,,