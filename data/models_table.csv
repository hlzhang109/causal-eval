Model,Lab,Playground,"Parameters
(B)","Tokens
trained (B)","Ratio Tokens:Params
(Chinchilla scalingâ‰¥20:1)","ALScore
""ALScore"" is a quick and dirty rating of the model's power. The formula is:
Sqr Root of (Parameters x Tokens) Ã· 300.
Any ALScore â‰¥ 1.0 is a powerful model in mid-2023.",MMLU,"MMLU
-Pro",GPQA,Training dataset,"Announced
â–¼",Public?,Paper / Repo,Arch,Notes
WormGPT,(Undisclosed),,6,402,67:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¡,https://slashnext.com/blog/wormgpt-the-generative-ai-tool-cybercriminals-are-using-to-launch-business-email-compromise-attacks/,Dense,GPT-J (2021) finetune/module.
Yi-Lightning,01-ai,https://platform.lingyiwanwu.com/,200,10000,50:1,4.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://platform.lingyiwanwu.com/docs#%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%AE%A1%E8%B4%B9,MoE,"""New MoE hybrid expert architecture"" and https://x.com/01AI_Yi/status/1845776529185476613"
Yi-Coder,01-ai,https://huggingface.co/collections/01-ai/yi-coder-66bdb00f5bdd611f9a008f30,9,6200,689:1,0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md,Dense,"6B=3T tokens, 9B=+0.8T tokens, 9B-Coder=+2.4T tokens=6.2T tokens. See Yi 1.5 34B in this table"
Yi-XLarge,01-ai,https://platform.01.ai/,2000,20000,10:1,21.1,85.1,,48.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://www.aixinzhijie.com/article/6845768,MoE,"Still training as of May/2024: https://appserversrc.8btc.cn/FnDYlEC4STBhphu6M3NL4CKH43FW dead link, use: https://finance.china.com.cn/roll/20240513/6116857.shtml"
Yi-Large,01-ai,https://platform.01.ai/,1000,15000,15:1,12.9,83.8,58.1,43.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://www.aixinzhijie.com/article/6845768,Dense,
Yi 1.5 34B,01-ai,https://huggingface.co/01-ai/Yi-1.5-34B-Chat,34.4,3600,105:1,1.2,76.8,52.3,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://github.com/01-ai/Yi-1.5,Dense,Uses 600B more training tokens than Yi 1.0 (Nov/2023).
Yi-34B,01-ai,https://huggingface.co/01-ai/Yi-34B,34.4,3000,88:1,1.1,76.3,43,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://github.com/01-ai/Yi,Dense,Controversy about Llama 2 base. https://twitter.com/kaifulee/status/1724673131875377465 MMLU=76.3 (PaLM 2=78.3) Outperforms Llama 2. Chinese and English. https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-lee-s-open-source-01-ai-bests-llama-2-according-to-hugging-face
Zhinao (Intellectual Brain),360 cn,https://ai.360.com/,100,2000,20:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://arxiv.org/abs/2402.01723v1,Dense,
Fuyu-Heavy,Adept,,120,5000,42:1,2.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¡,https://www.adept.ai/blog/adept-fuyu-heavy,Dense,"Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger.' Token estimate is based on Adept Persimmon-8B using many more tokens."
Fuyu,Adept,https://huggingface.co/adept/fuyu-8b,8,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://www.adept.ai/blog/fuyu-8b,Dense,"VLM. 8B available under open licence, Medium size is closed"
Persimmon-8B,Adept,https://www.adept.ai/blog/persimmon-8b,8,737,93:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://github.com/persimmon-ai-labs/adept-inference,Dense,Open Apache license and publicly accessible weights.
SEA-LIONv3,AI Singapore,https://huggingface.co/aisingapore/gemma2-9b-cpt-sea-lionv3-base,9.24,8200,888:1,0.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://www.linkedin.com/posts/leslieteo01_ai-machinelearning-nlp-activity-7258042808891027456-Tqab/,Dense,SEA-LION is a collection of Large Language Models (LLMs) which has been pretrained and instruct-tuned for the Southeast Asia (SEA) region. The Gemma2 9B CPT SEA-LIONv3 base model which has undergone continued pre-training from the base Gemma-2-9B model. SEA-LION stands for Southeast Asian Languages In One Network.' News: https://www.techinasia.com/news/ai-singapore-boosts-sea-ai-sealion-v3-model
Sea-Lion,AI Singapore,https://aisingapore.org/aiproducts/sea-lion/,7.5,980,131:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://huggingface.co/aisingapore/sealion7b,Dense,"MPT base. MMLU=26.87. Southeast Asian languages like Thai, Vietnamese and Bahasa Indonesia. https://www.computerweekly.com/feature/Sea-Lion-explained-Southeast-Asias-first-large-language-model"
Jamba 1.5,AI21,https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251,398,8000,21:1,5.9,81.2,53.5,36.9,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2408.12570,MoE,"Jamba 1.5 Mini (12B active/52B total) and Jamba 1.5 Large (94B active/398B total) are also optimized for business use cases and capabilities such as function calling, structured output (JSON), and grounded generation."
Jamba 1,AI21,https://huggingface.co/ai21labs/Jamba-v0.1,52,5000,97:1,1.7,67.4,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2403.19887,MoE,"MoE. Open weights, licensed under Apache 2.0. Announce: https://arxiv.org/abs/2403.19887"
Jurassic-2,AI21,Studio,178,,,,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Mar/2023,ğŸŸ¢,https://www.ai21.com/blog/introducing-j2,Dense,
J-1 RBG,AI21,ask-rbg.ai,178,300,2:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Jul/2022,ğŸŸ¢,https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1,Dense,J-1 fine-tuned with RBG law corpus
Jurassic-1,AI21,Studio,178,300,2:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Aug/2021,ğŸŸ¢,https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1,Dense,Emulated GPT-3 dataset
Weaver,AIWaves.cn,https://www.wawawriter.com/,34,2018,60:1,0.9,,,,ğŸ“š,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.17268,Dense,Llama? 'All Weaver models are initialized from powerful open-source LLMs.' English waitlist: https://www.wawawriter.com/en/
aiXcoder-7B,aiXcoder,https://github.com/aixcoder-plugin/aixcoder-7b,7,1200,172:1,0.3,,,,ğŸŒ‹,Oct/2024,ğŸŸ¢,https://arxiv.org/abs/2410.13187v1,Dense,Dataset: The Stack
Pharia-1-LLM-7B,Aleph Alpha,https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,7,7700,"1,100:1",0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://aleph-alpha.com/introducing-pharia-1-llm-transparent-and-compliant/,Dense,
Luminous Supreme Control,Aleph Alpha,https://app.aleph-alpha.com/playground/completion,70,588,9:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Feb/2023,ğŸŸ¢,https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf,Dense,â€˜Controlâ€™ means instruction tuned
Luminous,Aleph Alpha,AA playground,200,,,,,,,ğŸ•¸,Nov/2021,ğŸŸ¢,https://www.aleph-alpha.de/pricing,Dense,Devs from EleutherAI
Qwen2.5-Max,Alibaba,https://chat.qwenlm.ai/,325,20000,62:1,8.5,87.9,69,60.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://qwenlm.github.io/blog/qwen2.5-max/,MoE,"""Qwen2.5-Max emerges as a milestone in MoE development, featuring an impressive 325 billion parameters. The model has been pretrained on over 20 trillion tokens and further refined with advanced post-training methodologies such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF)."" https://wandb.ai/byyoung3/ml-news/reports/Qwen2-5-Max-Advancing-Large-Scale-Mixture-of-Expert-Models---VmlldzoxMTEyMjUyNg"
QwQ-32B,Alibaba,https://huggingface.co/spaces/Qwen/QwQ-32B-preview,32,18000,563:1,2.5,,,65.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://qwenlm.github.io/blog/qwq-32b-preview/,Dense,Reasoning. Scores 1/5 on latest ALPrompt 2024 H2. Qwen with Question=QwQ
Marco-o1,Alibaba,https://huggingface.co/AIDC-AI/Marco-o1,7,7000,"1,000:1",0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2411.14405,Dense,"Reasoning. No evals. Qwen2-7B-Instruct with a combination of the filtered Open-O1 CoT dataset, Marco-o1 CoT dataset, and Marco-o1 Instruction dataset."
Qwen2.5-Coder,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,32.5,5500,170:1,1.4,79.1,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2412.15115,Dense,https://qwenlm.github.io/blog/qwen2.5-coder-family/ Jack Clark from Anthropic is saying itâ€™s actually 18T tokens from Qwen2.5 + 5.5T tokens for a total of 23.5T tokens. That doesnâ€™t seem right from my interpretation of the technical report.
Qwen2.5,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,72,18000,250:1,3.8,86.1,71.1,49,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://arxiv.org/abs/2412.15115,Dense,
Qwen2,Alibaba,https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct,72,7000,98:1,2.4,84.2,55.6,37.9,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2407.10671,Dense,Instruct MMLU=82. Instruct GPQA=41.9. https://qwenlm.github.io/blog/qwen2/
Qwen2-57B-A14B,Alibaba,https://github.com/QwenLM/Qwen2?tab=readme-ov-file,57,4500,79:1,1.7,76.5,43,34.3,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2407.10671,MoE,https://qwenlm.github.io/blog/qwen2/
Qwen-Max,Alibaba,https://chat.lmsys.org/,300,6000,20:1,4.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction,Dense,https://twitter.com/JustinLin610/status/1787584325367529509
Qwen-1.5 110B,Alibaba,https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo,111,3000,28:1,1.9,80.4,49.9,35.9,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://qwenlm.github.io/blog/qwen1.5-110b/,Dense,"Worse performance on GPQA (72B=36.3, 110B=35.9)."
Qwen1.5-MoE-A2.7B,Alibaba,https://qwenlm.github.io/blog/qwen-moe/,14.3,1500,105:1,0.5,62.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://qwenlm.github.io/blog/qwen-moe/,MoE,"MoE. ""Of particular significance is the fact that, through upcycling, the necessity for training an equivalent volume of tokens as in the original model has been eliminated."" I assumed half of the original 3T tokens"
Qwen-1.5 72B,Alibaba,https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat,72,3000,42:1,1.5,77.5,52.6,36.3,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://qwenlm.github.io/blog/qwen1.5/,Dense,
SeaLLM-13b,Alibaba,https://github.com/damo-nlp-sg/seallms,13,2000,154:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.00738,Dense,"Llama 2 for Southeast Asian (SEA) languages: Vietnamese ğŸ‡»ğŸ‡³, Indonesian ğŸ‡®ğŸ‡©, Thai ğŸ‡¹ğŸ‡­, Malay ğŸ‡²ğŸ‡¾, KhmerğŸ‡°ğŸ‡­, LaoğŸ‡±ğŸ‡¦, TagalogğŸ‡µğŸ‡­ and BurmeseğŸ‡²ğŸ‡²"
Qwen,Alibaba,https://huggingface.co/Qwen,72,3000,42:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2309.16609,Dense,Chinese. Full name is 'Tongyi Qianwen' é€šä¹‰åƒé—®. 'Lags behind both GPT-3.5 and GPT-4'. Originally 7B/14B params Apr/2023
Llama-3.1-Tulu-3-405B,Allen AI,https://playground.allenai.org/,405,15600,39:1,8.4,87,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B,Dense,Lower MMLU score than Llama 3.1 405B base.
OLMo 2,Allen AI,https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc,13,5600,431:1,0.9,68.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2501.00656,Dense,Open Language Model (OLMo) 2 Apache 2.0 license for research and educational use. Paper coming. Data: 5 trillion tokens (1.2 epochs of 4T tokens) + 100B tokens (3 runs) + 300B tokens (1 run) merged. https://huggingface.co/allenai/OLMo-2-1124-13B & playground: https://playground.allenai.org/
TÃœLU 3,Allen AI,https://playground.allenai.org/,70,15600,223:1,3.5,83.1,65.8,45.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://allenai.org/papers/tulu-3-report.pdf,Dense,"Llama 3.1 post-training, worse performance on most benchmarks. Post training methods include new Reinforcement Learning with Verifiable Rewards (RLVR). ""We perform supervised fine-tuning on new capability-focused synthetic data mixed with existing instruction datasets. We then perform preference tuning on on-policy synthetic preference data. We finish training Llama TÃ¼lu3 with our new method, Reinforcement Learning with Verifiable Rewards."""
Molmo,Allen AI,https://molmo.allenai.org/,72,7000,98:1,2.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://molmo.allenai.org/paper.pdf,Dense,ViT: Llava as Qwen2 (or Olmo) + CLIP. Multimodal Open Language Model built by Ai2. Announce: https://molmo.allenai.org/blog
OLMoE-1B-7B,Allen AI,https://huggingface.co/collections/allenai/olmoe-66cf678c047657a30c8cd3da,6.9,5900,856:1,0.7,54.1,,23,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://arxiv.org/abs/2409.02060v1,MoE,"Open Language (OL) Mixture of Experts (MoE). ""We train OLMoE-1B-7B for 5 trillion tokens, however, some recent dense models train significantly longer, such as Llama 3 with 15 trillion tokens. To the best of our knowledge, there has been no large MoE that has been overtrained as much as OLMoE-1B-7B. Specifically, taking the active parameters of OLMoE-1B-7B, our token multiplier is around 5,000 (5T / 1B). There are likely benefits to training even longer, but to what degree overtraining is effective for MoEs and how it differs from dense models still requires more research."""
OLMo,Allen AI,https://huggingface.co/allenai/OLMo-7B,7,2500,358:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://allenai.org/olmo/olmo-paper.pdf,Dense,Open Language Model (OLMo)
Unified-IO 2,Allen AI,https://unified-io-2.allenai.org/,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.17172,Dense,"600TB dataset (plus 120+ fine-tuning datasets) includes '1B imagetext pairs, 1T text tokens, 180M video clips, 130M interleaved image & text, 3M 3D assets, and 1M agent trajectories.'"
TÃœLU 2,Allen AI,https://huggingface.co/allenai/tulu-2-dpo-70b,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://arxiv.org/abs/2311.10702,Dense,Llama 2 finetune with RLHF direct preference optimization (DPO).
Unified-IO,Allen AI,Limited demo,2.8,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2022,ğŸ”´,https://github.com/jiasenlu/unified-io/blob/main/UnifiedIOv1.pdf,Dense,Based on T5. Demo only
Macaw,Allen AI,Allen (static demo only),11,,,,,,,ğŸ†€ğŸ…°,Sep/2021,ğŸŸ¡,https://arxiv.org/abs/2109.02593,Dense,Chatbot
Nova Pro,Amazon,https://aws.amazon.com/bedrock/,90,10000,112:1,3.2,85.9,,46.9,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,Dense,"Multimodal, same performance as Llama 3.2 90B âˆ´ est 90B. Model card was hidden: https://assets.amazon.science/9f/a3/ae41627f4ab2bde091f1ebc6b830/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf via https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card"
HLAT,Amazon,,7,1800,258:1,0.4,41.318,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸ”´,https://arxiv.org/abs/2404.10630,Dense,HLAT=High-quality LLM pre-trained on AWS Trainium. Same arch as Llama 7B. The pre-training is performed up to 64 Amazon EC2 trn1.32xlarge instances with totalling up to 1024 AWS Trainium accelerators. Read more about Trainium: https://www.aboutamazon.com/news/aws/what-you-need-to-know-about-the-aws-ai-chips-powering-amazons-partnership-with-anthropic
Titan,Amazon,https://aws.amazon.com/bedrock/titan/,200,4000,20:1,3,70.4,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Apr/2023,ğŸŸ¢,https://www.techrepublic.com/article/amazon-bedrock-titan-cloud-artificial-intelligence/,Dense,"No official information at all. 2nd hand via Jack Clark: https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon '$65m training run. Specifically, they trained a 200B dense model on 4T tokens of data across 13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.'"
Multimodal-CoT,Amazon,https://github.com/amazon-science/mm-cot,0.738,,,,,,,ğŸŒ‹,Feb/2023,ğŸŸ¢,https://arxiv.org/abs/2302.00923,Dense,Models <1B with vision CoT
AlexaTM 20B,Amazon,Github (train/deploy),20,1300,65:1,0.5,,,,ğŸ†† ğŸ•¸,Aug/2022,ğŸŸ¢,https://assets.amazon.science/ee/20/3abcf2304d9b8d68da2006ff7107/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model.pdf,Dense,Wikipedia and mC4 only. seq2seq
AMD OLMo,AMD,https://huggingface.co/amd/AMD-OLMo,1,1308,"1,308:1",0.1,30.52,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://www.amd.com/en/developer/resources/technical-articles/introducing-the-first-amd-1b-language-model.html,Dense,1 billion parameter LMs trained from scratch using 1.3T tokens on a cluster of AMD Instinct MI250 GPUs.
AMD-Llama-135m,AMD,https://huggingface.co/amd/AMD-Llama-135m,0.135,670,"4,963:1",0,23.02,,,ğŸ“š ğŸŒ‹,Sep/2024,ğŸŸ¢,https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html,Dense,"Small language model (SLM). Trained on AMD Instinctâ„¢ MI250 accelerators. ""Pretrain Dataset: We employed the SlimPajama and Project Gutenberg dataset to pretrain the 135M model. Project Gutenberg is a library of over 70,000 free eBooks approximately. This sums up to 670B tokens"""
Transformer++,American Express,,0.212,0.1,1:1,0,,,,ğŸ“š,Mar/2020,ğŸ”´,https://arxiv.org/abs/2003.04974,Dense,"Not to be confused with the more common usage of Transformer++, the ~2023 Transformer++ based on Llama. See Mamba paper."
Claude 3.5 Sonnet (new),Anthropic,https://claude.ai/,175,20000,115:1,6.2,90.5,78,65,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf#page=51,Dense,Absurd naming scheme. Paper addendum pp51-64: https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf#page=51
Claude 3.5 Sonnet,Anthropic,https://poe.com/Claude-3.5-Sonnet,70,15000,215:1,3.4,88.7,76.1,67.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸ”´,https://www.anthropic.com/news/claude-3-5-sonnet,Dense,MMLU=90.4 with prompting. Model card: https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf
Claude 3 Opus,Anthropic,https://claude.ai/,2000,40000,20:1,29.8,86.8,68.5,59.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://www.anthropic.com/claude-3-model-card,MoE,"Original MMLU=86.8 (GPT-4=86.4). MMLU=88.2 with CoT prompting. Original GPQA=50.4. 200k context, 1M for researchers."
Claude 2.1,Anthropic,https://claude.ai/,130,2500,20:1,1.9,78.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://www.anthropic.com/index/claude-2-1,Dense,"Less hallucinations, 200k context length, tool use"
Claude 2,Anthropic,https://claude.ai/,130,2500,20:1,1.9,78.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf,Dense,"More HHH, 200k context length"
RL-CAI,Anthropic,,52,400,8:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Dec/2022,ğŸ”´,https://arxiv.org/abs/2212.08073,Dense,RLAIF=reinforcement learning with AI feedback
Anthropic-LM 52B,Anthropic,,52,400,8:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Dec/2021,ğŸ”´,https://arxiv.org/abs/2112.00861,Dense,Internal research only
4M-21,Apple,https://github.com/apple/ml-4m/,3,1000,334:1,0.2,,,,ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2406.09406,Dense,"Vision model based on T5-XXL. Modalities: RGB, Caption, Bounding boxes, Semantic segmentation, Depth, Human poses, Surface normals, CLIP, DINOv2, ImageBind, Metadata, Canny edges, SAM edges, SAM instances, Color palette. Project page: https://4m.epfl.ch/"
Apple On-Device model Jun/2024,Apple,https://github.com/apple/corenet/tree/main/projects/openelm,3.04,1500,494:1,0.2,26.76,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2404.14619,Dense,"https://lifearchitect.ai/apple/ Likely to be the Apple OpenELM model (Apr/2024). ""two of these models â€” a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute"". https://machinelearning.apple.com/research/introducing-apple-foundation-models The server-based model is possibly Ferret, although it is more properly called a multimodal model (not just language). It could also be Apple GPT based on their Ajax framework: https://archive.md/f3C0r"
OpenELM,Apple,https://huggingface.co/apple/OpenELM-3B-Instruct,3.04,1500,494:1,0.2,26.76,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.14619,Dense,"On-device model (laptop, phone). Open-source Efficient Language Models (OpenELM). https://venturebeat.com/ai/apple-releases-openelm-small-open-source-ai-models-designed-to-run-on-device/"
Ferret-UI,Apple,https://github.com/apple/ml-ferret,13,2000,154:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.05719,Dense,"Vicuna base, multimodal. Extension of Ferret from Oct/2023."
ReALM-3B,Apple,,3,134,45:1,0.1,,,,ğŸŒ‹,Mar/2024,ğŸ”´,https://arxiv.org/abs/2403.20329,Dense,FLAN-T5 (Oct/2022) finetune.
MM1,Apple,,30,2010,67:1,0.8,,,,ğŸŒ‹,Mar/2024,ğŸ”´,https://arxiv.org/abs/2403.09611,Dense,"VLM, outperforms Flamingo 80B (Apr/2022) across benchmarks. 2T text tokens + ~10B+ other text (estimate). Unreleased."
Ask,Apple,,20,,,,,,,ğŸŒ‹,Feb/2024,ğŸ”´,https://www.macrumors.com/2024/02/22/applecare-advisors-testing-new-ask-tool/,Dense,Internal employee model only
MGIE,Apple,https://github.com/tsujuifu/pytorch_mgie,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://openreview.net/forum?id=S1RKWSyZ2Y,Dense,MLLM and diffusion model initialized from LLaVA-7B (Llama 2 + Vicuna) + StableDiffusion-v1.5.
Ferret,Apple,https://github.com/apple/ml-ferret,13,2000,154:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Oct/2023,ğŸŸ¢,https://arxiv.org/abs/2310.07704,Dense,"Vicuna base, multimodal"
UniLM,Apple,https://jackcook.com/2023/09/08/predictive-text.html,0.034,1,30:1,0,,,,ğŸŒ‹,Sep/2023,ğŸŸ¢,https://github.com/jackcook/predictive-spy,Dense,Apple's Transformer model for iOS 17 + macOS Sonoma. Announce is actually Jun/2023. GPT-2 base? 128 token context window
AuroraGPT (ScienceGPT),Argonne National Laboratory,https://lifearchitect.ai/auroragpt/,2000,30000,15:1,,,,,,TBA,ğŸ”´,,,Three models targeted in Jul/2024: AuroraGPT-7B-P (Ponte Vecchio GPU testing) AuroraGPT-7B-A (Aurora) AuroraGPT-7B-A-S (Aurora + Science).
Formosa (FFM),Asus/TWS,,176,366,3:1,0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¡,https://www.asus.com/news/xxifirl2s2tzesl0/,Dense,"BLOOMZ finetune? Chinese, Taiwan's first LLM. Subscription hardware: https://archive.md/cVdJt"
DukunLM,AzaleAI,https://huggingface.co/azale-ai/DukunLM-13B-V1.0-Uncensored,13,1500,116:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Aug/2023,ğŸŸ¢,https://huggingface.co/azale-ai/DukunLM-13B-V1.0-Uncensored,Dense,Indonesian fine-tune of WizardLM (which is a Llama fine-tune).
Emu3,BAAI,https://huggingface.co/BAAI/Emu3-Gen,8,1000,125:1,0.3,,,,ğŸŒ‹,Sep/2024,ğŸŸ¢,https://arxiv.org/abs/2409.18869,Dense,"VLM. Dataset estimates are based on the unrelated UW/Salesforce dataset MINT-1T (3.4B images, 927M documents) https://arxiv.org/abs/2406.11271v1"
Tele-FLM-1T,BAAI,https://huggingface.co/CofeAI/Tele-FLM-1T,1000,15.7,1:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://arxiv.org/abs/2407.02783,Dense,"Technical arch testing only, ratio is too low for decent performance."
Tele-FLM,BAAI,https://huggingface.co/CofeAI/Tele-FLM,52,2000,39:1,1.1,64,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.16645,Dense,"Also known as FLM-2. ""We will open-source a 1T model checkpoint, namely Tele-FLM-1T, to advance further training and research."" Discussion paper Jul/2024: https://arxiv.org/abs/2407.02783"
Emu2,BAAI,https://baaivision.github.io/emu2/,37,4,1:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.13286,Dense,"VLM. Gemini clone. Outperforms Flamingo 80B. The Pile for text, but only sampled 3.6B tokens (1.4% of the dataset)."
FLM-101B,BAAI,https://huggingface.co/CofeAI/FLM-101B,101,245,3:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2309.03852,Dense,Train for $100k compute budget (on a cluster of 24 DGX-A800 GPU 8Ã—80G servers for 21 days)
Baichuan 2,Baichuan,https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md,13,2600,200:1,0.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf,Dense,Great paper. Chinese-English bilingual dataset
ERNIE 4.0 Turbo,Baidu,https://yiyan.baidu.com/,200,20000,100:1,6.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/,Dense,"""Ernie Bot has reached 300 million users since its launch [on 16/Mar/2023, public Aug/2023]"" Jun/2024"
ERNIE 4.0,Baidu,https://yiyan.baidu.com/,1000,20000,20:1,14.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://reuters.com/technology/chinas-baidu-unveils-latest-version-its-ernie-ai-model-2023-10-17/,Dense,Dense (confirmed). English-dubbed launch video (2h52m): https://twitter.com/i/broadcasts/1yNGaZaeallJj & https://youtu.be/wYozcsavRuM
ERNIE-Code,Baidu,,0.56,,,,,,,ğŸ•¸,Dec/2022,ğŸŸ¢,https://arxiv.org/abs/2212.06742#baidu,Dense,
ERNIE 3.0 Titan,Baidu,,260,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2021,ğŸŸ¢,https://arxiv.org/abs/2112.12731,Dense,
PLATO-XL,Baidu,Baidu,11,,,,,,,â¬† ğŸ‘¥,Sep/2021,ğŸŸ¢,https://arxiv.org/abs/2109.09519,Dense,Chatbot. Reddit comments + CN social
Sky-T1-32B-Preview,Berkeley,https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview,32,18000,563:1,2.5,,,56.8,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://novasky-ai.github.io/posts/sky-t1/,Dense,"""To generate our training data we use QwQ-32B-Preview, an open-source model with reasoning capabilities comparable to o1-preview. We curate the data mixture (see later section) to cover diverse domains that require reasoning, and a reject sampling procedure to improve the data quality. We then rewrite QwQ traces with GPT-4o-mini into a well-formatted version, inspired by Still-2, to improve data quality and ease parsing... Rejection Sampling: We discard QwQ samples if they are incorrect according to the solutions provided in datasets."""
Starling-7B,Berkeley,https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha,7,2000,286:1,0.4,,37.9,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://starling.cs.berkeley.edu/,Dense,Llama 2 7B -> OpenChat 7B -> Starling-7B (RLAIF)
Koala-13B,Berkeley,https://chat.lmsys.org/?model=koala-13b,13,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Apr/2023,ğŸŸ¢,https://bair.berkeley.edu/blog/2023/04/03/koala/,Dense,LLaMA base. Academic licence only.
LVM-3B,Berkeley/JHU,,3,420,140:1,0.1,,,,ğŸ–¼,Dec/2023,ğŸ”´,https://arxiv.org/abs/2312.00785,Dense,Paper is 25MB. First Large Vision Model (LVM); no text. Based on Llama and LAION 5B (1.49B).
mT0,BigScience,https://github.com/bigscience-workshop/xmtf,13,1000,77:1,0.4,,,,ğŸ†€ğŸ…° ğŸ•¸,Nov/2022,ğŸŸ¢,https://arxiv.org/abs/2211.01786,Dense,fine-tuned
BLOOMZ,BigScience,https://github.com/bigscience-workshop/xmtf,176,366,3:1,0.8,,,,â¬† ğŸ•¸,Nov/2022,ğŸŸ¢,https://arxiv.org/abs/2211.01786,Dense,fine-tuned
BLOOM (tr11-176B-ml),BigScience,https://huggingface.co/spaces/huggingface/bloom_demo,176,366,3:1,0.8,39.1,,,â¬† ğŸ•¸,Jul/2022,ğŸŸ¢,https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml,Dense,
BloombergGPT,Bloomberg,,50,569,12:1,0.6,39.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸ”´,https://arxiv.org/abs/2303.17564,Dense,"Video: https://youtu.be/m2Scj2SO85Y Underperforms GPT-3, based on BLOOM. Tokens: 'We select a model size motivated by Hoffmann et al. (2022) and train a 50 billion parameter model on 569 billion tokens from our corpus of over 700 billion tokens to produce a model that is competitive with larger models.'"
Platypus,Boston University,https://platypus-llm.github.io/,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2023,ğŸŸ¢,https://platypus-llm.github.io/Platypus.pdf,Dense,"Fine-tune of Llama 2, family includes merges with Beluga, Dolphin, and Camel fine-tunes."
GOODY-2,BRAIN,https://www.goody2.ai/chat,,,,,,,,ğŸŒ‹,Feb/2024,ğŸŸ¢,https://www.goody2.ai/goody2-modelcard.pdf,Dense,Satire (and hilarious). Probably Llama 2 with aggressive prompt. Wired interview: https://archive.md/toxHq
UI-TARS-72B,ByteDance,https://github.com/bytedance/UI-TARS-desktop?tab=readme-ov-file,72,9000,125:1,2.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://arxiv.org/abs/2501.12326,Dense,VLM. SoTA agent 'computer use' model to 23/Jan/2024.
Doubao-1.5-pro,ByteDance,https://www.volcengine.com/docs/82379/1330310#474f7dec,300,9000,30:1,5.5,88.6,80.1,65,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://team.doubao.com/en/special/doubao_1_5_pro,MoE,"Reasoning. Includes 2.4B param ViT. ""Doubao-1.5-pro uses a sparse MoE architecture. In the pre-training stage, the performance of the MoE model activated with only a small number of parameters can exceed that of ultra-large dense pre-trained models such as Llama3.1-405B. Through the study of the sparsity scaling law, the team determined the sparse ratio that balances performance and efficiency, and determined based on the MoE scaling law that a model activated with a small number of parameters can achieve the performance of a world-class model."""
530B,ByteDance,,530,300,1:1,1.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸ”´,https://arxiv.org/abs/2402.15627,Dense,"Trained using 12,288 A100 GPUs, replicating MT-NLG size"
175B,ByteDance,,175,300,2:1,0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸ”´,https://arxiv.org/abs/2402.15627,Dense,"Trained using 12,288 A100 GPUs, replicating GPT-3 size"
PandaGPT,Cambridge/Tencent,https://panda-gpt.github.io/,13,1000,77:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://github.com/yxuansu/PandaGPT/blob/main/PandaGPT.pdf,Dense,"Proto-AGI. 6 modalities (text, image/video, audio, depth, thermal, and IMU/accelerometer/gyroscope/compass). Based on Vicuna."
Rene,Cartesia,https://huggingface.co/cartesia-ai/Rene-v0.1-1.3b-pytorch,1.3,1500,"1,154:1",0.1,32.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://cartesia.ai/blog/2024-08-27-on-device,Dense,"On-device. ""hybrid architecture based on Mamba-2, with feedforward and sliding window attention layers interspersed"""
1T,Cerebras,https://cerebras.ai/press-release/cerebras-demonstrates-trillion-parameter-model-training-on-a-single-cs-3-system,1000,20000,20:1,14.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸ”´,https://cerebras.ai/press-release/cerebras-demonstrates-trillion-parameter-model-training-on-a-single-cs-3-system,Dense,"""For Sandiaâ€™s trillion parameter training run, Cerebras configured a 55 terabyte MemoryX device."""
Sparse Llama 7B,Cerebras,https://huggingface.co/spaces/neuralmagic/llama-2-sparse-transfer-chat-deepsparse,7,145,21:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.03594,Hybrid,"https://www.cerebras.net/blog/introducing-sparse-llama-70-smaller-3x-faster-full-accuracy ""For the 50% sparse model, we utilized 45 billion tokens of pretraining data, while an additional 100 billion tokens were used for the 70% model. This represents approximately 2% to 8% of the original 2 trillion tokens used to train the base Llama-2 model."""
FLOR-6.3B,Cerebras,https://huggingface.co/projecte-aina/FLOR-6.3B,6.3,481,77:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://www.cerebras.net/press-release/cerebras-systems-and-barcelona-supercomputing-center-train-industry-leading-multilingual-spanish-catalan-english-llm,Dense,"Spanish, Catalan. Bloom-7.1B (341B tok) + continued pre-training on 140B tok. Trained on Cerebras hardware."
BTLM-3B-8K,Cerebras,https://huggingface.co/cerebras/btlm-3b-8k-base,3,627,209:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://www.cerebras.net/blog/btlm-3b-8k-7b-performance-in-a-3-billion-parameter-model/,Dense,"Runs on devices with as little as 3GB of memory [iPhone, Macbook] when quantized to 4-bit"
Cerebras-GPT,Cerebras,https://huggingface.co/cerebras,13,260,20:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/,Dense,20:1 tokens to parameters as per https://lifearchitect.ai/chinchilla/
C1.2,Character.ai,https://blog.character.ai/character-ai/,20,1000,50:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://blog.character.ai/character-ai/,Dense,No details released.
Natural-SQL-7B,ChatDB,,7,2000,286:1,0.4,,,,ğŸŒ‹,Feb/2024,ğŸŸ¢,https://huggingface.co/chatdb/natural-sql-7b,Dense,Based on DeepSeek-Coder 6.7B.
Unnamed 1T,China Telecom Artificial Intelligence Research Institute,https://www.scmp.com/tech/big-tech/article/3280588/china-telecom-say-ai-model-1-trillion-parameters-trained-chinese-chips,1000,20000,20:1,14.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸ”´,https://www.scmp.com/tech/big-tech/article/3280588/china-telecom-say-ai-model-1-trillion-parameters-trained-chinese-chips,Dense,"Trained on Chinese GPUs: ""Ascend Atlas 800T A2 training server â€“ a Huawei product listed as supporting the Kunpeng 920 7265 or Kunpeng 920 5250 processors"" https://www.theregister.com/2024/10/02/china_telecom_model_trained_local_tech/"
TeleChat2-115B,China Telecom Artificial Intelligence Research Institute,https://modelscope.cn/models/TeleAI/TeleChat2-115B,115,10000,87:1,3.6,80.9,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://github.com/Tele-AI/TeleChat2,Dense,"Trained on Chinese GPUs: ""Ascend Atlas 800T A2 training server â€“ a Huawei product listed as supporting the Kunpeng 920 7265 or Kunpeng 920 5250 processors"" https://www.theregister.com/2024/10/02/china_telecom_model_trained_local_tech/"
Bamba-9B,CMU,https://huggingface.co/blog/bamba,9,2200,245:1,0.5,60.77,17.53,4.14,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://huggingface.co/blog/bamba,Dense,"""trained by IBM, Princeton, CMU, and UIUC on completely open data. At inference time, the model demonstrates 2.5x throughput improvement and 2x latency speedup compared to standard transformers in vLLM."""
Bi-Mamba,CMU,,2.7,1260,467:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸ”´,https://arxiv.org/abs/2411.11843,Dense,"Unreleased, but will be replicated. ""a scalable and powerful 1-bit Mamba architecture designed for more efficient large language models"""
Mamba-2,CMU,https://github.com/state-spaces/mamba,2.7,300,112:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.21060,Dense,Analysis: https://tridao.me/blog/2024/mamba2-part1-model/
Mamba,CMU,https://huggingface.co/havenhq/mamba-chat,2.8,300,108:1,0.1,26.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.00752,Dense,"The Pile, new arch beyond just Transformers. 2.7B MMLU=26.2. 7B MMLU=33.3."
Command R7B,Cohere,https://cohereforai-c4ai-command.hf.space/models/command-r7b-12-2024,7,2000,286:1,0.4,,28.5,7.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024,Dense,
Maya,Cohere,https://huggingface.co/maya-multimodal/maya,8,4800,600:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.07112,Dense,VLM.
Aya-Expanse-32B,Cohere,https://huggingface.co/CohereForAI/aya-expanse-32b,32,8000,250:1,1.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Oct/2024,ğŸŸ¢,https://cohere.com/blog/aya-expanse-connecting-our-world,Dense,"""Aya Expanse, a family of highly performant multilingual models that excels across 23 languages and outperforms other leading open-weights models...we have collaborated with over 3,000 researchers from 119 countries to expand cutting-edge multilingual research... 220 language ambassadors from around the world who have been part of this release"""
Aya-23-35B,Cohere,https://huggingface.co/spaces/CohereForAI/aya-23,35,4800,138:1,1.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://drive.google.com/file/d/1YKBPo61pnl97C1c_1C2ZVOnPhqf7MLSc/view,Dense,
Rerank 3,Cohere,https://docs.cohere.com/reference/rerank-1,104,4000,39:1,2.1,,,,ğŸ“š ğŸ•¸,Apr/2024,ğŸŸ¢,https://txt.cohere.com/rerank-3/,Dense,"RAG + semantic search, possibly backed by Command-R+."
Command-R+,Cohere,https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus,104,4000,39:1,2.1,75.7,,,ğŸ“š ğŸ•¸,Apr/2024,ğŸŸ¢,https://huggingface.co/CohereForAI/c4ai-command-r-plus,Dense,purpose-built to excel at real-world enterprise use cases. Announce with no arch details: https://txt.cohere.com/command-r-plus-microsoft-azure/
Command-R,Cohere,Cohere,35,700,20:1,0.5,,37.9,,ğŸ“š ğŸ•¸,Mar/2024,ğŸŸ¢,https://txt.cohere.com/command-r/,Dense,RAG and tool use
Aya-101,Cohere,https://huggingface.co/CohereForAI/aya-101,13,1000,77:1,0.4,,,,ğŸ“š ğŸ•¸,Feb/2024,ğŸŸ¢,https://cohere.com/research/aya/aya-model-paper.pdf,Dense,mT5 base.
Command xlarge,Cohere,Cohere,52.4,,,,,,,ğŸ“š ğŸ•¸,Sep/2021,ğŸŸ¢,https://arxiv.org/abs/2108.07790,Dense,Stealth 'ebooks and webpages'. 52B: https://crfm.stanford.edu/helm/v1.0/?models=1
PLLuM,Consortium,,20,2000,100:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://opi.org.pl/en/the-launch-of-the-first-polish-open-large-language-model-pllum/,Dense,Polish Large Language Model. Not yet available as of Sep/2024
MambaByte,Cornell,https://github.com/kyegomez/MambaByte,0.972,37.5,39:1,0,,,,ğŸ“šğŸŒ‹,Jan/2024,ğŸ”´,https://arxiv.org/abs/2401.13660,Dense,"Used bytes instead of tokens. 4 bytesâ‰ˆ1 token, so 150B bytesâ‰ˆ37.5B tokens"
Cedille FR-Boris,Coteries,"Cedille, TS",6,,,,,,,ğŸ†† ğŸ“š ğŸ•¸ ğŸ‡«ğŸ‡·,Nov/2021,ğŸŸ¢,https://github.com/coteries/cedille-ai,Dense,French only. GPT-J.
RFM-1,Covariant,https://vimeo.com/921866765,8,160,20:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¡,https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/,Dense,"Commercial, multimodal for robotics"
Dolly 2.0,Databricks,https://huggingface.co/databricks/dolly-v2-12b,12,300,25:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2023,ğŸŸ¢,https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm,Dense,Fine-tuned Pythia 12B
DeciLM-7B,Deci,https://console.deci.ai/infery-llm-demo,7.04,200,29:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://deci.ai/blog/introducing-DeciLM-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date,Dense,4.4x times faster than Mistral. English only.
DeciLM,Deci,https://huggingface.co/Deci/DeciLM-6b,5.7,200,36:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/,Dense,Faster inference (4.8Ã— throughput of Llama 2)
next-gen,DeepL,https://www.deepl.com/en/translator,7,1000,143:1,0.3,,,,ğŸŒ‹,Jul/2024,ğŸŸ¢,https://www.deepl.com/en/blog/next-gen-language-model,Dense,"""Built using our own groundbreaking, specialized LLM technology and proprietary training data, designed specifically for translation"""
SED,DeepMind,,,,,,,,,ğŸ•¸,Nov/2022,ğŸ”´,https://arxiv.org/abs/2211.04236,Dense,SED 420M (diffusion text model)
Sparrow,DeepMind,,70,1400,20:1,1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2022,ğŸ”´,https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf,Dense,Chatbot as a fine-tuned version of Chinchilla 70B
Perceiver AR,DeepMind,,1,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2022,ğŸ”´,https://arxiv.org/abs/2202.07765,Dense,"Context window=100,000. Params=364m wiki, 975M pg-19, 826M books, music=?, imagenet=770M,"
Gato (Cat),DeepMind,,1,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2022,ğŸ”´,https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf,Dense,"Proto-AGI. Generalist agent (LLM, VLM, robot)"
Chinchilla,DeepMind,,70,1400,20:1,1,67.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2022,ğŸ”´,https://arxiv.org/abs/2203.15556,Dense,First to double tokens per size increase
Gopher,DeepMind,,280,300,2:1,1,60,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2021,ğŸ”´,https://arxiv.org/abs/2112.11446,Dense,Dataset: https://lifearchitect.ai/whats-in-my-ai/
RETRO,DeepMind,,7.5,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2021,ğŸ”´,https://arxiv.org/abs/2112.04426,Dense,with retrieval
DeepSeek-R1,DeepSeek-AI,https://huggingface.co/deepseek-ai/DeepSeek-R1,685,14800,22:1,10.6,90.8,84,71.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf,MoE,"Reasoning. ""DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks"""
DeepSeek-V3,DeepSeek-AI,https://chat.deepseek.com/,685,14800,22:1,10.6,87.1,64.4,59.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,MoE,37B active. Explain: https://threadreaderapp.com/thread/1872318161883959485.html Announce: https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file
DeepSeek-R1-Lite,DeepSeek-AI,https://chat.deepseek.com/,67,2000,30:1,1.2,,,58.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://x.com/deepseek_ai/status/1859200141355536422,Dense,"Reasoning. Scores 0/5 on latest ALPrompt 2024 H2 ""DeepSeek-R1-Lite is currently still in the iterative development stage. It currently only supports web usage and does not support API calls. The base model used by DeepSeek-R1-Lite is also a relatively small model, unable to fully unleash the potential of long reasoning chains. At present, we are continuously iterating on the inference series models. In the future, the official DeepSeek-R1 model will be fully open-sourced. We will publicly release the technical report and deploy API services."" https://mp-weixin-qq-com.translate.goog/s/e1YnTxZlzFvjcmrLLTA8fw?_x_tr_sl=zh-CN&_x_tr_tl=en&_x_tr_hl=zh-TW"
DeepSeek-V2.5,DeepSeek-AI,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,236,10200,44:1,5.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf,MoE,"""DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct."""
DeepSeek-Coder-V2,DeepSeek-AI,https://chat.deepseek.com/coder,236,10200,44:1,5.2,79.2,63.63,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf,MoE,DeepSeek-V2 with additional 6 trillion tokens.
DeepSeek-V2,DeepSeek-AI,https://chat.deepseek.com/,236,8100,35:1,4.6,78.5,54.8,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.04434,MoE,"Huge dataset, 12% Chinese ""Therefore, we acknowledge that DeepSeek-V2 still has a slight gap in basic English capabilities with LLaMA3 70B""."
DeepSeek-VL,DeepSeek-AI,https://github.com/deepseek-ai/DeepSeek-VL?tab=readme-ov-file,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2403.05525,Dense,"Vision, based on DeepSeek-LLM-7B"
DeepSeek-Coder,DeepSeek-AI,https://coder.deepseek.com/,33,2000,61:1,0.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.14196,Dense,surpasses existing closed-source models like Codex and GPT-3.5... permissive license that allows for both research and unrestricted commercial use.'
DeepSeekMoE,DeepSeek-AI,,16,2000,125:1,0.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸ”´,https://arxiv.org/abs/2401.06066,MoE,"MoE activated parameters is 10-15% of dense, so I need to rethink ALScore for MoE. 'preliminary efforts to scale up DeepSeekMoE to 145B'"
DeepSeek,DeepSeek-AI,https://chat.deepseek.com/,67,2000,30:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.02954,Dense,Chinese/English. Outperforms Llama 2. MMLU=71.3 outperforms GPT-3.5.
Pile-T5,EleutherAI,https://huggingface.co/EleutherAI/pile-t5-xxl,11,2000,182:1,0.5,53.84,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://blog.eleuther.ai/pile-t5/,Dense,
Pythia,EleutherAI,https://huggingface.co/EleutherAI/pythia-12b,12,300,25:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2023,ğŸŸ¢,https://arxiv.org/abs/2304.01373,Dense,
GPT-NeoX-20B,EleutherAI,"TS, Goose",20,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2022,ğŸŸ¢,https://github.com/EleutherAI/gpt-neox,Dense,Latest model to Feb/2022
GPT-J,EleutherAI,"TS, Goose",6,402,67:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2021,ğŸŸ¢,https://github.com/kingoflolz/mesh-transformer-jax,Dense,Popular
xLSTM,ELLIS,,2.7,15,6:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸ”´,https://arxiv.org/abs/2405.04517,Dense,"New method LSTM to xLSTM, see also RNNs. Code/weights doesn't seem to be released. https://github.com/AI-Guru/xlstm-resources"
MEDITRON,EPFL,https://huggingface.co/epfl-llm/meditron-70b,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://arxiv.org/abs/2311.16079,Dense,"Llama 2 trained on med data using NVIDIA Megatron-LM. ""outperforms Llama-2-70B, GPT-3.5 (text-davinci-003, 8-shot), and Flan-PaLM on multiple medical reasoning tasks."""
PassGPT,ETH ZÃ¼rich,,,,,,,,,ğŸŒ‹,Jun/2023,ğŸ”´,https://arxiv.org/abs/2306.01545,Dense,GPT-2 trained on leaked passwords
ESM3,EvolutionaryScale,https://github.com/evolutionaryscale/esm,98,771,8:1,0.9,,,,ğŸŒ‹,Jun/2024,ğŸŸ¡,https://www.evolutionaryscale.ai/blog/esm3-release,Dense,"Biology large language model: ""sequence, structure, and function are all masked and predicted during training, ESM3 can generate in all three modalities."" 1.4B only released."
ULMFiT,Fast.ai,https://docs.fast.ai/tutorial.text.html,0.034,0.1,3:1,0,,,,ğŸ††,Jan/2018,ğŸŸ¢,https://arxiv.org/abs/1801.06146,Dense,Aussie Prof Jeremy Howard: https://www.abc.net.au/news/science/2023-11-15/jeremy-howard-taught-ai-to-the-world-and-helped-invent-chatgpt/103092474
f1,Fireworks,https://fireworks.ai/models/fireworks/f1-preview/playground,,,,,,,42.4,,Nov/2024,ğŸŸ¢,https://fireworks.ai/blog/fireworks-compound-ai-system-f1,Compound,"""a compound AI model specialized in complex reasoning, that interweaves multiple open models at the inference layer. """
AnyGPT,Fudan University,https://junzhan2000.github.io/AnyGPT.github.io/,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2402.12226,Dense,Llama 2 7B backbone with new matrices ('reshaping the embedding matrix and prediction layer')
MOSS,Fudan University,https://moss.fastnlp.top/,16,430,27:1,0.3,,,,ğŸ•¸ ğŸŒ‹,Feb/2023,ğŸŸ¢,https://txsun1997.github.io/blogs/moss.html,Dense,Major bandwidth issues: https://www.reuters.com/technology/china-fudan-university-team-apologises-after-chatgpt-style-platform-crashes-2023-02-21/
Fugaku-LLM,Fujitsu,https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B-instruct,13,380,30:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html,Dense,"Japanese. CPU trained: 158,976+ A64FX CPUs (7M+ cores), zero GPUs. https://en.wikipedia.org/wiki/Fugaku_(supercomputer)"
Luna,Galileo,https://www.rungalileo.io/blog/introducing-galileo-luna-a-family-of-evaluation-foundation-models,0.44,162,369:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2406.00975,Dense,Based on DeBERTA-large (440M). RoBERTa=162B token dataset.
TimesFM,Google,,0.2,100,500:1,0,,,,ğŸ†† ğŸŒ‹,Feb/2024,ğŸ”´,https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html,Dense,Time-series forecasting only. 'a large pretraining corpus of 100B real world time-points' may be more than 100B tokens.
AudioPaLM,Google,https://google-research.github.io/seanet/audiopalm/examples/,340,3600,11:1,3.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Jun/2023,ğŸ”´,https://arxiv.org/abs/2306.12925,Dense,a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation
PaLM 2,Google,https://console.cloud.google.com/vertex-ai/generative/language/create/chat,340,3600,11:1,3.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,May/2023,ğŸŸ¢,https://ai.google/static/documents/palm2techreport.pdf,Dense,"â€œWhat we found in our work is that itâ€™s not really the sort of size of model â€” that the larger is not always better,â€ Deepmind VP Zoubin Ghahramani said in a press briefing ahead of todayâ€™s announcement. â€œThatâ€™s why weâ€™ve provided a family of models of different sizes. We think that actually parameter count is not really a useful way of thinking about the capabilities of models and capabilities are really to be judged by people using the models and finding out whether theyâ€™re useful in the tests that they try to achieve with these models.â€"
CoLT5,Google,,5.2,,,,,,,ğŸ†€ğŸ…° ğŸ•¸,Mar/2023,ğŸ”´,https://arxiv.org/abs/2303.09752,Dense,up to 64k context window [48k words or about 96 pages -Alan]
RT-1,Google,,0.035,,,,,,,ğŸŒ‹,Dec/2022,ğŸ”´,https://robotics-transformer.github.io/assets/rt1.pdf,Dense,
Flan-T5,Google,TS,11,1100,100:1,0.4,,,,ğŸ†€ğŸ…° ğŸ•¸,Oct/2022,ğŸŸ¢,https://arxiv.org/abs/2210.11416,Dense,T5=1T tokens + LM-adapted T5 as 100B tokens
Flan-PaLM,Google,,540,780,2:1,2.2,73.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Oct/2022,ğŸ”´,https://arxiv.org/abs/2210.11416,Dense,
U-PaLM,Google,,540,780,2:1,2.2,74.1,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Oct/2022,ğŸ”´,https://arxiv.org/abs/2210.11399,Dense,
PaLI,Google,,17,,,,,,,ğŸŒ‹,Sep/2022,ğŸ”´,https://arxiv.org/abs/2209.06794,Dense,"PaLM Vision model, new datasets of 10B multilingual text-image pairs"
â€˜monorepo-Transformerâ€™,Google,,0.5,,,,,,,ğŸ•¸,Jul/2022,ğŸ”´,https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html,Dense,Unnamed. Writes >3% of internal google code.
Minerva,Google,,540,818.5,2:1,2.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2022,ğŸ”´,https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html,Dense,PaLM finetuned on LaTeX/arXiv maths
LIMoE,Google,,5.6,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Jun/2022,ğŸ”´,https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html,MoE,
UL2 20B,Google,,20,1000,50:1,0.5,39.2,,,ğŸ•¸,May/2022,ğŸ”´,https://arxiv.org/abs/2205.05131,Dense,Unifying Language model. C4 only.
LaMDA 2,Google,YouTube (video only),137,,,,,,,â¬† ğŸ•¸ ğŸ‘¥,May/2022,ğŸŸ¡,https://arxiv.org/abs/2201.08239,Dense,Chatbot with tiny walled garden demo TBA
PaLM-Coder,Google,,540,780,2:1,2.2,,,,ğŸ•¸,Apr/2022,ğŸ”´,https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf,Dense,
PaLM,Google,,540,780,2:1,2.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Apr/2022,ğŸ”´,https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdff,Dense,
GLaM,Google,,1200,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Dec/2021,ğŸ”´,https://arxiv.org/abs/2112.06905,MoE,
BERT-480,Google,,480,,,,,,,ğŸ†† ğŸ“š ğŸ•¸,Nov/2021,ğŸ”´,https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-v4-pods-for-large-model-training,Dense,Submission to benchmarks. Original dataset was BookCorpus + Wikipedia: https://arxiv.org/pdf/1810.04805.pdf
BERT-200,Google,,200,,,,,,,ğŸ†† ğŸ“š ğŸ•¸,Nov/2021,ğŸ”´,https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-v4-pods-for-large-model-training (same as above),Dense,Submission to benchmarks. Original dataset was BookCorpus + Wikipedia: https://arxiv.org/pdf/1810.04805.pdf
FLAN,Google,,137,,,,,,,â¬† ğŸ•¸ ğŸ‘¥,Sep/2021,ğŸ”´,https://arxiv.org/abs/2109.01652,Dense,Fine-tuned LaMDA
LaMDA,Google,YouTube (video only),137,,,,,,,â¬† ğŸ•¸ ğŸ‘¥,Jun/2021,ğŸ”´,https://arxiv.org/abs/2201.08239,Dense,Chatbot
Switch,Google,,1600,576,1:1,3.2,,,,ğŸ†€ğŸ…° ğŸ•¸,Jan/2021,ğŸŸ¢,https://arxiv.org/abs/2101.03961,MoE,
Meena,Google,,2.6,10000,"3,847:1",0.5,,,,ğŸ‘¥ ğŸŒ‹,Jan/2020,ğŸ”´,https://arxiv.org/abs/2001.09977,Dense,Dialogue model. Trained 61B tokens for 164x epochs to 10T tokens!
T5,Google,,11,1000,91:1,0.3,,,,ğŸ†€ğŸ…° ğŸ•¸,Oct/2019,ğŸŸ¢,https://arxiv.org/abs/1910.10683,Dense,"C4 + NLP language problems. ""compared the following three configurations: First, the standard baseline model, which was pre-trained on 235 â‰ˆ 34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5), which we refer to as â€œbaseline-1Tâ€; and third, T5-Base."""
BERT,Google,Hugging Face,0.34,137,403:1,0,,,,ğŸ†† ğŸ“š,Oct/2018,ğŸŸ¢,https://arxiv.org/abs/1810.04805,Dense,
Transformer (big),Google,https://github.com/tensorflow/tensor2tensor?tab=readme-ov-file#walkthrough,0.213,0.1,1:1,0,,,,ğŸ“š,Jun/2017,ğŸŸ¢,https://arxiv.org/abs/1706.03762,Dense,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs... For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."""
Transformer (base),Google,https://github.com/tensorflow/tensor2tensor?tab=readme-ov-file#walkthrough,0.065,0.1,2:1,0,,,,ğŸ“š,Jun/2017,ğŸŸ¢,https://arxiv.org/abs/1706.03762,Dense,"""We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs... For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."""
Gemini 2.0 Pro,Google DeepMind,https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-pro-exp-02-05,200,20000,100:1,6.7,,79.1,64.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Feb/2025,ğŸŸ¢,https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/,MoE,"Context=2M. Disappointing benchmarks, this is the 'pro' (medium) not 'ultra' (large) model."
Gemini 2.0 Flash exp,Google DeepMind,https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-2.0-flash-exp,30,30000,"1,000:1",3.2,87,76.4,62.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2,MoE,"Gemini 2.0 Flash was first model released, 11/Dec/2024. ""New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities"" Announce: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/"
Gemini-1.5-Pro-002,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,1500,30000,20:1,22.4,,75.8,59.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/,MoE,Sparse MoE. Context window=2M
Data-Gemma,Google DeepMind,https://huggingface.co/google/datagemma-rig-27b-it,27,13000,482:1,2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf,Dense,"RAG/RIG: ""the LLM is fine-tuned to produce natural language Data Commons queries alongside statistics"""
Gemini 1.5 Flash-8B,Google DeepMind,https://ai.google.dev/,8,8000,"1,000:1",0.8,68.1,,30.8,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2403.05530,Dense,Announce: https://x.com/OfficialLoganK/status/1828480085353234535 1M context for all modalities.
FLAMe,Google DeepMind,,24,1000,42:1,0.5,,,,ğŸ‘¥,Jul/2024,ğŸ”´,https://arxiv.org/abs/2407.10817v1,Dense,LLM-as-a-Judge autorater. Foundational Large Autorater Models (FLAMe). Uses an instruction-tuned PaLM-2-24B model. Unrelated to Microsoft FLAME Jan/2023.
Gemma 2,Google DeepMind,https://huggingface.co/google/gemma-2-27b-it,27,13000,482:1,2,75.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,Dense,Announce: https://blog.google/technology/developers/google-gemma-2/
LearnLM,Google DeepMind,https://learning.google.com/experiments/learn-about/signup,1500,30000,20:1,22.4,,,72,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¡,https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf,MoE,"Fine-tuned + prompted Gemini (Dec/2023). ""The results of LearnLM-Tutor reproduce the performance of Gemini Pro, for example an MMLU score of 0.72 and MATH score of 0.33."""
Gemini 1.5 Flash,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,8,10000,"1,250:1",0.9,78.9,59.1,39.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://goo.gle/GeminiV1-5,MoE,1M context length.
Med-Gemini-L 1.0,Google DeepMind,https://twitter.com/alan_karthi/status/1785117450528264216,1500,30000,20:1,22.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸ”´,https://arxiv.org/abs/2404.18416,Dense,"Med-Gemini-M 1.0 and Med-Gemini-L 1.0 (Pro and Ultra finetunes) ""For language tasks that require less complex reasoning, such as summarizing medical notes and creating referral letters, we introduce Med-Gemini-M 1.0 by fine-tuning the Gemini 1.0 Pro model. For other tasks that require more advanced reasoning, we introduce Med-Gemini-L 1.0 by fine-tuning the Gemini 1.0 Ultra model using a self-training method to enable the models to efficiently use web search."""
Hawk,Google DeepMind,,7,300,43:1,0.2,35,,,ğŸ†† ğŸ“šğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.19427,Dense,MMLU=35. RNN.
Griffin,Google DeepMind,,14,300,22:1,0.2,49.5,,,ğŸ†† ğŸ“šğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.19427,Dense,MMLU=49.5. RNN.
Gemma,Google DeepMind,https://labs.pplx.ai/,7,6000,858:1,0.7,64.3,33.7,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Dense,"MMLU=64.3 (Llama 2 70B=68.9, ChatGPT 20B=70). Text only. Probably dense. Largest trained dataset (6T) besides frontier models."
Gemini 1.5 Pro,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,1500,30000,20:1,22.4,85.9,69,46.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://goo.gle/GeminiV1-5,MoE,Sparse MoE. Context window=1M and 10M for research
MedLM,Google DeepMind,https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/medlm,,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¡,https://cloud.google.com/static/vertex-ai/docs/generative-ai/medlm/MedLM-model-card.pdf,Dense,Available to 'white-listed' orgs only.
Gemini Ultra 1.0,Google DeepMind,https://deepmind.google/technologies/gemini/,1500,30000,20:1,22.4,83.7,,35.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Dense,"Original MMLU=83.7. MMLU=90.04 with prompting. Chinchilla (20:1), dense, maybe 600B-2000T."
Q-Transformer,Google DeepMind,https://qtransformer.github.io/,,,,,,,,ğŸŒ‹,Nov/2023,ğŸ”´,https://qtransformer.github.io/assets/qtransformer.pdf,Dense,"Robotics, builds on RT-1"
Mirasol3B,Google DeepMind,,3,,,,,,,ğŸŒ‹,Nov/2023,ğŸ”´,https://arxiv.org/abs/2311.05698,Dense,Combiner + autoregressive transformer for video/audio/text
PaLI-3,Google DeepMind,,5,,,,,,,ğŸŒ‹,Oct/2023,ğŸ”´,https://arxiv.org/abs/2310.09199,Dense,VLM. Next iteration of PaLI via Pathways. https://lifearchitect.ai/pathways/
RT-X,Google DeepMind,https://robotics-transformer-x.github.io/,55,,,,,,,ğŸŒ‹,Oct/2023,ğŸŸ¢,https://robotics-transformer-x.github.io/paper.pdf,Dense,"Robotics using UL2. 'RT-1 model trained using the robotic data mixture as RT-1-X, and the RT-2 model trained using the robotic data mixture as RT-2-X.'"
Med-PaLM M,Google DeepMind,,540,780,2:1,2.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Jul/2023,ğŸ”´,https://arxiv.org/abs/2307.14334,Dense,Uses PaLM 1. Already outperformed by Med-PaLM 2. Med-PaLM Multimodal (Med-PaLM M).
DIDACT,Google DeepMind,,,37900,,,,,,ğŸŒ‹,Jun/2023,ğŸ”´,https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html,Dense,Iterative coding model trained on Google's monorepo. Jacob: https://twitter.com/jacobaustin132/status/1663972128176128002
Med-PaLM 2,Google DeepMind,,340,3600,11:1,3.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Mar/2023,ğŸ”´,https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/,Dense,"Recently, our next iteration, Med-PaLM 2, consistently performed at an â€œexpertâ€ doctor level on medical exam questions, scoring 85%. This is an 18% improvement from Med-PaLMâ€™s previous performance and far surpasses similar AI models."
Med-PaLM 1,Google DeepMind,,540,780,2:1,2.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Dec/2022,ğŸ”´,https://arxiv.org/abs/2212.13138,Dense,Collab between Google & DeepMind. Makes 1% less errors than humans
H2O-Danube3-4B,H2O.ai,https://h2o.ai/platform/danube/personal-gpt/,4,6000,"1,500:1",0.5,55.18,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jul/2024,ğŸŸ¢,https://arxiv.org/abs/2407.09276,Dense,"Runs natively and fully offline on mobile phone. ""H2O-Danube3 is a family of decoder only LLM models that use the general Llama model architecture adopting core principles from Llama 2 and Mistral with custom parameters determining the shape of each layer and total parameter count. We use the Mistral tokenizer..."" MMLU for chat=54.74, base=55.18 via https://huggingface.co/h2oai/h2o-danube3-4b-base"
LeoLM,Hessian AI/LAION,https://huggingface.co/LeoLM/leo-hessianai-13b,13,2065,159:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://laion.ai/blog/leo-lm/,Dense,Llama 2 'extended' and pretrained on 2000B Llama 2 tokens + 65B tokens of German
Cosmo-1B,HF,https://huggingface.co/HuggingFaceTB/cosmo-1b,1.8,180,100:1,0.1,,,,âš›ï¸,Feb/2024,ğŸŸ¢,https://huggingface.co/blog/cosmopedia,Dense,Synthetic data (25B tokens of synthetic data for 6 epochs + code). MMLU=32.4
StarCoder 2,HF/ServiceNow,,15,4300,287:1,0.8,,,,ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.19173,Dense,"The Stack v2=900B tokens, 5 epochs to 4.3T tokens"
StarCoder,HF/ServiceNow,https://huggingface.co/bigcode/starcoderbase,15.5,1000,65:1,0.4,,,,ğŸŒ‹,May/2023,ğŸŸ¢,https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view,Dense,
PanGu 5.0 Super,Huawei,https://www.huaweicloud.com/intl/en-us/product/modelarts.html,1000,20000,20:1,14.9,,,,ğŸŒ‹,Jun/2024,ğŸŸ¡,https://www.huaweicentral.com/huawei-cloud-unveils-pangu-large-model-5-0/,MoE,https://x.com/faridofanani96/status/1804079517193113850/photo/1
YunShan,Huawei,,7,1748,250:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸ”´,https://arxiv.org/abs/2312.17276,Dense,Finance + law fine-tune of PanGu-Ï€
PanGu-Pi,Huawei,,7,1600,229:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸ”´,https://arxiv.org/abs/2312.17276,Dense,"Dense, named PanGu-Ï€"
PanGu-Sigma,Huawei,,1085,,,,,,,ğŸŒ‹,Mar/2023,ğŸ”´,https://arxiv.org/abs/2303.10845,MoE,Sparse. 1.085T parameters named PanGu-Î£.
PanGu-Coder,Huawei,,2.6,,,,,,,ğŸŒ‹,Jul/2022,ğŸ”´,https://arxiv.org/abs/2207.11280,Dense,Python via GH
ruGPT-3,Huawei/Sberbank,Sber Cloud,1.3,,,,,,,"ğŸ•¸ ""170GB data""",Feb/2021,ğŸŸ¢,https://github.com/sberbank-ai/ru-gpts,Dense,Russian GPT-3 with input from Huawei
SmolLM2,Hugging Face,https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9,1.7,1000,589:1,0.1,42.3,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9,Dense,"Base and instruct versions, with Apache 2.0 license"
SmolLM,Hugging Face,https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966,1.7,1000,589:1,0.1,39.97,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jul/2024,ğŸŸ¢,https://huggingface.co/blog/smollm,Dense,"Dataset includes new Cosmopedia v2 synthetic data. 135M and 360M models,each trained on 600B tokens from Smollm-Corpus. 1.7B model trained on 1T tokens from Smollm-Corpus."
Idefics2,Hugging Face,https://huggingface.co/HuggingFaceM4/idefics2-8b,8.4,,,,,,,ğŸ†† ğŸ•¸,Apr/2024,ğŸŸ¢,https://huggingface.co/blog/idefics2,Dense,Clone of Flamingo now using Mistral 7B. Named after Asterix and Obelix's dog Idefix (Image-aware Decoder Enhanced Ã  la Flamingo with Interleaved Cross-attentionS)
IDEFICS,Hugging Face,https://huggingface.co/spaces/HuggingFaceM4/idefics_playground,80,,,,,,,ğŸ†† ğŸ•¸,Aug/2023,ğŸŸ¢,https://huggingface.co/blog/idefics,Dense,Clone of Flamingo using Llama-1 65B. Named after Asterix and Obelix's dog Idefix (Image-aware Decoder Enhanced Ã  la Flamingo with Interleaved Cross-attentionS)
Tk-Instruct,Hugging Face,https://instructions.apps.allenai.org/demo,11,,,,,,,ğŸ†€ğŸ…° ğŸ•¸,Apr/2022,ğŸŸ¢,https://arxiv.org/abs/2204.07705,Dense,Based on T5.
Zephyr 141B-A35B,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,35,2000,58:1,0.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2403.07691,MoE,mixtral-8x22b finetune using Odds Ratio Preference Optimization (ORPO).
Zephyr,Hugging Face H4,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,7.3,800,110:1,0.3,,33,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha,Dense,Mistral with 'aligned' data removed from dataset
Granite 3.1 8B,IBM,https://huggingface.co/ibm-granite/granite-3.1-8b-instruct,8,12000,"1,500:1",1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://github.com/ibm-granite/granite-3.1-language-models?tab=readme-ov-file,Dense,
Granite 3.0 8B,IBM,https://huggingface.co/ibm-granite/granite-3.0-8b-base,8,12000,"1,500:1",1,65.54,33.27,32.13,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,http://ibm.biz/granite-report,Dense,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
Granite-3.0-3B-A800M-Instruct,IBM,https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct,3,10000,"3,334:1",0.6,50.16,20.51,26.85,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,http://ibm.biz/granite-report,MoE,Announce: https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models
Granite Code,IBM,https://github.com/ibm-granite/granite-code-models,34,3500,103:1,1.1,50,,,ğŸŒ‹,May/2024,ğŸŸ¢,https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf,Dense,"MMLU=50 for 8B model only. Dataset: publicly available datasets (e.g., GitHub Code Clean, Starcoder data), public code repositories, and issues from GitHub."
MoLM,IBM,https://github.com/ibm/moduleformer,8,300,38:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2306.04640,MoE,ModuleFormer is based on the Sparse Mixture of Experts (MoE).
Granite,IBM,https://www.ibm.com/granite,13,2500,193:1,0.6,57,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://www.ibm.com/downloads/cas/X9W4O6BM,,"Original trained on 1T tokens, update 15/Feb/2024 trained on 2.5T tokens: granite-13b-chat-v2 (v2.1.0). ""At IBM, we curated 6.48TB of data to train our LLM Granite.13B. This was reduced to 2.07 TB after pre-processing, a 68% decrease."""
LongLLaMA,IDEAS/DeepMind,https://github.com/CStanKonrad/long_llama,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://arxiv.org/abs/2307.03170,Dense,256k context length
Yuan 2.0,IEIT,https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/README-EN.md,102.6,288,3:1,0.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://arxiv.org/abs/2311.15786,Dense,"Chinese + EN dataset include The Pile: DM, arxiv, wikipedia, book3, stack exchange, Freelaw and medical"
iFlytekSpark-13B,iFlyTek,https://gitee.com/iflytekopensource/iFlytekSpark-13B,13,3000,231:1,0.7,63.02,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://www.ithome.com/0/748/030.htm,Dense,"pre-trained on a massive high-quality data set with a total of more than 3 trillion tokens, and then fine-tuned on fine-tuned diversified alignment data.'"
Xinghuo 3.5 (Spark),iFlyTek,,200,4000,20:1,3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://www.laitimes.com/en/article/6f50u_6vhbm.html,Dense,GPT-4 competitor. https://www.shine.cn/biz/tech/2401304331/
Jais,Inception,https://huggingface.co/inception-mbzuai,13,395,31:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2023,ğŸŸ¢,https://arxiv.org/abs/2308.16149,Dense,"Arabic, trained in Abu Dhabi, UAE using Cerebras."
ChuXin,Independent,https://huggingface.co/chuxin-llm/Chuxin-1.6B-Base,1.6,2300,"1,438:1",0.2,41.07,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.04828,Dense,"""results on the â€Needle In A Haystackâ€(NIAH) tests indicate that ChuXin-1M performs well across all context window lengths up to 1M."""
Parakeet,Independent,https://colab.research.google.com/drive/1gI8CM9Bz9ov0-E6aL2jF808rE56UtZyF?usp=sharing,0.378,3,8:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://news.ycombinator.com/item?id=39745700#39745702,Dense,Tiny model (378M) for testing
phi-CTNL,Independent,,0.1,0.01,1:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2309.08632,Dense,Satire. MMLU=100. 'phi-CTNL (pronounced â€œfictionalâ€) that achieves perfect results across diverse academic benchmarks'
GPT-4chan,Independent,https://huggingface.co/ykilcher/gpt-4chan/discussions/4,6,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2022,ğŸŸ¢,https://arxiv.org/abs/2001.07487,Dense,Warning for inappropriate content. GPT-J.
Inflection-3 Pi (3.0),Inflection AI,https://developers.inflection.ai/,1200,20000,17:1,16.3,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Oct/2024,ğŸŸ¢,https://developers.inflection.ai/docs,Dense,"Inference via Intel GaudiÂ® 3 128 GB, on-premise available. Minimum spend $100 credits."
Inflection-3 Productivity (3.0),Inflection AI,https://developers.inflection.ai/,1200,20000,17:1,16.3,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Oct/2024,ğŸŸ¢,https://developers.inflection.ai/docs,Dense,"Inference via Intel GaudiÂ® 3 128 GB, on-premise available. Minimum spend $100 credits."
Inflection-2.5,Inflection AI,https://inflection.ai/inflection-2,1200,20000,17:1,16.3,85.5,,38.4,ğŸ†† ğŸ“š â¬† ğŸ•¸,Mar/2024,ğŸŸ¢,https://inflection.ai/inflection-2-5,Dense,
Inflection-2,Inflection AI,https://inflection.ai/inflection-2,1200,20000,17:1,16.3,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Nov/2023,ğŸŸ¢,https://inflection.ai/inflection-2,Dense,"â€œnow the 2nd best LLM in the worldâ€. Finished training 19/Nov/2023, waiting for fine-tuning and release."
Inflection-1,Inflection AI,https://docs.google.com/forms/d/e/1FAIpQLScM9Iz1KzaRlfgDrYrldoPDnXbhO5LW3-hqmQCd56YpheEN7g/viewform,120,2000,17:1,1.6,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Jun/2023,ğŸŸ¢,https://inflection.ai/assets/Inflection-1_0622.pdf,Dense,"Comparable with benchmarking results from InternLM 104B, 1-2% better. â€˜Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.â€™"
Pi,Inflection AI,https://pi.ai/talk,60,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,May/2023,ğŸŸ¢,https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2022/03/08/reid-hoffman-has-set-up-a-new-ai-company-with-deepminds-co-founder.html,Dense,No indication of params/tokens. Devs from DeepMind.
ModernBERT,International,https://huggingface.co/blog/modernbert,0.395,2000,"5,064:1",0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.13663v1,Dense,"""a proper workhorse model, for retrieval, classification, etc."" https://bsky.app/profile/howard.fm/post/3ldod2afps62x"
Moxin-7B,International,https://github.com/moxin-org/Moxin-LLM,7,2000,286:1,0.4,60.97,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.06845,Dense,"""Fully Open Source"" with pre-training code, configurations, training and fine-tuning datasets, and intermediate checkpoints."
DCLM-Baseline 7B 2.6T,International,https://huggingface.co/apple/DCLM-Baseline-7B,7,2600,372:1,0.4,63.7,,,ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¡,https://arxiv.org/abs/2406.11794,Dense,"New dataset: 240T tokens: 8Ã— larger than previous SOTA dataset. DCLM-Pool is 240T, DCLM-Baseline is 3.8T: ""we combine our 3.8T DCLM-BASELINE with the StarCoder and ProofPile2 data to arrive at a 4.1T token dataset. We train a 7B model for 2.5T tokens"" and ""We release the DCLM benchmark, framework, models, and datasets at https://datacomp.ai/dclm."""
MAP-Neo,International,https://map-neo.github.io/,7,4500,643:1,0.6,58.14,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.19327,Dense,"""first fully open-sourced bilingual LLM with comparable performance to existing state-of-the-art LLMs... we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided."""
Aurora-M,International,https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407,15.5,2035,132:1,0.6,,,,ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2404.00399,Dense,
Reader-LM,Jina AI,https://huggingface.co/jinaai/reader-lm-1.5b,1.54,2.5,2:1,0,,,,ğŸ•¸,Sep/2024,ğŸŸ¢,https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/,Dense,"HTML->Markdown. Specialist small model; outperforms GPT-4o general model, does not outperform Gemini Pro 1.5."
jina-embeddings-v2,Jina AI,https://huggingface.co/jinaai/jina-embeddings-v2-base-en,0.435,,,,,,,ğŸ•¸,Oct/2023,ğŸŸ¢,https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/,Dense,Alternative to text-embedding-ada-002. Related v1 paper: https://arxiv.org/abs/2307.11224
DocLLM,JPMorgan,,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸ”´,https://arxiv.org/abs/2401.00908,Dense,Document spatial layout structure.
AceGPT,KAUST/Shenzhen,https://huggingface.co/FreedomIntelligence/AceGPT-13B,13,2010,155:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://github.com/FreedomIntelligence/AceGPT/tree/main,Dense,Arabic. Llama 2 + RLAIF
Skywork MoE 16x13B,Kunlun Tech,https://huggingface.co/Skywork/Skywork-MoE-Base,146,,,,77.4,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf,MoE,"CN + EN. ""(MoE) model with 146 billion parameters, 16 experts, and 22 billion activated parameters. This model is initialized from the pre-existing dense checkpoints of our Skywork-13B model."""
Skywork-13B,Kunlun Tech,,13,3200,247:1,0.7,62.7,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://arxiv.org/abs/2310.19341,Dense,CN + EN.
Helium-1,Kyutai,https://huggingface.co/kyutai/helium-1-preview-2b,2,2500,"1,250:1",0.2,51.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://kyutai.org/2025/01/13/helium.html,Dense,"""Helium-1 preview, an initial version of our new backbone language model with 2B parameters, targeting edge and mobile devices... We use token level distillation of a 7B parameters model to train Helium-1 preview."""
Helium 7B,Kyutai,https://moshi.chat/,7,1000,143:1,0.3,,,,âš›ï¸,Jul/2024,ğŸŸ¢,https://youtu.be/hm2IJSKcYvo,Dense,"""1. The model is fine-tuned on 100K transcripts generated by Helium itself. 2. These transcripts are highly detailed, heavily annotated with emotion and style, and conversational. 3. Text to Speech Engine is further fine-tuned on 20 hours of audio recorded by Alice and licensed."""
OpenFlamingo-9B,LAION,https://huggingface.co/openflamingo/OpenFlamingo-9B,8.3,1000,121:1,0.3,,,,ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://laion.ai/blog/open-flamingo/,Dense,Uses LLaMA-7B. Demo: https://7164d2142d11.ngrok.app/
EXAONE-3.5,LG,https://huggingface.co/collections/LGAI-EXAONE/exaone-35-674d0e1bb3dcd2ab6f39dbb4,32,6500,204:1,1.5,78.3,,39.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.04862,Dense,â€œEXAONEâ€=â€œEXpert AI for EveryONEâ€. Training tokens/ratio dropped from EXAONE-3 7.8B with 8T (Aug/2024) to this (Dec/2024) 7.8B with 9T to 32B with 6.5T.
EXAONE 3.0,LG,https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,7.8,8000,"1,026:1",0.8,,27.4,10.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2408.03541,Dense,â€œEXAONEâ€=â€œEXpert AI for EveryONEâ€
Alfred-40B-0723,LightOn,https://huggingface.co/lightonai/alfred-40b-0723,40,1000,25:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://www.lighton.ai/blog/lighton-s-blog-4/introducing-alfred-40b-0723-38,Dense,First finetuned version of Falcon with RLHF. Enterprise: https://www.lighton.ai/paradigm
VLM-4,LightOn,Muse,10,,,,,,,ğŸ•¸,Mar/2022,ğŸŸ¢,https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2022/03/08/reid-hoffman-has-set-up-a-new-ai-company-with-deepminds-co-founder.html,Dense,Params corrected 25/Apr/2022
EON-8B,LinkedIn,https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform,8,15000,"1,875:1",1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸ”´,https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform,Dense,"""We found the EON-8B model (a domain-adapted Llama 3.1-8B variant) to be 75x and 6x cost effective in comparison to GPT-4 and GPT-4o respectively (Figure 4)... On tasks seen during training, the EON-8B model outperformed base Llama-3-8B-Instruct and its performance was comparable to SOTA GPT models."""
LFM-40B,Liquid AI,https://labs.perplexity.ai/,40,2000,50:1,0.9,78.76,55.63,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://www.liquid.ai/liquid-foundation-models,MoE,"40BA12B. Some controversy/concern over company. Liquid Foundation Models (LFM). ""Human preference optimization techniques have not been applied extensively to our models yet."""
K2,LLM360,https://huggingface.co/LLM360/K2,65,1400,22:1,1,64.8,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://www.llm360.ai/blog/several-new-releases-to-further-our-mission.html,Dense,"""K2-65B is a fully reproducible LLM outperforming Llama 2 70B using 35% less compute."""
MaLA-500,LMU,https://huggingface.co/MaLA-LM/mala-500,10,2000,200:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.13303,Dense,Extends Llama 2 7B to 10B using 534 languages.
LTM-2-mini,Magic,https://magic.dev/blog/100m-token-context-windows,20,2000,100:1,0.7,,,,ğŸŒ‹,Aug/2024,ğŸ”´,https://magic.dev/blog/100m-token-context-windows,Dense,Context=100M tokens equals ~10 million lines of code or ~750 novels.
LTM-1,Magic,https://magic.dev/blog/ltm-1,,,,,,,,ğŸŒ‹,Jun/2023,ğŸ”´,https://magic.dev/blog/ltm-1,Dense,Context window=5M
Llama 4,Meta AI,https://x.com/Ahmad_Al_Dahle/status/1851822285377933809,,,,,,,,,TBA,,,Dense,"Training Oct/2024-Feb/2025 on 100,000 H100s. Due 2025."
BLT,Meta AI,https://github.com/facebookresearch/blt,8,4500,563:1,0.6,57.4,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/,Dense,"Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance"
Large Concept Model,Meta AI,https://github.com/facebookresearch/large_concept_model?tab=readme-ov-file,7,2700,386:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/,Dense,"""autoregressive sentence prediction in an embedding space."" 7.7T tokens is a misprint, should be 2.2T as in paper."
Llama 3.3,Meta AI,https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct,70,15000,215:1,3.4,86,68.9,50.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,Dense,"Drop-in replacement for Llama 3.1 70B, comparable performance to Llama 3.1 405B."
Llama 3.2 90B,Meta AI,https://www.llama.com/,90,9000,100:1,3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://www.llama.com/,Dense,Vision (VLM)
Llama 3.2 3B,Meta AI,https://www.llama.com/,3.21,9000,"2,804:1",0.6,63.4,,32.8,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://www.llama.com/,Dense,"Text (LLM). ""Pre-training. [For Llama 3.2 3B] We prune the models from their 8B siblings and use logits from the 8B and 70B models as token-level targets (token-level distillation). We then use knowledge distillation to recover performance."""
Llama 3.1 405B,Meta AI,https://www.meta.ai/,405,15600,39:1,8.4,88.6,73.3,51.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,Dense,Announce: https://ai.meta.com/blog/meta-llama-3-1/ Model card: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md
Chameleon,Meta AI,https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live,34,9200,271:1,1.9,65.8,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.09818,Dense,Multimodal
Llama 3 70B,Meta AI,https://meta.ai/,70,15000,215:1,3.4,82,52.8,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://ai.meta.com/blog/meta-llama-3/,Dense,Instruct MMLU-Pro=56.2
MobileLLM,Meta AI,https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95,1,1000,"1,000:1",0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.14905,Dense,Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
CodeLlama-70B,Meta AI,https://huggingface.co/codellama/CodeLlama-70b-hf,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/,Dense,Paper link is to 34B from Aug/2023. This 70B model finished training Jan/2024.
SeamlessM4T-Large v2,Meta AI,https://seamless.metademolab.com/expressive/,2.3,,,,,,,ğŸŒ‹,Nov/2023,ğŸŸ¢,https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/,Dense,Based on NLLB and older models. https://github.com/facebookresearch/seamless_communication
Llama 2 Long,Meta AI,,70,2400,35:1,1.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸ”´,https://arxiv.org/abs/2309.16039,Dense,"Unreleased to date. Context window=32,768 tokens (compare to Llama 2=4096 tokens)"
Code Llama 34B,Meta AI,https://github.com/facebookresearch/codellama,34,2600,77:1,1,,,,ğŸ†† ğŸ•¸,Aug/2023,ğŸŸ¢,https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/,Dense,"Outperforms GPT-3.5. Initial Llama 2 (2T tokens) trained on 500B tokens of code, 100B tokens of python"
Llama 2,Meta AI,https://www.llama2.ai/,70,2000,29:1,1.2,68.9,37.5,26.26,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,Dense,"Context window=4096. MMLU=68.9 (GPT-3.5=70.0, GPT-4=86.4)"
BlenderBot 3x,Meta AI,https://parl.ai/projects/bb3x/,175,300,2:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Jun/2023,ğŸŸ¢,https://arxiv.org/abs/2306.04707,Dense,OPT-175B with new dialogue data
LIMA,Meta AI,,65,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸ”´,https://arxiv.org/abs/2305.11206,Dense,"LLaMA-65B with nearly no fine-tuning, no RLHF"
LLaMA-65B,Meta AI,Weights leaked: https://github.com/facebookresearch/llama/pull/73/files,65,1400,22:1,1,68.9,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Feb/2023,ğŸŸ¢,https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/,Dense,"Researchers only, noncommercial only. 'LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B.'"
Toolformer+Atlas 11B+NLLB 54B,Meta AI,Replicated: https://github.com/conceptofmind/toolformer,6.7,402,60:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2023,ğŸ”´,https://arxiv.org/abs/2302.04761,Dense,Based on GPT-J 6.7B + access to other models via API
OPT-IML,Meta AI,https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML,175,300,2:1,0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸,Dec/2022,ğŸŸ¢,https://arxiv.org/abs/2212.12017,Dense,Instruct
Galactica,Meta AI,https://galactica.org/,120,450,4:1,0.8,52.6,,,ğŸ“š,Nov/2022,ğŸŸ¢,https://galactica.org/static/paper.pdf,Dense,scientific only
Atlas,Meta AI,,11,40,4:1,0.1,47.9,,,ğŸ†† ğŸ•¸,Aug/2022,ğŸŸ¢,https://arxiv.org/abs/2208.03299,Dense,
BlenderBot 3,Meta AI,blenderbot.ai (US only),175,300,2:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Aug/2022,ğŸŸ¢,https://github.com/facebookresearch/ParlAI/blob/main/projects/bb3/BB3_main_tech_report.pdf,Dense,
NLLB,Meta AI,Github (train/deploy),54.5,,,,,,,ğŸŒ‹,Jul/2022,ğŸŸ¢,https://research.facebook.com/publications/no-language-left-behind/,MoE,"54.5B MOE, 3.3B dense. 200+ languages"
OPT-175B,Meta AI,HF (train/deploy),175,300,2:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,May/2022,ğŸŸ¢,https://arxiv.org/abs/2205.01068,Dense,Only 30B available (Jun/2022)
InCoder,Meta AI,https://huggingface.co/spaces/facebook/incoder-demo,6.7,,,,,,,"GH, StackOverflow",Apr/2022,ğŸŸ¢,https://arxiv.org/abs/2204.05999,Dense,Python and JavaScript
SeeKeR,Meta AI,,2.7,,,,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Mar/2022,ğŸŸ¢,https://arxiv.org/abs/2203.13224,Dense,BART and compared to GPT-2
CM3,Meta AI,,13,,,,,,,ğŸ†† ğŸ•¸,Jan/2022,ğŸŸ¢,https://arxiv.org/abs/2201.0752,Dense,LLM with multimodal capabilities
XGLM,Meta AI,,7.5,,,,,,,ğŸ•¸,Dec/2021,ğŸŸ¢,https://arxiv.org/abs/2112.10668,Dense,"Multilingual: 30 languages, 16 families."
Fairseq,Meta AI,"TS, Goose",1100,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ•¸ ğŸ•¸,Dec/2021,ğŸŸ¢,https://arxiv.org/abs/2112.10684,Dense,13B & 1100B param models.
BlenderBot 2.0,Meta AI,,9.4,,,,,,,ğŸ‘¥ ğŸŒ‹,Jul/2021,ğŸŸ¢,https://parl.ai/projects/blenderbot2/,Dense,Chatbot
Megatron-11B,Meta AI,InferKit,11,2200,200:1,0.5,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Apr/2020,ğŸŸ¢,https://github.com/pytorch/fairseq/tree/main/examples/megatron_11b,Dense,My favourite model until GPT-3 and GPT-4 came along: https://github.com/facebookresearch/fairseq/blob/main/examples/megatron_11b/README.md
RoBERTa,Meta AI,Hugging Face,0.355,2200,"6,198:1",0.1,27.9,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Jul/2019,ğŸŸ¢,https://arxiv.org/abs/1907.11692,Dense,"calcs: ""In total, this batch size and number of steps corresponds to pre-training on 235 â‰ˆ 34B tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2 35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training."" https://arxiv.org/pdf/1910.10683.pdf MMLU shows RoBERTa-base 125M only=27.9 (not 355M)"
MAI-1,Microsoft,https://arstechnica.com/information-technology/2024/05/microsoft-developing-mai-1-language-model-that-may-compete-with-openai-report/,500,10000,20:1,7.5,,,,,TBA,,https://www.reuters.com/technology/microsoft-readies-new-ai-model-compete-with-google-openai-information-reports-2024-05-06/,Dense,Potential failed training run 2024. MAI=Microsoft artificial intelligence. MSFT CTO statement: https://archive.md/XRSgS
phi-4,Microsoft,https://huggingface.co/microsoft/phi-4,14,10000,715:1,1.2,84.8,70.4,56.1,âš›ï¸,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.08905,Dense,Use unsloth: https://huggingface.co/unsloth/phi-4-GGUF & https://www.reddit.com/r/singularity/comments/1i0kso4/i_fixed_4_bugs_in_microsofts_opensource_phi4_model/
GRIN MoE,Microsoft,https://huggingface.co/microsoft/GRIN-MoE,60,4025,68:1,1.6,79.4,,,âš›ï¸,Sep/2024,ğŸŸ¢,https://huggingface.co/microsoft/GRIN-MoE/blob/main/GRIN_MoE.pdf,MoE,"16x3.8B ""only 6.6B activate parameters"". GRIN=GRadient-INformed. ""GRIN MoE is pre-trained on 4T tokens as a Causal Language Model. The same training dataset has been used to train Phi-3 dense models"""
phi-3.5-MoE,Microsoft,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,60,4900,82:1,1.8,78.9,54.3,36.8,âš›ï¸,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2407.13833https://arxiv.org/abs/2407.13833,MoE,
phi-3.5-mini,Microsoft,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,3.8,3400,895:1,0.4,69,47.4,30.4,âš›ï¸,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2407.13833,Dense,
SpreadsheetLLM,Microsoft,,1760,13000,8:1,15.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸ”´,https://arxiv.org/abs/2407.09025v1,Dense,"Notable finetune of GPT4-0125-preview ""outperforming the vanilla approach by 25.6% in GPT4â€™s in-context learning setting"""
Causal Axioms,Microsoft,,0.067,1.2,18:1,0,,,,âš›ï¸,Jul/2024,ğŸ”´,https://arxiv.org/abs/2407.07612v1,Dense,"""the training dataset follows a specific structure, we develop a custom tokenizer. Alphanumeric node names are tokenized at a character level, while special terms such as â€˜causesâ€™, â€˜Doesâ€™, â€˜causeâ€™, â€˜Yesâ€™, and â€˜Noâ€™ are tokenized at the word level... Our training setup consists of around 175k instances of sequential chains with size of chains ranging from 3 to 6 nodes... All models are trained for 100 epochs. [LifeArchitect.ai estimate is 12 tokens per node x 6 nodes x 175,000 instances x 100 epochs = 1.26B tokens]"" Based on GPT-2 arch."
YOCO,Microsoft,https://github.com/microsoft/unilm/tree/master/YOCO,3,1600,534:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://arxiv.org/abs/2405.05254,Dense,"With Tsingua. You Only Cache Once (YOCO). Long context ""1M context length with near-perfect needle retrieval accuracy"""
TinyStories,Microsoft,https://huggingface.co/roneneldan/TinyStories-33M,0.033,50,"1,516:1",0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2305.07759,Dense,Precursor to phi.
phi-3-medium,Microsoft,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,14,4800,343:1,0.9,78.2,55.7,,âš›ï¸,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.14219,Dense,"Preview only, benchmarks being investigated as of May/2024."
phi-3-mini,Microsoft,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,3.8,3300,869:1,0.4,68.8,45.7,,âš›ï¸,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.14219,Dense,"""phi3-mini can be quantized to 4-bits so that it only occupies â‰ˆ 1.8GB of memory. We tested the quantized model by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running natively on-device and fully offline achieving more than 12 tokens per second."""
WizardLM-2-8x22B,Microsoft,https://huggingface.co/MaziyarPanahi/WizardLM-2-8x22B-GGUF,141,2000,15:1,1.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://wizardlm.github.io/WizardLM2/,MoE,Base model = mistral-8x22b.
BitNet b1.58,Microsoft,https://huggingface.co/1bitLLM/bitnet_b1_58-xl,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.17764,Dense,
WaveCoder-DS-6.7B,Microsoft,,6.7,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸ”´,https://arxiv.org/abs/2312.14187,Dense,"To obtain WaveCoder models, We choose StarCoder-15B, CodeLLaMa (7B and 13B), DeepseekCoder-6.7B as the base model and fine-tune all the base model for 3 epochs"
Transformers-Arithmetic,Microsoft,,0.1,0.3,3:1,0,,,,â¬†,Nov/2023,ğŸ”´,https://arxiv.org/abs/2311.14737,Dense,Proving maths is not memorized. Uses GPT-2-style model. SÃ©bastien Bubeck
Orca 2,Microsoft,,13,2001,154:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¡,https://arxiv.org/abs/2311.11045,Dense,"Llama 2 13B (2T) -> Orca 2 (GPT-4 finetune). Still an imitation model, overhyped: The False Promise of Imitating Proprietary LLMs https://arxiv.org/abs/2305.15717"
Phi-2,Microsoft,https://replicate.com/lucataco/phi-2,2.7,1400,519:1,0.2,,,,âš›ï¸,Nov/2023,ğŸŸ¢,https://huggingface.co/microsoft/phi-2,Dense,https://twitter.com/SebastienBubeck/status/1724854157004190095
Florence-2,Microsoft,https://huggingface.co/microsoft/Florence-2-large,0.771,,,,,,,ğŸŒ‹,Nov/2023,ğŸŸ¢,https://arxiv.org/abs/2311.06242,Dense,"VLM, Flamingo alt"
Kosmos-2.5,Microsoft,,1.3,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸ”´,https://arxiv.org/abs/2309.11419,Dense,
Phi-1.5,Microsoft,https://huggingface.co/microsoft/phi-1_5,1.3,150,116:1,0,,,,ğŸ“šâš›ï¸,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2309.05463,Dense,Textbooks only. 30B-token dataset
WizardLM,Microsoft,https://huggingface.co/WizardLM/WizardLM-70B-V1.0,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Aug/2023,ğŸŸ¢,https://github.com/nlpxucan/WizardLM,Dense,Assume Llama-2 fine-tune. Outperforms text-davinci-003. May merge this entry with the Apr/2023 7B release
Kosmos-2,Microsoft,https://44e505515af066f4.gradio.app/,1.6,360,225:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2023,ğŸŸ¢,https://arxiv.org/abs/2306.14824,Dense,Proto-AGI. Multimodal large language model (MLLM). a multimodal large language model with grounding capability built upon KOSMOS-1
Phi-1,Microsoft,,1.3,51,40:1,0,,,,ğŸ“šâš›ï¸,Jun/2023,ğŸ”´,https://arxiv.org/abs/2306.11644,Dense,"Code model. â€˜breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, for roughly 8 passes over 7B tokens (slightly over 50B total tokens seen) followed by finetuning on less than 200M tokens.â€™"
Orca,Microsoft,https://aka.ms/orca-lm,13,1000,77:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2023,ğŸŸ¡,https://arxiv.org/abs/2306.02707,Dense,"LLaMA -> Vicuna -> Orca (GPT-4 finetune). Still an imitation model, overhyped: The False Promise of Imitating Proprietary LLMs https://arxiv.org/abs/2305.15717"
WizardLM,Microsoft,https://6f8173a3550ed441ab.gradio.live/,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Apr/2023,ğŸŸ¢,https://arxiv.org/abs/2304.12244,Dense,LLaMA 7B self-instructed fine-tune.
Kosmos-1,Microsoft,,1.6,360,225:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2023,ğŸ”´,https://arxiv.org/abs/2302.14045,Dense,"Proto-AGI. Multimodal large language model (MLLM). Ravenâ€™s Progressive Matrices as real images, not digits as in testing of text-davinci-003 at https://lifearchitect.ai/ravens/"
FLAME,Microsoft,,0.06,9,150:1,0,,,,ğŸŒ‹,Jan/2023,ğŸ”´,https://arxiv.org/abs/2301.13779,Dense,"T5 for Excel formulas, very small 60M params, ""We start from a dataset of 927M formulas"" estimate 10x multiplier for 9B tokens"
PACT,Microsoft,https://github.com/microsoft/PACT,,0.03,,0,,,,ğŸŒ‹,Oct/2022,ğŸŸ¢,https://arxiv.org/abs/2209.11133,Dense,"Trained on ~5TB data, 2GB model download. 'In general we see an improvement in model performance as we increase the number of training tokens. Interestingly, larger models did not necessarily result in better performance for robot navigation. Even though larger models consistently presented better loss values for action prediction on a static dataset, (Fig. 7 b), when it comes to real-time deployment the larger network capacity introduces inference delays that become a disadvantage and lead to earlier crashes. For example, while LiDAR perception measurements arrive to the vehicle every 0.077s (13Hz), the largest model of 24 layers takes on average 0.023s for inference with a RTX3090 GPU, roughly 40% longer the 3 layer model (0.016s). These time differences can amount to even larger performance gaps in small embedded systems, and further emphasize the importance of multiple downstream task architectures sharing a common representation branch for real-time robotics applications.'"
Z-Code++,Microsoft,,0.71,500,705:1,0.1,,,,ğŸ†† ğŸ•¸,Aug/2022,ğŸ”´,https://arxiv.org/abs/2208.09770v1,Dense,"abstractive text summarization, 710M, outperforms PaLM 540B. ""Due to the limited computational resource, Z-Code++LARGE is trained with only 500B tokens instead of 1T tokens as that for mT5 training."""
GODEL-XL,Microsoft,,2.7,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Jun/2022,ğŸŸ¢,https://arxiv.org/abs/2206.11309#microsoft,Dense,"XL: GPT-3 175B in paper, GPT-J 2.7B released"
DeBERTaV3,Microsoft,,1.5,162,108:1,0.1,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Nov/2021,ğŸŸ¢,https://arxiv.org/abs/2111.09543,Dense,RoBERTa=162B token dataset.
MT-NLG,Microsoft/NVIDIA,,530,270,1:1,1.3,,,,ğŸ†† ğŸ“šâ¬† ğŸŒ‹ ğŸ•¸ ğŸ•¸,Oct/2021,ğŸ”´,https://arxiv.org/abs/2201.11990,Dense,
MiniMax-Text-01,MiniMax,https://github.com/MiniMax-AI/MiniMax-01,456,7200,16:1,6,88.5,75.7,54.4,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://arxiv.org/abs/2501.08313,MoE,"A45.9B. ""The pre-training corpus for MiniMax-Text-01 encompasses a comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and programming code... repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times... Our findings indicate that low-quality data suffer a substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs"" Login playground: https://www.hailuo.ai/"
Mistral Small 3,Mistral,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,24,8000,334:1,1.5,80.73,54.37,45.3,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,Dense,"MMLU=base, -Pro=base, GPQA=instruct. ""When quantized, Mistral Small 3 can be run privately on a single RTX 4090 or a Macbook with 32GB RAM."" ""Mistral Small 3 is neither trained with RL nor synthetic data"""
Pixtral Large,Mistral,https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411,124,6000,49:1,2.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://mistral.ai/news/pixtral-large/,Dense,Open-weights multimodal model built on top of Mistral Large 2.
Ministral 8B,Mistral,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,8,6000,750:1,0.7,65,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://mistral.ai/news/ministraux/,Dense,"""Introducing the worldâ€™s best edge models"""
Pixtral-12b-240910,Mistral,https://huggingface.co/mistralai/Pixtral-12B-2409,12,6000,500:1,0.9,69.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://mistral.ai/news/pixtral-12b/,Dense,"""Pixtral was trained to be a drop-in replacement for Mistral Nemo 12B."""
Mistral Large 2,Mistral,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,123,8000,66:1,3.3,84,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://mistral.ai/news/mistral-large-2407/,Dense,Fits on a single node for inference.
NeMo,Mistral,https://huggingface.co/mistralai/Mistral-Nemo-Base-2407,12,2000,167:1,0.5,68,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://mistral.ai/news/mistral-nemo/,Dense,"With NVIDIA. ""Drop-in replacement of Mistral 7B"". ""trained using Megatron-LM, part of NVIDIA NeMo, with 3,072 H100 80GB Tensor Core GPUs"" https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/"
Codestral Mamba,Mistral,https://huggingface.co/mistralai/mamba-codestral-7B-v0.1,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://mistral.ai/news/codestral-mamba/,Dense,"""Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length."""
Mathstral,Mistral,https://huggingface.co/mistralai/mathstral-7B-v0.1,7,2000,286:1,0.4,63.47,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://mistral.ai/news/mathstral/,Dense,"""Weâ€™re contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning."""
Codestral,Mistral,https://huggingface.co/mistralai/Codestral-22B-v0.1,22,2000,91:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://mistral.ai/news/codestral/,Dense,Fluent in 80+ programming languages
mixtral-8x22b,Mistral,https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1,141,2000,15:1,1.8,77.75,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://mistral.ai/news/mixtral-8x22b/,MoE,"MoE=22Bx8, seq=65536."
Mistral Small,Mistral,https://chat.mistral.ai/chat,7,3000,429:1,0.5,72.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://mistral.ai/news/mistral-large/,Dense,Optimised for latency and cost.
Mistral Large,Mistral,https://poe.com/Mistral-Large,300,8000,27:1,5.2,81.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://mistral.ai/news/mistral-large/,Dense,"MMLU=81.2 (same as Flan-PaLM 2 340B, higher than PaLM 2 340B MMLU=78.3), 32k context window. API only (not open source)."
miqu 70b,Mistral,https://huggingface.co/miqudev/miqu-1-70b,70,3000,43:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://huggingface.co/miqudev/miqu-1-70b,Dense,"Leaked, proper version soon: https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/"
Mistral-medium,Mistral,https://poe.com/,180,3500,20:1,2.6,75.3,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://mistral.ai/news/la-plateforme/,Dense,"MMLU=75.3% (GPT-3.5-turbo 20B=70%, Llama 2 70B=68.9%)"
mixtral-8x7b-32kseqlen,Mistral,https://www.together.ai/blog/mixtral,46.7,8000,172:1,2,70.6,43.3,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2401.04088,MoE,"MoE=7Bx8, aka mistral-small. 'Concretely, Mixtral has 45B total parameters but only uses 12B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12B model.'"
Mistral 7B,Mistral,https://huggingface.co/mistralai,7.3,800,110:1,0.3,,30.9,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://mistral.ai/news/announcing-mistral-7b/,Dense,"Apache 2.0, Sliding Window Attention (SWA) to handle longer sequences at smaller cost"
JetMoE-8B,MIT,https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat,8,1250,157:1,0.3,49.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://huggingface.co/jetmoe/jetmoe-8b,MoE,
Kimi k1.5,Moonshot AI,https://github.com/MoonshotAI/kimi-k1.5?tab=readme-ov-file,500,15000,30:1,9.1,87.4,,51.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://arxiv.org/abs/2501.12599,Dense,"Reasoning. ""our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities---e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista---matching OpenAI's o1"". GPQA score is my estimate from pp13â€“14, noting that ""the scores above come from an internal long-cot model with much smaller model size than k1.5 long-CoT model."""
k0-math,Moonshot AI,https://kimi.moonshot.cn/,100,2000,20:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://www.globaltimes.cn/page/202411/1323248.shtml,Dense,"Reasoning, maths only. Very little info available. Chinese. Long context. No paper."
Kimi Chat,Moonshot AI,https://kimi.moonshot.cn/,100,2000,20:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://www.chinadaily.com.cn/a/202403/22/WS65fce476a31082fc043be1b1.html,Dense,Chinese. Long context. No paper.
DBRX,MosaicML,https://huggingface.co/spaces/databricks/dbrx-instruct,132,12000,91:1,4.2,73.7,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,MoE,"MoE. Trained for $10M on 3,072 NVIDIA H100s connected by 3.2Tbps Infiniband."
MPT,MosaicML,https://huggingface.co/mosaicml/mpt-7b,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://twitter.com/NaveenGRao/status/1654496162492084227,Dense,Llongboi' -Apache 2.0 license suitable for commercial use. -Base 7B LLM trained on 1T tokens outperforms LLaMA and GPT3. -64K+ context length. -$200k to train from scratch.
MPT,MosaicML,https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly,1.3,200,154:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2023,ğŸŸ¢,https://twitter.com/jefrankle/status/1649060478910357504,Dense,More 1B models coming with different datasets. Many more.
NexusRaven-V2 13B,Nexusflow.ai,https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo,,,,,,,,ğŸŒ‹,Dec/2023,ğŸŸ¢,https://github.com/nexusflowai/NexusRaven-V2/tree/master,Dense,Based on CodeLlama. 'surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.'
GPT4All-LoRa,Nomic,https://github.com/nomic-ai/gpt4all,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf,Dense,chatbot trained on ~800k GPT-3.5-Turbo Generations based on LLaMa
DisTrO 15B,Nous Research,https://distro.nousresearch.com/,15,100,7:1,0.1,23.48,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://github.com/NousResearch/DisTrO?tab=readme-ov-file,Dense,"""About 14 DGXes scattered around the globe. Sometimes more sometimes less, it varies depending on availability. On average, around 112 H100s."" https://x.com/bloc97_/status/1863675225810043331 ""we introduce DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware."""
OLMo-Bitnet-1B,Nous Research,https://huggingface.co/NousResearch/OLMo-Bitnet-1B,1,60,60:1,0,,,,ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2402.17764,Dense,1.58-bit quantized (ternary weights) means we can run a 70B model in ~14GB VRAM. See also BitNet b1.58
OtterHD-8B,NTU,https://github.com/Luodian/Otter,8,737,93:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://arxiv.org/abs/2311.04219,Dense,Evolution of Persimmon-9.3B and Fuyu 8B
Cosmos Nemotron 34B,NVIDIA,https://build.nvidia.com/nvidia/cosmos-nemotron-34b,34,400000,"11,765:1",12.3,,,,ğŸŒ‹,Jan/2025,ğŸŸ¢,https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai,Dense,"VLM. MMMU=47.33. ""VILA project becomes part of Cosmos Nemotron family"" https://github.com/NVlabs/Cosmos-Nemotron Vision Encoder: SigLIP-400M, Language Encoder: Yi-34B https://blogs.nvidia.com/blog/nemotron-model-families/"
Cosmos 1.0,NVIDIA,https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World,14,400000,"28,572:1",7.9,,,,ğŸŒ‹,Jan/2025,ğŸŸ¢,https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai,Dense,"WFM (world foundation model). ""The models range in size from 4 billion to 14 billion parameters, with Nano being the smallest and Ultra being the largest... ""Cosmos WFM models, were trained on 9,000 trillion tokens [9,000T] from 20 million hours of real-world human interactions, environment, industrial, robotics, and driving data..."" https://techcrunch.com/2025/01/06/nvidia-releases-its-own-brand-of-world-models/ Actual working: https://lifearchitect.ai/cosmos/"
Llama-3.1-Nemotron-70B,NVIDIA,https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct,70,15000,215:1,3.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct/modelcard,Dense,Related paper: https://arxiv.org/abs/2410.01257
nGPT,NVIDIA,https://github.com/lucidrains/nGPT-pytorch,1,400,400:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://arxiv.org/abs/2410.01131,Dense,"""a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized...reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length."""
NLVM 1.0,NVIDIA,https://huggingface.co/nvidia/NVLM-D-72B,72,18000,250:1,3.8,82,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸŸ¢,https://arxiv.org/abs/2409.11402,Dense,"Flamingo clone. ""we use Qwen2-72B-Instruct as the default text-only LLM backbone. We also employ Nous-Hermes-2-Yi-34B for ablation study and faster experimentation... we use InternViT-6B as the default vision encoder"""
Minitron-4B,NVIDIA,https://huggingface.co/nvidia/Minitron-4B-Base,4,94,24:1,0.1,58.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2407.14679,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
Minitron-8B,NVIDIA,https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base,4,94,24:1,0.1,63.8,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/,Dense,Pruned and distilled from Nemotron-4 15B: https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/
Nemotron-4-340B,NVIDIA,https://build.nvidia.com/nvidia/nemotron-4-340b-instruct,340,9000,27:1,5.8,81.1,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf,Dense,"Open-source equiv of Mar/2023 GPT-4 (1760MoEâ‰ˆ340B, 13T), same param count but 2x the tokens of May/2023 PaLM 2 (340B, 3.6T), competitor to Nov/2023 Grok-1 (314B, 6T). Trained on 6,144 H100s. ~1.3TB for inference. 50+ natural and 40+ coding languages. Trained between December 2023 and May 2024. MMLU 0-shot for instruct=78.7, 5-shot for base=81.1. Permalink for paper: https://research.nvidia.com/publication/2024-06_nemotron-4-340b"
Nemotron-4 15B,NVIDIA,,15,8000,534:1,1.2,64.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.16819,Dense,
Audio Flamingo,NVIDIA,https://huggingface.co/spaces/nvidia/audio-flamingo-demo,1,20,20:1,0,,,,ğŸŒ‹,Feb/2024,ğŸŸ¡,https://arxiv.org/abs/2402.01831,Dense,Project page: https://audioflamingo.github.io/
Nemotron-3 22B,NVIDIA,https://huggingface.co/nvidia/nemotron-3-8b-base-4k,22,3800,173:1,1,54.4,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/,Dense,"8B released, 22B internal."
Nemotron-2 43B,NVIDIA,,43,3800,89:1,1.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸ”´,https://arxiv.org/abs/2311.09528,Dense,Used to train HelpSteer (16/Nov/2023): https://arxiv.org/abs/2311.09528
Retro 48B,NVIDIA,,48,1200,25:1,0.8,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://arxiv.org/abs/2310.07713,Dense,the largest LLM pretrained with retrieval before instruction tuning.'
GPT-2B-001,NVIDIA,https://huggingface.co/nvidia/GPT-2B-001,2,1100,550:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://huggingface.co/nvidia/GPT-2B-001,Dense,No paper yet
VIMA,NVIDIA,Open: https://vimalabs.github.io/,0.2,,,,,,,ğŸŒ‹,Oct/2022,ğŸŸ¢,https://arxiv.org/abs/2210.03094,Dense,
NeMo Megatron-GPT 20B,NVIDIA,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,20,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2022,ğŸŸ¢,https://huggingface.co/nvidia/nemo-megatron-gpt-20B,Dense,
Megatron-LM,NVIDIA,,8.3,800,97:1,0.3,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Sep/2019,ğŸŸ¢,https://arxiv.org/abs/1909.08053,Dense,
GPT-5,OpenAI,https://lifearchitect.ai/whats-in-gpt-5/,5400,114000,22:1,,,,,,TBA,,,MoE,"Due 2025. Showing dense param count, but will be MoE model."
GPT-6,OpenAI,https://lifearchitect.ai/gpt-6/,,,,,,,,,TBA,,,,Due 2025.
o4,OpenAI,https://lifearchitect.ai/o4/,,,,,,,,,TBA,,,,Due 2025.
o3-mini,OpenAI,https://chatgpt.com/?model=o3-mini,20,13000,650:1,1.7,,,77,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jan/2025,ğŸŸ¢,https://openai.com/index/o3-mini-system-card/,Dense,"Reasoning. GPQA=79.7 for 'high' thinking. ALPrompt 2025H1=1/5. My analysis is that this modelâ€™s performance is very poor, with responses often becoming disordered and illogical. OpenAI compared o3-mini to OpenAIâ€™s software engineers, and it performed very poorly (o3-mini=0%, o1=12%). ""o3-mini models have the lowest performance, with scores of 0%â€¦ We suspect o3-miniâ€™s low performance is due to poor instruction following and confusion about specifying tools in the correct format. The model often attempts to use a hallucinated bash tool rather than python despite constant, multi-shot prompting and feedback that this format is incorrect. This resulted in long conversations that likely hurt its performance."" (o3-mini paper, p31)"
GPT-4b,OpenAI,,8,4000,500:1,0.6,,,,ğŸŒ‹,Jan/2025,ğŸ”´,https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/,Dense,"Protein sequence model. ""The model was trained on examples of protein sequences from many species, as well as information on which proteins tend to interact with one another. While thatâ€™s a lot of data, itâ€™s just a fraction of what OpenAIâ€™s flagship chatbots were trained on, making GPT-4b an example of a â€œsmall language modelâ€ that works with a focused data set."" https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/"
o3,OpenAI,https://lifearchitect.ai/o3/,5000,100000,20:1,74.5,,,87.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://lifearchitect.ai/o3/,MoE,Reasoning. SoTA model for Dec/2024. Parameter estimate is very rough centrepoint for range 400B-52T.
o1-2024-12-17,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,91.8,,75.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸ”´,https://openai.com/index/o1-and-new-tools-for-developers/,MoE,"""o1-2024-12-17 sets new state-of-the-art results on several benchmarks, improving cost-efficiency and performance."""
o1,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,92.3,91,79,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://openai.com/index/introducing-chatgpt-pro/,MoE,"Reasoning. ""a version of our most intelligent model that thinks longer for the most reliable responses"" System card about safety only: https://cdn.openai.com/o1-system-card-20241205.pdf"
gpt-4o-2024-11-20,OpenAI,https://chat.com/,200,20000,100:1,6.7,85.7,,46,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://platform.openai.com/docs/models#gpt-4o,MoE,"Material decrease in benchmark scores (GPQA: -13.37%, MMLU: -3.38%) compared to Aug/2024. Pruned? Quantized? https://github.com/openai/simple-evals"
o1-preview,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,92.3,91,78.3,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸ”´,https://openai.com/index/introducing-openai-o1-preview/,MoE,Reasoning.
GPT-4o mini,OpenAI,https://chatgpt.com/,8,13000,"1,625:1",1.1,82,,40.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,MoE,"Omnimodel. ""OpenAI would not disclose exactly how large GPT-4o mini is, but said itâ€™s roughly in the same tier as other small AI models, such as Llama 3 8b, Claude Haiku and Gemini 1.5 Flash."" https://techcrunch.com/2024/07/18/openai-unveils-gpt-4o-mini-a-small-ai-model-powering-chatgpt/ ""tested GPT-4o to identify potential risks, which we have addressed and plan to share the details of in the forthcoming GPT-4o system card and Preparedness scorecard."" And related paper about instruction hierarchy: https://arxiv.org/abs/2404.13208"
CriticGPT,OpenAI,,3,1000,334:1,0.2,,,,ğŸ‘¥,Jun/2024,ğŸ”´,https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf,Dense,"""LLM Critics Help Catch LLM Bugs"" Announce: https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"
GPT-4o,OpenAI,https://chatgpt.com/,200,20000,100:1,6.7,88.7,72.6,53.6,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸ”´,https://openai.com/index/gpt-4o-system-card/,MoE,"gpt-4o-2024-05-13 no longer easily available, so hidden in the Model Table rankings. Omnimodel. â€˜[GPT-4o is] likely an early checkpoint of GPT-5â€™. https://twitter.com/drjimfan/status/1790089671365767313 ELO: https://twitter.com/LiamFedus/status/1790064963966370209 Demo: https://youtu.be/DQacCB9tDaw"
gpt-4-turbo-2024-04-09,OpenAI,https://chat.openai.com/,70,13000,186:1,3.2,86.5,63.7,49.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,"This is such a significantly better model that I've added it here. This GPQA=46.5%, old GPT-4 GPQA=36%. https://twitter.com/EpochAIResearch/status/1778463039932584205 MMLU scores are unclear, but may have improved by 1%: https://twitter.com/OpenAI/status/1778602770784002136. Final benchmarks are here: https://archive.md/6Cc0Z"
GPT-4 Turbo,OpenAI,https://chat.openai.com/,70,13000,186:1,3.2,86.4,,46.5,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,https://openai.com/blog/new-models-and-developer-products-announced-at-devday
GPT-4 MathMix,OpenAI,,1760,13000,8:1,15.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸ”´,https://arxiv.org/abs/2305.20050,MoE,"Unreleased, includes step by step research"
"GPT-4 Classic (gpt-4-0314 & gpt-4-0613, non-Turbo)",OpenAI,https://chat.openai.com/,1760,13000,8:1,15.9,86.4,,35.7,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://cdn.openai.com/papers/gpt-4.pdf,MoE,Original MMLU=86.4. MMLU=90.1 with prompting. Proto-AGI. 1.76T parameters MoE.
ChatGPT (gpt-3.5-turbo),OpenAI,https://chat.openai.com/,20,,,,70,,28.1,ğŸ†† ğŸ“š â¬† ğŸ•¸,Nov/2022,ğŸŸ¢,https://openai.com/blog/chatgpt,Dense,"Instruct with strict policies (""extremely limited"")"
text-davinci-003,OpenAI,https://chat.openai.com/,,,,,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Nov/2022,ğŸŸ¢,https://openai.com/blog/chatgpt,Dense,
6.9B FIM,OpenAI,,6.9,100,15:1,0.1,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Jul/2022,ğŸ”´,https://arxiv.org/pdf/2207.14255.pdf,Dense,"Several models: 8 sizes, NLP, Code, FIM/non-FIM. 100B tokens for 6.9B params... beyond chinchilla"
Codex,OpenAI,Playground,12,,,,,,,ğŸ•¸,Aug/2021,ğŸŸ¢,https://arxiv.org/abs/2107.03374,Dense,Code
GPT-3,OpenAI,Sunset/deprecated :-(,175,300,2:1,0.8,43.9,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,May/2020,ğŸŸ¢,https://arxiv.org/abs/2005.14165,Dense,No RLHF (base only). Popular: 3.1M wpm. Dataset: https://lifearchitect.ai/whats-in-my-ai/
GPT-2,OpenAI,Hugging Face,1.5,10,7:1,0,32.4,,,â¬†,Feb/2019,ğŸŸ¢,https://openai.com/blog/better-language-models/,Dense,Reddit outbound only
GPT-1,OpenAI,Hugging Face,0.117,0.003,1:1,0,,,,ğŸ“š,Jun/2018,ğŸŸ¢,https://openai.com/blog/language-unsupervised/,Dense,"Books only. ""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens."" =3,276,800"
Teuken-7B,OpenGPT-X,https://huggingface.co/openGPT-X/Teuken-7B-instruct-research-v0.4,7,4000,572:1,0.6,50,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2410.03730,Dense,"24 EU languages (60% non-English): bg, cs, da, de, el, en, es, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv. https://opengpt-x.de/models/teuken-7b-de/ & paper date is Sep/2024."
Orion-14B,OrionStar,https://github.com/OrionStarAI/Orion,14,2500,179:1,0.6,69.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.12246,Dense,"English, Chinese, Japanese, Korean, and other languages."
pplx-70b-online,Perplexity,https://labs.perplexity.ai/,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://blog.perplexity.ai/blog/introducing-pplx-online-llms,Dense,Web access. Higher 'freshness' and 'truth' scores.
Pleias 1.0,PleIAs,https://huggingface.co/PleIAs/Pleias-3b-Preview,3,1086,362:1,0.2,,,,ğŸ•¸ âš›ï¸,Dec/2024,ğŸŸ¢,https://huggingface.co/blog/Pclanglais/common-models,Dense,"Trained on the Jean Zay supercomputer, 192x H100s for 20 days. Dataset is new CC + Synthetic: https://huggingface.co/datasets/PleIAs/common_corpus"
METAGENE-1,Prime Intellect,https://huggingface.co/metagene-ai,7,370,53:1,0.2,,,,ğŸŒ‹,Jan/2025,ğŸŸ¢,https://metagene.ai/metagene-1-paper.pdf,Dense,"Llama-2-7B base. ""METAGENE-1 is a 7B parameter metagenomic foundation model designed for pathogen detection and pandemic monitoring, trained on over 1.5 trillion base pairs [âˆ¼370 billion tokens (â‰ˆ1.69 trillion base pairs)] of DNA and RNA collected via metagenomic sequencing of wastewater."""
INTELLECT-1,Prime Intellect,https://huggingface.co/PrimeIntellect/INTELLECT-1,10,1000,100:1,0.3,49.89,,28.32,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf,Dense,"Training complete 22/Nov/2024. Fully distributed training: ""the first decentralized training run of a 10-billion-parameter model, inviting anyone to contribute compute and participate. This brings us one step closer towards open source AGI."""
RakutenAI-7B,Rakuten Group,https://huggingface.co/Rakuten/RakutenAI-7B,7,3000,429:1,0.5,61.31,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2403.15484,Dense,Japanese. Mistral 7B derivative.
202305-refact2b-mqa-lion,Refact,https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/,1.6,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¡,https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/,Dense,"LiON vs Adam, code, RedPajama+The Stack"
Reka Core,Reka AI,https://poe.com/RekaCore,300,10000,34:1,5.8,83.2,,38.2,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,https://www.reka.ai/news/reka-core-our-frontier-class-multimodal-language-model
Reka Edge,Reka AI,https://chat.reka.ai/,7,4500,643:1,0.6,63.1,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,
Reka Flash,Reka AI,https://poe.com/RekaFlash,21,5000,239:1,1.1,73.5,,34,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,My testing shows very poor performance equiv with tiny model
Yasa-1,Reka AI,https://reka.ai/announcing-our-multimodal-ai-assistant/,,,,,,,,ğŸŒ‹,Oct/2023,ğŸŸ¡,https://reka.ai/product/,Dense,"Multi-modal. No public arch info. Researchers from DeepMind, Google, Baidu and Meta building enterprise models"
Yasa,Reka AI,https://reka.ai/product/,,,,,,,,,Jun/2023,ğŸŸ¡,https://reka.ai/product/,Dense,"No public arch info. Researchers from DeepMind, Google, Baidu and Meta building enterprise models"
Hanooman,Reliance,,40,,,,,,,ğŸŒ‹,Feb/2024,ğŸŸ¢,https://www.hanooman.ai/,Dense,"11 Indian languages like Hindi, Tamil, and Marathi"
YuLan-Mini,Renmin,https://github.com/RUC-GSAI/YuLan-Mini,2.4,1080,450:1,0.2,51.79,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.17743,Dense,"""1.08T tokens for training. Among them are 481B English web data, 138B general English knowledge, 227B code pre-training data, 16.7B code instruction data, 93.8B mathematics pre-training data, 15.5B mathematics instruction data, and 108B Chinese data."""
YuLan-Base-12B,Renmin,https://github-com.translate.goog/RUC-GSAI/YuLan-Chat?_x_tr_sl=zh-CN&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=sc,12,1700,142:1,0.5,55.7,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://arxiv.org/abs/2406.19853,Dense,"""YuLan's training is finished on Jan, 2024 and has achieved performance on par with state-of-the-art LLMs across various English and Chinese benchmarks."""
Sonus-1 Reasoning,Rubik's AI,https://chat.sonus.ai/sonus/,405,15000,38:1,8.2,90.15,73.1,67.3,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://sonus.ai/blog/sonus-1,Dense,"Likely a Llama 3.1 405B wrapper. ALPrompt 2024H1=5/5. ALPrompt 2024H2=2/5. ALPrompt 2025H1=1/5. This is a strange model: slow and smart, but not as performant as o1. No arch details at all."
Deepthought-8B,Ruliad,https://chat.ruliad.co/,8,15000,"1,875:1",1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha,Dense,Reasoning. No evals. Llama 3.1 8B base.
RWKV-7 Goose,RWKV,https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7,0.4,332,830:1,0,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7,Dense,"RWKV (pronounced RwaKuv) is an RNN: ""multilingual, supporting over 100 languages and code."". Full run is 332B tokens of 3.1T dataset."
RWKV-v6 Finch,RWKV,https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2,7.63,2500,328:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://huggingface.co/BlinkDL/rwkv-6-world,Dense,RWKV (pronounced RwaKuv) is an RNN: https://twitter.com/BlinkDL_AI/status/1787834625211158562
RWKV-v5 EagleX,RWKV,https://huggingface.co/recursal/EagleX_1-7T,7.52,1700,227:1,0.4,40.14,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b,Dense,RWKV (pronounced RwaKuv) is an RNN: Built on the RWKV-v5 architecture (a linear transformer with 10-100x+ lower inference cost)
RWKV-v5 Eagle 7B,RWKV,https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2,7.52,1100,147:1,0.3,33.21,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers,Dense,"RWKV (pronounced RwaKuv) is an RNN: Built on the RWKV-v5 architecture (a linear transformer with 10-100x+ lower inference cost), Trained on 1.1 Trillion Tokens across 100+ languages. Original paper: https://arxiv.org/abs/2305.13048"
RWKV-4,RWKV,https://huggingface.co/BlinkDL,14,332,24:1,0.2,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸ ğŸŒ‹,Nov/2022,ğŸŸ¢,https://arxiv.org/abs/2305.13048,Dense,RWKV (pronounced RwaKuv) is an RNN: https://www.reddit.com/r/MachineLearning/comments/yxt8sa/r_rwkv4_7b_release_an_attentionfree_rnn_language/
Sailor2,Sail,https://huggingface.co/spaces/sail/Sailor2-20B-Chat,20,18510,926:1,2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2024,ğŸŸ¢,https://github.com/sail-sg/sailor2,Dense,SEA languages. Continual pretraining based on Qwen2.5. Project page: https://sea-sailor.github.io/blog/sailor2/
Sailor,Sail,https://huggingface.co/sail,7,200,29:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.03608v1,Dense,"SEA languages. Based on Qwen-1.5. https://github.com/sail-sg/sailor-llm ""Generally Sailor models consume around 200B tokens, completing a full pass through the SailCraft corpus once. However, the Sailor-0.5B model undergoes training with 400B tokens, equivalent to 2 epochs."""
EvoLLM-JP,Sakana AI,https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B,10,800,80:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2403.13187,Dense,"Japanese. Model merge 'our EvoLLM-JP-A is a merge of shisa-gamma-7b-v1, Arithmo2-Mistral-7B, and Abel7B-002' https://sakana.ai/evolutionary-model-merge/"
SFR-LLaMA-3.1-70B-Judge,Salesforce,https://blog.salesforceairesearch.com/sfr-judge/,70,15000,215:1,3.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2024,ğŸ”´,https://arxiv.org/abs/2409.14664,Dense,"Code coming soon: https://github.com/SalesforceAIResearch/SFRJudge ""we opt to focus on datasets that evaluate modern (2023 and beyond) LLM responses, as older datasets likely contain lower quality responses from less capable models, with correspondingly stale annotations. We supplement human-annotated data with synthetically generated data to endow our judge models with specific capabilities (e.g., following fine-grained rubrics in evaluation)"""
xLAM,Salesforce,https://huggingface.co/Salesforce/xLAM-8x22b-r,141,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://huggingface.co/Salesforce/xLAM-8x22b-r,MoE,64K sequence length. Released under Apache-2.0.
XGen,Salesforce,https://github.com/salesforce/xgen,7,1500,215:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://blog.salesforceairesearch.com/xgen/,Dense,8K sequence length. Released under Apache-2.0.
CodeT5+,Salesforce,https://huggingface.co/Salesforce/codet5p-16b,16,,,,,,,ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://arxiv.org/abs/2305.07922,Dense,"InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001'"
CodeGen,Salesforce,"TS, Goose",16,,,,,,,"ğŸ•¸ BigQuery, BigPython",Mar/2022,ğŸŸ¢,https://arxiv.org/abs/2203.13474,Dense,Code
CodeT5,Salesforce,,0.7,,,,,,,"ğŸ•¸ BigQuery, BigPython",Mar/2022,ğŸŸ¢,https://arxiv.org/abs/2109.00859,Dense,Code. Large introduced in https://arxiv.org/pdf/2207.01780.pdf
EvaByte,SambaNova,https://huggingface.co/EvaByte/EvaByte,6.5,1500,231:1,0.3,50.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,w,https://hkunlp.github.io/blog/2025/evabyte/,Dense,"""efficient byte-level processing at scale... [compared to tokenizer-based LMs:] 5x less training data, excelling in coding tasks, and decoding up to 2x faster. Its token-free design also brings added flexibility, avoiding tokenizer quirks while naturally extending to multimodal applications without any architecture tweaks."""
Samba-1,SambaNova,https://trysambanova.ai/,1400,20000,15:1,17.6,,,,ğŸŒ‹,Feb/2024,ğŸŸ¡,https://sambanova.ai/press/secure-one-trillion-parameter-generative-ai-model-for-the-enterprise,CoE,CoE: Collection of experts: Llama2 7B / 13B / 70B Mistral 7B DeepSeek Coder 1.3B / 6.7B / 33B Falcon 40B DePlot CLIP Llava
Gauss,Samsung,https://koreajoongangdaily.joins.com/news/2023-11-08/business/tech/Samsung-unveils-generative-AI-model-Gauss/1908889,7,,,,,,,,Nov/2023,ğŸŸ¡,https://koreajoongangdaily.joins.com/news/2023-11-08/business/tech/Samsung-unveils-generative-AI-model-Gauss/1908889,Dense,"Gauss Language specializing in generating texts, Gauss Code on software and code description and Gauss Image for image creation."
sarvam-2b,Sarvam AI,https://huggingface.co/sarvamai/sarvam-2b-v0.5,2,4000,"2,000:1",0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://huggingface.co/sarvamai/sarvam-2b-v0.5,Dense,"Indic languages supported are: Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu."
mGPT,Sber,Hugging Face,13,,,,,,,ğŸ†† ğŸ•¸,Apr/2022,ğŸŸ¡,https://arxiv.org/abs/2204.07580,Dense,60 languages. Only 1.3B model available
SenseNova 5.5,SenseTime,https://platform.sensenova.cn/home#/home,600,10000,17:1,8.2,,,,âš›ï¸,Jul/2024,ğŸŸ¢,https://www.sensetime.com/en/news-detail/51168278?categoryId=1072,MoE,"""The model training was based on over 10TB tokens [sic, taken as 10T tokens instead of 10TB=2T tokens] of high-quality training data, including a large amount of synthetically-generated reasoning chain data, which help to enhance its reasoning capabilities."" & ""The updates include SenseNova 5o, the first real-time multimodal model in China, which provides a new AI interaction model on par with GPT-4oâ€™s streaming interaction capabilities"""
SenseNova 5.0,SenseTime,,600,10000,17:1,8.2,84.78,,42.93,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://news.futunn.com/en/post/41290101/a-large-shangtang-multi-modal-model-with-600-billion-parameters,MoE,GPT-4 scale; low media coverage; no demo in Western world. https://www.techinasia.com/sensetime-pauses-trading-stock-rises-30-model-launch
Meta-Transformer,Shanghai AI Laboratory/CUHK,https://github.com/invictus717/MetaTransformer,2,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://arxiv.org/abs/2307.10802,Dense,"Proto-AGI. 12 modalities (text, image, point cloud, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, Inertial Measurement Unit (IMU), and graph data)."
OREAL-32B,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/OREAL-32B,32,4000,125:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Feb/2025,ğŸŸ¢,https://arxiv.org/abs/2502.06781,Dense,Reasoning. OREAL=Outcome REwArd-based reinforcement Learning.
InternLM3,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm3-8b-instruct,8,4000,500:1,0.6,76.6,57.6,37.4,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2025,ğŸŸ¢,https://huggingface.co/internlm/internlm3-8b-instruct,Dense,"""InternLM3 is trained on only 4 trillion high-quality tokens, saving more than 75% of the training cost compared to other LLMs of similar scale."" Playground: https://internlm-chat.intern-ai.org.cn/"
InternVL 2.5,Shanghai AI Laboratory/SenseTime,https://huggingface.co/spaces/OpenGVLab/InternVL,78,18120,233:1,4,86.1,71.1,49,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://arxiv.org/abs/2412.05271,Dense,"Reasoning. Benchmarks are estimates based on Qwen2.5 72B Instruct as the base LLM (InternVL 2.5=InternViT-6B-448px-V2.5 5.5B + Qwen2.5-72B-Instruct). ""Notably, Qwen2-VL processed a cumulative total of 1.4T tokens, while our InternVL2.5-78B is trained on just âˆ¼120B tokens [of vision].""Dataset... we identify repetitive generation as one of the most detrimental issues. In many open-source or synthetic datasets, a small number of repetitive samplesâ€”comprising merely thousands of examples in our Stage 2 data mixtureâ€”can cause the model to spiral into repetitive loops, particularly in long-form outputs or CoT reasoning tasks. This phenomenon undermines the effectiveness of test-time scaling strategies. To address this challenge and support future research, we designed an efficient data filtering pipeline to remove low-quality samples, thereby minimizing the risk of repetitive generation."" Repo: https://github.com/OpenGVLab/InternVL"
InternLM2.5,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm2_5-20b-chat,20,2600,130:1,0.8,73.5,,38.4,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_7b.md,Dense,"""The release of InternLM2.5 series contains 7B model size for now and we are going to release the 1.8B and 20B versions soon"" [20B released around 1/Aug/2024]"
InternLM2,Shanghai AI Laboratory/SenseTime,https://github.com/InternLM/InternLM,20,2600,130:1,0.8,67.7,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2403.17297,Dense,
InternLM,Shanghai AI Laboratory/SenseTime,https://internlm-org.translate.goog/?_x_tr_sl=zh&_x_tr_tl=en,104,1600,16:1,1.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2023,ğŸ”´,https://github.com/InternLM/InternLM-techreport,Dense,"Outperforms ChatGPT, LLaMA on RACE-h, Chinese + English"
Viking,Silo AI,,33,2000,61:1,0.9,,,,ğŸŒ‹,Apr/2024,ğŸŸ¢,https://www.silo.ai/blog/viking-7b-13b-33b-sailing-the-nordic-seas-of-multilinguality,Dense,"Viking uses an architecture similar to Llama 2, with flash attention, rotary embeddings, grouped query attention and supports a 4k sequence length'"
Poro,Silo AI,https://huggingface.co/LumiOpen/Poro-34B,34.2,1000,30:1,0.6,,,,ğŸŒ‹,Feb/2024,ğŸŸ¢,https://www.silo.ai/blog/viking-7b-13b-33b-sailing-the-nordic-seas-of-multilinguality,Dense,"Uses a BLOOM architecture with ALiBi embeddings to allow for context window extrapolation. While model architecture for the initial model has been kept simple, future models under progress will support additional capabilities, such as flash attention, rotary embeddings and grouped query attention.'"
NExT-GPT,Singapore,https://next-gpt.github.io/,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2309.05519,Dense,Multimodal. Vicuna 7B + other modalities
Arctic,Snowflake AI Research,https://arctic.streamlit.app/,480,3500,8:1,4.3,67.3,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/,Hybrid,"""Arctic uses a unique Dense-MoE Hybrid transformer architecture. It combines a 10B dense transformer model with a residual 128Ã—3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating."""
Apollo,SRIBD/CUHK,https://apollo.llmzoo.com/,7,2500,358:1,0.4,,,,ğŸ†† ğŸ“šğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://arxiv.org/abs/2403.03640,Dense,Qwen 1.8B as base. Medical focus.
Stable Code Instruct 3B,Stability AI,https://huggingface.co/stabilityai/stable-code-instruct-3b,2.7,560,208:1,0.1,,,,ğŸŒ‹,Mar/2024,ğŸŸ¢,https://stability.ai/news/introducing-stable-code-instruct-3b,Dense,"Context window=16,384. Trained on The Stack dataset."
Stable Beluga 2.5,Stability AI,,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://stability.ai/news/putting-the-ai-supercomputer-to-work,Dense,"Mentioned in Stability release about Intel chips 11/Mar/2024, availablity unknown"
Japanese StableLM Alpha 7B,Stability AI,https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b,7,750,108:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2023,ğŸŸ¢,https://stability.ai/blog/stability-ai-new-jplm-japanese-language-model-stablelm,Dense,Best-performing openly available language model for Japanese speakers.
Stable Code 3B,Stability AI,https://huggingface.co/stabilityai/stablecode-completion-alpha-3b-4k,2.7,560,208:1,0.1,,,,ğŸŒ‹,Aug/2023,ğŸŸ¢,https://stability.ai/blog/stablecode-llm-generative-ai-coding,Dense,"Context window=16,384. Trained on The Stack dataset."
Stable Beluga 2,Stability AI,https://huggingface.co/stabilityai/FreeWilly2,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models,Dense,Fine-tuned Llama 2. Non-commercial use license. Codename was FreeWilly2
Stable Beluga 1,Stability AI,https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor,65,1400,22:1,1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models,Dense,Fine-tuned LLaMA-1. Non-commercial use license. Codename was FreeWilly1
StableLM,Stability AI,https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat,65,1500,24:1,1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2023,ğŸŸ¢,https://github.com/stability-AI/stableLM/,Dense,"contains 1.5 trillion tokens, roughly 3x the size of The Pile. These models will be trained on up to 1.5 trillion tokens. The context length for these models is 4096 tokens."
s1-32B,Stanford,https://github.com/simplescaling/s1,32,18000,563:1,2.5,,,59.6,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Feb/2025,ğŸŸ¢,https://arxiv.org/abs/2501.19393,Dense,"Reasoning. Based on Qwen2.5-32B-Instruct. ""we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the modelâ€™s thinking process or lengthening it by appending â€œWaitâ€ multiple times to the modelâ€™s generation when it tries to end. This can lead the model to doublecheck its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24)."""
TTT-Linear,Stanford,https://github.com/test-time-training/ttt-lm-jax,1.3,26,20:1,0,,,,ğŸ“š,Aug/2024,ğŸŸ¢,https://arxiv.org/abs/2407.04620,Dense,"Test-Time Training (TTT) layers. Real-time learning by Stanford, UC, and Meta. Potential for frontier models in 2025+."
Med-Flamingo,Stanford,https://github.com/snap-stanford/med-flamingo,8.3,1000,121:1,0.3,,,,ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://arxiv.org/abs/2307.15189,Dense,"Uses LAION OpenFlamingo 9B, based on LLaMA-7B text + 1.3B vision"
Alpaca,Stanford,https://crfm.stanford.edu/alpaca/,7,1000,143:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸ‘¥,Mar/2023,ğŸŸ¢,https://github.com/tatsu-lab/stanford_alpaca,Dense,Stanford Alpaca: An Instruction-following LLaMA model'
Diffusion-LM,Stanford,Github (train/deploy),0.3,,,,,,,ğŸŒ‹ ğŸ‘¥,May/2022,ğŸŸ¢,https://arxiv.org/abs/2205.14217,Dense,GPT-J with synthetic data
Step-2,StepFun,https://platform.stepfun.com/#language-step2,1000,13000,13:1,12,82.9,63,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://platform.stepfun.com/docs/llm/text,MoE,"Launched early Jul/2024: https://pandaily.com/stepfun-releases-three-large-models-of-the-step-series/ ""StepFun, founded in April 2023 with the mission to â€œScale-up possibilities for everyone,â€ unites top talent in artificial intelligence from both domestic and international backgrounds, and is dedicated to advancing toward AGI. The company has already launched the Step series of foundation models, which includes Step-2, a cutting-edge trillion-parameter Mixture of Experts (MoE) language model; Step-1.5V, a powerful multimodal large model; and Step-1V, an innovative image generation model, among others."""
TinyLlama,SUTD/Independent,https://github.com/jzhang38/TinyLlama,1.1,3000,"2,728:1",0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.02385,Dense,"Overtrained' using 2,727 tokens per parameter. Dataset was 1T: 3 epochs to 3T seen. Singapore"
Hunyuan-Large,Tencent,https://huggingface.co/tencent/Tencent-Hunyuan-Large,389,7000,18:1,5.5,89.9,60.2,42.4,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2411.02265,MoE,"Hunyuan-Large is pre-trained on 7T tokens, which contains nearly 1.5T tokens of high-quality and diverse synthetic data.' '389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens'"
FuseLLM,Tencent,https://github.com/fanqiwan/FuseLLM,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.10491,Dense,"Fusion of Llama-2-7B (2T tok), OpenLLaMA-7B (2T tok), and MPT-7B (1T tok)."
LLaMA Pro,Tencent,https://huggingface.co/TencentARC/LLaMA-Pro-8B,8.3,2080,251:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://arxiv.org/abs/2401.02415,Dense,We pre-train LLAMA PROâ€™s expanded blocks on 80B tokens using open-source code and math data for 2830 GPU Hours (16 NVIDIA H800 GPUs for about 7 days).
Hunyuan,Tencent,https://www.tencent.com/en-us/articles/2201685.html,100,2000,20:1,1.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2402.01723v1,Dense,
Fox-1,TensorOpera,https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1,1.6,3005,"1,879:1",0.2,44.99,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2411.05281,Dense,Gold standard for dataset documentation
BOLT2.5B,ThirdAI,https://huggingface.co/spaces/thirdai/BOLT2.5B,2.5,40,16:1,0,,,,ğŸ•¸,Sep/2023,ğŸŸ¢,https://medium.com/thirdai-blog/introducing-the-worlds-first-generative-llm-pre-trained-only-on-cpus-meet-thirdai-s-bolt2-5b-10c0600e1af4,Dense,CPU trained
Falcon 3,TII,https://huggingface.co/tiiuae/Falcon3-10B-Base,10,16000,"1,600:1",1.3,73.1,42.5,34.1,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Dec/2024,ğŸŸ¢,https://huggingface.co/blog/falcon3,Dense,"""We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens... upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data."""
Falcon Mamba 7B,TII,https://falconllm.tii.ae/falcon-models.html,7,6000,858:1,0.7,62.11,14.47,8.05,ğŸ•¸,Aug/2024,ğŸŸ¢,https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html,Dense,https://huggingface.co/spaces/tiiuae/falcon-mamba-playground
Falcon 2 11B,TII,https://huggingface.co/tiiuae/falcon-11B,11,5500,500:1,0.8,58.37,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2024,ğŸŸ¢,https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas,Dense,Announce: https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas
Falcon 180B,TII,https://huggingface.co/spaces/tiiuae/falcon-180b-demo,180,3500,20:1,2.6,70.6,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2023,ğŸŸ¢,https://arxiv.org/abs/2311.16867,Dense,Major milestone for open source models (largest open dense model to date).
Falcon,TII,TS,40,1000,25:1,0.7,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model,Dense,Abu Dhabi
NOOR,TII,,10,,,,,,,ğŸ†† ğŸ“š ğŸ•¸ ğŸ‡¦ğŸ‡ª,Apr/2022,ğŸ”´,https://www.tii.ae/news/technology-innovation-institute-announces-launch-noor-worlds-largest-arabic-nlp-model,Dense,"Arabic. ""Worldâ€™s largest high-quality cross-domain Arabic dataset, combining web data with books, poetry, news articles, and technical information"""
StripedHyena 7B,Together,https://api.together.xyz/playground/language/togethercomputer/StripedHyena-Hessian-7B,7.65,,,,,,,ğŸŒ‹,Dec/2023,ğŸŸ¢,https://www.together.ai/blog/stripedhyena-7b,Dense,"RedPajama (C4), new arch beyond just Transformers"
LLaMA-2-7B-32K,Together,https://huggingface.co/togethercomputer/LLaMA-2-7B-32K,7,2000,286:1,0.4,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2023,ğŸŸ¢,https://together.ai/blog/llama-2-7b-32k,Dense,32k context window instead of 4k (Llama 2)
GPT-NeoX-Chat-Base-20B,Together,https://huggingface.co/spaces/togethercomputer/OpenChatKit,20,,,,33.6,,,ğŸ†† ğŸ“š â¬† ğŸ•¸ ğŸŒ‹,Mar/2023,ğŸŸ¢,https://github.com/togethercomputer/OpenChatKit,Dense,"instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai. '"
GPT-JT,Together,https://huggingface.co/spaces/togethercomputer/GPT-JT,6,,,,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸ ğŸŒ‹,Nov/2022,ğŸŸ¢,https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai,Dense,
MiniCPM-2.4B,Tsinghua,https://github.com/OpenBMB/MiniCPM/,2.4,1100,459:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/abs/2404.06395,Dense,MoE option=https://huggingface.co/openbmb/MiniCPM-MoE-8x2B
Eurus,Tsinghua,https://huggingface.co/collections/openbmb/eurus-660bc40bec5376b3adc9d1c5,70,2000,29:1,1.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://huggingface.co/collections/openbmb/eurus-660bc40bec5376b3adc9d1c5,Dense,Fine-tune of Mistral-7B and CodeLlama-70B.
xTrimoPGLM,Tsinghua,,100,1000,10:1,1.1,,,,ğŸŒ‹,Jul/2023,ğŸ”´,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,Dense,Protein language model
OpenChat,Tsinghua,https://huggingface.co/openchat/openchat_3.5,13,2000,154:1,0.5,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2022,ğŸŸ¢,https://arxiv.org/abs/2309.11235,Dense,Llama 2 13B -> OpenChat 13B
CodeGeeX,Tsinghua,,13,850,66:1,0.4,,,,ğŸŒ‹,Sep/2022,ğŸŸ¢,https://github.com/THUDM/CodeGeeX,Dense,
GLM-130B,Tsinghua,https://huggingface.co/spaces/THUDM/GLM-130B,130,400,4:1,0.8,,,,ğŸ†† ğŸ“š â¬† ğŸ•¸,Aug/2022,ğŸŸ¢,https://arxiv.org/abs/2210.02414,Dense,"50% English (200B tokens), so included here"
MatMul-Free LM,UCSC,https://github.com/ridgerchu/matmulfreellm,2.7,100,38:1,0.1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jun/2024,ğŸŸ¢,https://arxiv.org/abs/2406.02528,Dense,"""we explore alternative methods for mixing tokens without relying on matrix multiplications."" Compared with Transformer++ based on Llama-2, not to be confused with the pre-GPT-3 American Express Transformer++ paper from 2/Mar/2020. Instead, Transformer++ is defined in the Mamba paper: 'Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and SwiGLU MLP (Shazeer 2020)'"
Raven,UI/NVIDIA,,11,40,4:1,0.1,,,,ğŸ†† ğŸ•¸,Aug/2023,ğŸ”´,https://arxiv.org/abs/2308.07922,Dense,RAG Atlas
TowerLLM,Unbabel,https://unbabel.com/meet-towerllm/,7,1020,146:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Feb/2024,ğŸŸ¢,https://arxiv.org/abs/2402.17733,Dense,"Commercial product, Llama-2 as base."
SOLAR-10.7B,Upstage AI,https://huggingface.co/upstage/SOLAR-10.7B-v1.0,10.7,,,,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.15166,Dense,South Korean. Llama-2 arch. SOTA for its size (Dec/2023).
Guanaco,UW,https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi,65,1400,22:1,1,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,May/2023,ğŸŸ¢,https://arxiv.org/abs/2305.14314,Dense,LLaMA-65B via QLoRA
Mockingbird,Vectara,https://vectara.com/platform/,9,1000,112:1,0.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹ âš›ï¸,Jul/2024,ğŸŸ¢,https://vectara.com/blog/mockingbird-a-rag-and-structured-output-focused-llm/,Dense,"""At <10B parameters it's an LLM trained to provide optimal results for RAG and structured outputs."""
MotionLM,Waymo,,0.09,,,,,,,ğŸŒ‹,Sep/2023,ğŸ”´,https://arxiv.org/abs/2309.16534,Dense,LLM for autonomous vehicle forecasting. https://youtu.be/jrMMNmN21I8?t=1560
GAIA-1,Wayve,https://wayve.ai/thinking/scaling-gaia-1/,9,,,,,,,ğŸŒ‹,Sep/2023,ğŸ”´,https://arxiv.org/abs/2309.17080,Dense,"World model, generates video. Uses T5-large 770M for language + all vision parameters"
WeLM,Wechat,https://welm.weixin.qq.com/docs/playground/,10,300,30:1,0.2,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Sep/2022,ğŸŸ¢,https://arxiv.org/abs/2209.10372,Dense,13% English tokens and 87% Chinese
YAYI 2,Wenge,https://huggingface.co/wenge-research/yayi2-30b,30,2650,89:1,0.9,80.5,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Dec/2023,ğŸŸ¢,https://arxiv.org/abs/2312.14862,Dense,Dataset=240TB filtered to 10.6TB for 2.65T tokens
Palmyra-Med-70B,Writer,https://huggingface.co/Writer/Palmyra-Med-70B-32K,70,1200,18:1,1,,,,ğŸŒ‹,Jul/2024,ğŸŸ¢,https://writer.com/blog/palmyra-med-fin-models/,Dense,Medical. MMLU Medical Genetics=94.0
Palmyra-Fin-70B,Writer,https://huggingface.co/Writer/Palmyra-Fin-70B-32K,70,1200,18:1,1,,,,ğŸŒ‹,Jul/2024,ğŸŸ¢,https://writer.com/blog/palmyra-med-fin-models/,Dense,"Financial. ""across a variety of real-world financial use cases. It outperformed popular models like Claude 3.5 Sonnet, GPT-4o, and Mixtral-8x7b"""
Palmyra X,Writer,,72,1200,17:1,1,70.2,,,ğŸŒ‹,Jan/2024,ğŸŸ¢,https://writer.com/blog/palmyra-helm-benchmark/,Dense,"Palmyra X V2, Palmyra X V3, Palmyra X V4. https://venturebeat.com/ai/why-writers-palmyra-llm-is-the-little-ai-model-that-could-for-enterprises/"
Palmyra,Writer,https://huggingface.co/models?search=palmyra,20,300,15:1,0.3,,,,ğŸŒ‹,Feb/2023,ğŸŸ¢,https://writer.com/blog/palmyra/,Dense,"Only up to 5B available open-source 'trained on over 300 billion tokens of text data, and the size of the resulting model is over 20 billion parameters. ' https://writer.com/product/cowrite/"
Grok-3,xAI,https://lifearchitect.ai/whats-in-grok/,928,36200,40:1,,,,,,TBA,,,MoE,"Training Jul-Dec 2024 on 100,000 H100s. Due 2025H1. Showing dense param count, but will be MoE model."
Grok-2,xAI,https://x.com/i/grok,400,15000,38:1,8.2,87.5,75.5,56,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Aug/2024,ğŸŸ¢,https://x.ai/blog/grok-2,Dense,"MMLU-Pro=75.5=SOTA. Claude 3.5S MMLU-Pro=72.83. ""Grok-2 has been tested on the LMSYS leaderboard under the name ""sus-column-r."" At the time of this blog post, it is outperforming both Claude 3.5 Sonnet and GPT-4-Turbo."" [Alan: Grok is Heinlein, Sixth Column is also Heinlein: https://en.wikipedia.org/wiki/Sixth_Column ]"
Grok-1.5,xAI,https://grok.x.ai/,180,6000,34:1,3.5,81.3,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Mar/2024,ğŸŸ¢,https://x.ai/blog/grok-1.5,MoE,Context=128k.
Grok-1,xAI,https://grok.x.ai/,314,6000,20:1,4.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸŸ¢,https://github.com/xai-org/grok-1,MoE,Context window=8192. UI: https://twitter.com/TobyPhln/status/1721053802235621734
Grok-0,xAI,https://grok.x.ai/,33,2000,61:1,0.9,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2023,ğŸ”´,https://web.archive.org/web/20231105051542/https://x.ai/,Dense,"Announced Nov/2023, trained Jul/2023"
Xmodel-LM,XiaoduoAI,https://github.com/XiaoduoAILab/XmodelLM,1.1,2064,"1,877:1",0.2,25.9,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Nov/2024,ğŸŸ¢,https://arxiv.org/abs/2411.10083,Dense,SLM
Lemur,XLANG Lab,https://github.com/OpenLemur/Lemur,70,2090,30:1,1.3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2023,ğŸŸ¢,https://arxiv.org/abs/2310.06830,Dense,https://arxiv.org/abs/2310.06830
YaLM 100B,Yandex,Github (train/deploy),100,300,3:1,0.6,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸,Jun/2022,ğŸŸ¢,https://github.com/yandex/YaLM-100B,Dense,"Megatron-LM clone, Russian/English: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6"
GLM-4,Zhipu AI (Tsinghua),https://open.bigmodel.cn/,200,4000,20:1,3,,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jan/2024,ğŸŸ¢,https://pandaily.com/zhipu-ai-unveils-glm-4-model-with-advanced-performance-paralleling-gpt-4/,Dense,Best Chinese model to date based on analysis. Follows OpenAI roadmap. MMLU=81.5. 'hundreds of billions of parameters' https://www.chatglm.cn/
Zamba2-7B,Zyphra,https://huggingface.co/Zyphra/Zamba2-7B,7,3100,443:1,0.5,67.2,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Oct/2024,ğŸŸ¢,https://www.zyphra.com/post/zamba2-7b,Dense,"Mamba2 ""trained on 128 H100 GPUS for approximately 50 days using our internal training framework developed atop Megatron-LM"""
Zamba2-small,Zyphra,https://huggingface.co/Zyphra/Zamba2-2.7B,2.7,3100,"1,149:1",0.3,55,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Jul/2024,ğŸŸ¢,https://www.zyphra.com/post/zamba2-small,Dense,Mamba2
Zamba 7B,Zyphra,https://huggingface.co/Zyphra/Zamba-7B-v1,7,1050,150:1,0.3,57.72,,,ğŸ†† ğŸ“šâ¬† ğŸ•¸ ğŸŒ‹,Apr/2024,ğŸŸ¢,https://arxiv.org/html/2405.16712v1,Dense,Mamba1
,,,,,,,,,,,,,,,
About this sheet,,,,,,,,,,,,,,,